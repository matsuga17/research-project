#!/usr/bin/env python3
"""
cleaner.py - ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
è‡ªç„¶è¨€èªãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚­ãƒ«ç”¨

ä½¿ç”¨æ³•:
    python cleaner.py <file_path> --action <action> [options]
    
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:
    missing     - æ¬ æå€¤å‡¦ç†
    duplicates  - é‡è¤‡å‰Šé™¤
    outliers    - å¤–ã‚Œå€¤å‡¦ç†
    dtypes      - å‹å¤‰æ›
    normalize   - æ­£è¦åŒ–
    all         - å…¨è‡ªå‹•ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
"""

import pandas as pd
import numpy as np
import argparse
import json
from pathlib import Path
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


def load_data(path: str) -> pd.DataFrame:
    """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    suffix = Path(path).suffix.lower()
    if suffix == '.csv':
        return pd.read_csv(path, low_memory=False)
    elif suffix in ['.xlsx', '.xls']:
        return pd.read_excel(path)
    elif suffix == '.json':
        return pd.read_json(path)
    elif suffix in ['.parquet', '.pq']:
        return pd.read_parquet(path)
    else:
        return pd.read_csv(path, low_memory=False)


def save_data(df: pd.DataFrame, path: str, original_path: str = None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜"""
    suffix = Path(path).suffix.lower()
    if suffix == '.csv':
        df.to_csv(path, index=False)
    elif suffix in ['.xlsx', '.xls']:
        df.to_excel(path, index=False)
    elif suffix == '.json':
        df.to_json(path, orient='records', force_ascii=False)
    elif suffix in ['.parquet', '.pq']:
        df.to_parquet(path, index=False)
    else:
        df.to_csv(path, index=False)


def handle_missing(df: pd.DataFrame, strategy: str = 'auto', 
                   columns: list = None, fill_value=None) -> tuple:
    """
    æ¬ æå€¤å‡¦ç†
    
    strategy:
        auto     - å‹ã«å¿œã˜ã¦è‡ªå‹•é¸æŠ
        drop     - æ¬ æè¡Œã‚’å‰Šé™¤
        mean     - å¹³å‡å€¤ã§è£œå®Œ
        median   - ä¸­å¤®å€¤ã§è£œå®Œ
        mode     - æœ€é »å€¤ã§è£œå®Œ
        ffill    - å‰æ–¹è£œå®Œ
        bfill    - å¾Œæ–¹è£œå®Œ
        constant - æŒ‡å®šå€¤ã§è£œå®Œ
    """
    df_clean = df.copy()
    changes = []
    
    if columns is None:
        columns = df.columns[df.isnull().any()].tolist()
    
    for col in columns:
        if col not in df.columns:
            continue
        
        missing_before = df_clean[col].isnull().sum()
        if missing_before == 0:
            continue
        
        if strategy == 'drop':
            df_clean = df_clean.dropna(subset=[col])
            changes.append({
                'column': col,
                'action': 'drop',
                'rows_removed': missing_before
            })
        
        elif strategy == 'auto':
            if pd.api.types.is_numeric_dtype(df_clean[col]):
                fill_val = df_clean[col].median()
                df_clean[col].fillna(fill_val, inplace=True)
                changes.append({
                    'column': col,
                    'action': 'fill_median',
                    'fill_value': fill_val,
                    'filled': missing_before
                })
            else:
                fill_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else ''
                df_clean[col].fillna(fill_val, inplace=True)
                changes.append({
                    'column': col,
                    'action': 'fill_mode',
                    'fill_value': str(fill_val),
                    'filled': missing_before
                })
        
        elif strategy == 'mean':
            fill_val = df_clean[col].mean()
            df_clean[col].fillna(fill_val, inplace=True)
            changes.append({'column': col, 'action': 'fill_mean', 'fill_value': fill_val, 'filled': missing_before})
        
        elif strategy == 'median':
            fill_val = df_clean[col].median()
            df_clean[col].fillna(fill_val, inplace=True)
            changes.append({'column': col, 'action': 'fill_median', 'fill_value': fill_val, 'filled': missing_before})
        
        elif strategy == 'mode':
            fill_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else None
            df_clean[col].fillna(fill_val, inplace=True)
            changes.append({'column': col, 'action': 'fill_mode', 'fill_value': str(fill_val), 'filled': missing_before})
        
        elif strategy == 'ffill':
            df_clean[col].fillna(method='ffill', inplace=True)
            changes.append({'column': col, 'action': 'ffill', 'filled': missing_before})
        
        elif strategy == 'bfill':
            df_clean[col].fillna(method='bfill', inplace=True)
            changes.append({'column': col, 'action': 'bfill', 'filled': missing_before})
        
        elif strategy == 'constant':
            df_clean[col].fillna(fill_value, inplace=True)
            changes.append({'column': col, 'action': 'fill_constant', 'fill_value': str(fill_value), 'filled': missing_before})
    
    return df_clean, changes


def handle_duplicates(df: pd.DataFrame, subset: list = None, 
                      keep: str = 'first') -> tuple:
    """é‡è¤‡å‰Šé™¤"""
    df_clean = df.copy()
    dup_count = df_clean.duplicated(subset=subset, keep=False).sum()
    
    if dup_count > 0:
        df_clean = df_clean.drop_duplicates(subset=subset, keep=keep)
        removed = len(df) - len(df_clean)
        changes = [{
            'action': 'remove_duplicates',
            'subset': subset,
            'keep': keep,
            'duplicates_found': dup_count,
            'rows_removed': removed
        }]
    else:
        changes = [{'action': 'remove_duplicates', 'message': 'No duplicates found'}]
    
    return df_clean, changes


def handle_outliers(df: pd.DataFrame, columns: list = None, 
                    method: str = 'iqr', action: str = 'remove',
                    threshold: float = 1.5) -> tuple:
    """
    å¤–ã‚Œå€¤å‡¦ç†
    
    method:
        iqr     - IQRæ³•ï¼ˆå››åˆ†ä½ç¯„å›²ï¼‰
        zscore  - Zã‚¹ã‚³ã‚¢æ³•
    
    action:
        remove  - å¤–ã‚Œå€¤ã‚’å«ã‚€è¡Œã‚’å‰Šé™¤
        cap     - å¢ƒç•Œå€¤ã§ã‚­ãƒ£ãƒƒãƒ—
        nan     - NaNã«ç½®æ›
    """
    df_clean = df.copy()
    changes = []
    
    if columns is None:
        columns = df.select_dtypes(include='number').columns.tolist()
    
    for col in columns:
        if col not in df.columns or not pd.api.types.is_numeric_dtype(df_clean[col]):
            continue
        
        if method == 'iqr':
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - threshold * IQR
            upper = Q3 + threshold * IQR
        
        elif method == 'zscore':
            mean = df_clean[col].mean()
            std = df_clean[col].std()
            lower = mean - threshold * std
            upper = mean + threshold * std
        
        outliers_mask = (df_clean[col] < lower) | (df_clean[col] > upper)
        outlier_count = outliers_mask.sum()
        
        if outlier_count == 0:
            continue
        
        if action == 'remove':
            df_clean = df_clean[~outliers_mask]
            changes.append({
                'column': col,
                'action': 'remove_outliers',
                'method': method,
                'bounds': [lower, upper],
                'outliers_removed': int(outlier_count)
            })
        
        elif action == 'cap':
            df_clean.loc[df_clean[col] < lower, col] = lower
            df_clean.loc[df_clean[col] > upper, col] = upper
            changes.append({
                'column': col,
                'action': 'cap_outliers',
                'method': method,
                'bounds': [lower, upper],
                'outliers_capped': int(outlier_count)
            })
        
        elif action == 'nan':
            df_clean.loc[outliers_mask, col] = np.nan
            changes.append({
                'column': col,
                'action': 'nan_outliers',
                'method': method,
                'bounds': [lower, upper],
                'outliers_replaced': int(outlier_count)
            })
    
    return df_clean, changes


def handle_dtypes(df: pd.DataFrame, conversions: dict = None,
                  auto_detect: bool = True) -> tuple:
    """
    å‹å¤‰æ›
    
    conversions: {'column_name': 'target_type'}
    target_type: datetime, numeric, category, string
    """
    df_clean = df.copy()
    changes = []
    
    if auto_detect:
        # æ—¥ä»˜åˆ—ã®è‡ªå‹•æ¤œå‡ºã¨å¤‰æ›
        date_keywords = ['date', 'time', 'created', 'updated', 'timestamp']
        for col in df_clean.select_dtypes(include=['object']).columns:
            if any(kw in col.lower() for kw in date_keywords):
                try:
                    df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
                    changes.append({
                        'column': col,
                        'action': 'convert_datetime',
                        'from': 'object',
                        'to': 'datetime64'
                    })
                except:
                    pass
        
        # æ•°å€¤ã¨ã—ã¦è§£é‡ˆå¯èƒ½ãªæ–‡å­—åˆ—åˆ—
        for col in df_clean.select_dtypes(include=['object']).columns:
            try:
                numeric_col = pd.to_numeric(df_clean[col], errors='coerce')
                if numeric_col.notna().sum() / len(df_clean) > 0.9:
                    df_clean[col] = numeric_col
                    changes.append({
                        'column': col,
                        'action': 'convert_numeric',
                        'from': 'object',
                        'to': str(numeric_col.dtype)
                    })
            except:
                pass
    
    if conversions:
        for col, target_type in conversions.items():
            if col not in df_clean.columns:
                continue
            
            original_type = str(df_clean[col].dtype)
            
            if target_type == 'datetime':
                df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')
            elif target_type == 'numeric':
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
            elif target_type == 'category':
                df_clean[col] = df_clean[col].astype('category')
            elif target_type == 'string':
                df_clean[col] = df_clean[col].astype(str)
            
            changes.append({
                'column': col,
                'action': f'convert_{target_type}',
                'from': original_type,
                'to': str(df_clean[col].dtype)
            })
    
    return df_clean, changes


def auto_clean(df: pd.DataFrame, config: dict = None) -> tuple:
    """
    è‡ªå‹•ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å…¨å‡¦ç†ã‚’å®Ÿè¡Œ
    """
    if config is None:
        config = {
            'missing_strategy': 'auto',
            'remove_duplicates': True,
            'handle_outliers': False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ç„¡åŠ¹
            'auto_dtypes': True
        }
    
    all_changes = []
    df_clean = df.copy()
    
    # 1. å‹å¤‰æ›
    if config.get('auto_dtypes', True):
        df_clean, changes = handle_dtypes(df_clean, auto_detect=True)
        all_changes.extend(changes)
    
    # 2. é‡è¤‡å‰Šé™¤
    if config.get('remove_duplicates', True):
        df_clean, changes = handle_duplicates(df_clean)
        all_changes.extend(changes)
    
    # 3. æ¬ æå€¤å‡¦ç†
    if config.get('missing_strategy'):
        df_clean, changes = handle_missing(df_clean, strategy=config['missing_strategy'])
        all_changes.extend(changes)
    
    # 4. å¤–ã‚Œå€¤å‡¦ç†
    if config.get('handle_outliers', False):
        df_clean, changes = handle_outliers(df_clean, action='cap')
        all_changes.extend(changes)
    
    return df_clean, all_changes


def print_summary(original_df: pd.DataFrame, clean_df: pd.DataFrame, changes: list):
    """ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°çµæœã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›"""
    print("=" * 60)
    print("ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°çµæœ")
    print("=" * 60)
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå¤‰åŒ–")
    print(f"  è¡Œ: {len(original_df):,} â†’ {len(clean_df):,} ({len(clean_df) - len(original_df):+,})")
    print(f"  åˆ—: {len(original_df.columns)} â†’ {len(clean_df.columns)}")
    
    print(f"\nğŸ”§ å®Ÿè¡Œã•ã‚ŒãŸå‡¦ç†")
    for change in changes:
        action = change.get('action', 'unknown')
        col = change.get('column', 'all')
        
        if 'fill' in action:
            print(f"  âœ“ [{col}] {action}: {change.get('filled', 0)}ä»¶ã‚’è£œå®Œ")
        elif 'remove' in action or 'drop' in action:
            removed = change.get('rows_removed', change.get('outliers_removed', 0))
            print(f"  âœ“ [{col}] {action}: {removed}ä»¶ã‚’å‰Šé™¤")
        elif 'convert' in action:
            print(f"  âœ“ [{col}] {action}: {change.get('from')} â†’ {change.get('to')}")
        elif 'cap' in action:
            print(f"  âœ“ [{col}] {action}: {change.get('outliers_capped', 0)}ä»¶ã‚’ã‚­ãƒ£ãƒƒãƒ—")
        else:
            print(f"  âœ“ {action}")
    
    print(f"\nğŸ“ˆ æ¬ æå€¤å¤‰åŒ–")
    print(f"  ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‰: {original_df.isnull().sum().sum():,}")
    print(f"  ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {clean_df.isnull().sum().sum():,}")
    
    print("\n" + "=" * 60)
    print("âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°')
    parser.add_argument('file_path', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--action', '-a', required=True,
                        choices=['missing', 'duplicates', 'outliers', 'dtypes', 'all'],
                        help='å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--strategy', '-s', default='auto',
                        help='æ¬ æå€¤å‡¦ç†æˆ¦ç•¥')
    parser.add_argument('--columns', '-c', nargs='+', help='å¯¾è±¡åˆ—')
    parser.add_argument('--keep', default='first', choices=['first', 'last', False],
                        help='é‡è¤‡å‡¦ç†æ™‚ã®ä¿æŒæ–¹æ³•')
    parser.add_argument('--method', '-m', default='iqr', choices=['iqr', 'zscore'],
                        help='å¤–ã‚Œå€¤æ¤œå‡ºæ–¹æ³•')
    parser.add_argument('--outlier-action', default='remove', choices=['remove', 'cap', 'nan'],
                        help='å¤–ã‚Œå€¤å‡¦ç†æ–¹æ³•')
    parser.add_argument('--json', action='store_true', help='JSONå½¢å¼ã§å‡ºåŠ›')
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {args.file_path}")
    df_original = load_data(args.file_path)
    print(f"âœ… {len(df_original):,}è¡Œ Ã— {len(df_original.columns)}åˆ—")
    
    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    if args.action == 'missing':
        df_clean, changes = handle_missing(df_original, strategy=args.strategy, columns=args.columns)
    elif args.action == 'duplicates':
        df_clean, changes = handle_duplicates(df_original, subset=args.columns, keep=args.keep)
    elif args.action == 'outliers':
        df_clean, changes = handle_outliers(df_original, columns=args.columns, 
                                           method=args.method, action=args.outlier_action)
    elif args.action == 'dtypes':
        df_clean, changes = handle_dtypes(df_original, auto_detect=True)
    elif args.action == 'all':
        df_clean, changes = auto_clean(df_original)
    
    # çµæœå‡ºåŠ›
    if args.json:
        result = {
            'original_shape': list(df_original.shape),
            'clean_shape': list(df_clean.shape),
            'changes': changes
        }
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        print_summary(df_original, df_clean, changes)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    if args.output:
        save_data(df_clean, args.output)
        print(f"\nğŸ“„ ä¿å­˜å®Œäº†: {args.output}")
    
    return df_clean


if __name__ == '__main__':
    main()
