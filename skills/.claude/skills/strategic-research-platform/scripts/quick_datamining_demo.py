"""
Strategic Management Research Hub - Quick Start Script
======================================================

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã‚’æœ€é€Ÿã§é–‹å§‹ã™ã‚‹ãŸã‚ã®
ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    python quick_datamining_demo.py

å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«:
    - ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆCSV, Stata, Excelç­‰ï¼‰
    - datamining_config.yamlï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

å‡ºåŠ›:
    - ./demo_output/ ä»¥ä¸‹ã«å…¨çµæœãŒä¿å­˜ã•ã‚Œã¾ã™

Author: Strategic Management Research Hub v3.1
Date: 2025-11-01
"""

import sys
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import pandas as pd
import numpy as np

# ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(SCRIPT_DIR))

# æœ¬ã‚¹ã‚­ãƒ«ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from advanced_strategic_datamining import AdvancedStrategicDataMining
from ml_causal_inference_integrated import CausalMLIntegration


def print_banner():
    """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒãƒŠãƒ¼è¡¨ç¤º"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   Strategic Management Research Hub v3.1                      â•‘
    â•‘   Quick Start: Data Mining Demo                              â•‘
    â•‘                                                               â•‘
    â•‘   This demo will run comprehensive data mining analyses      â•‘
    â•‘   on your panel data in just a few minutes!                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def generate_sample_data(n_firms=100, n_years=10):
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    
    å®Ÿéš›ã®ç ”ç©¶ã§ã¯ã€è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
    """
    logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    
    np.random.seed(42)
    
    data = []
    for firm_id in range(n_firms):
        # ä¼æ¥­å›ºæœ‰åŠ¹æœ
        firm_effect = np.random.normal(0, 0.02)
        base_rd = np.random.exponential(0.05)
        
        for year in range(2010, 2010 + n_years):
            # æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
            time_effect = (year - 2010) * 0.001
            
            # æˆ¦ç•¥å¤‰æ•°
            rd_intensity = max(0, base_rd + np.random.normal(0, 0.01))
            capital_intensity = np.random.uniform(0.2, 0.8)
            advertising_intensity = np.random.exponential(0.03)
            international_sales = np.random.uniform(0, 0.5)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆç†è«–çš„é–¢ä¿‚ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰
            roa = (
                0.05  # ãƒ™ãƒ¼ã‚¹
                + 0.3 * rd_intensity  # R&DåŠ¹æœï¼ˆRBVï¼‰
                - 0.05 * capital_intensity  # è³‡æœ¬é›†ç´„åº¦
                + 0.1 * advertising_intensity  # å·®åˆ¥åŒ–
                + firm_effect  # ä¼æ¥­å›ºæœ‰
                + time_effect  # æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
                + np.random.normal(0, 0.02)  # ãƒã‚¤ã‚º
            )
            
            data.append({
                'firm_id': f'FIRM_{firm_id:03d}',
                'year': year,
                'roa': roa,
                'rd_intensity': rd_intensity,
                'capital_intensity': capital_intensity,
                'advertising_intensity': advertising_intensity,
                'international_sales': international_sales,
                'firm_size': np.random.normal(10, 2),
                'leverage': np.random.uniform(0.2, 0.6),
                'firm_age': np.random.randint(5, 50),
                'ma_dummy': np.random.binomial(1, 0.15)  # M&Aå®Ÿæ–½ãƒ€ãƒŸãƒ¼
            })
    
    df = pd.DataFrame(data)
    
    # å°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    df['roa_lead1'] = df.groupby('firm_id')['roa'].shift(-1)
    df['roa_lead2'] = df.groupby('firm_id')['roa'].shift(-2)
    
    # ROAå¤‰åŒ–
    df['roa_change'] = df.groupby('firm_id')['roa'].diff()
    
    logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)} observations, {df['firm_id'].nunique()} firms")
    
    return df


def run_quick_demo(data_path=None, output_dir='./demo_output/'):
    """
    ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢å®Ÿè¡Œ
    
    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    print_banner()
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    logger.info("=" * 60)
    logger.info("STEP 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    logger.info("=" * 60)
    
    if data_path is None:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        df_panel = generate_sample_data(n_firms=100, n_years=10)
        logger.info("â†’ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    else:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if data_path.endswith('.dta'):
            df_panel = pd.read_stata(data_path)
        elif data_path.endswith('.csv'):
            df_panel = pd.read_csv(data_path)
        elif data_path.endswith(('.xls', '.xlsx')):
            df_panel = pd.read_excel(data_path)
        else:
            raise ValueError(f"Unsupported file format: {data_path}")
        
        logger.info(f"â†’ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {data_path}")
    
    logger.info(f"   {len(df_panel)} observations, {df_panel['firm_id'].nunique()} firms")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    logger.info("\n" + "=" * 60)
    logger.info("STEP 2: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–")
    logger.info("=" * 60)
    
    dm = AdvancedStrategicDataMining(
        data=df_panel,
        firm_id='firm_id',
        time_var='year',
        output_dir=output_dir,
        random_state=42
    )
    
    logger.info("â†’ åˆæœŸåŒ–å®Œäº†")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
    logger.info("\n" + "=" * 60)
    logger.info("STEP 3: æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆStrategic Group Analysisï¼‰")
    logger.info("=" * 60)
    
    try:
        strategic_features = [
            'rd_intensity',
            'capital_intensity',
            'advertising_intensity',
            'international_sales'
        ]
        
        groups = dm.strategic_group_analysis(
            features=strategic_features,
            n_clusters=4,
            method='kmeans',
            save_results=True
        )
        
        logger.info("â†’ âœ… æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æå®Œäº†")
        logger.info(f"   {groups['n_clusters']} ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®š")
        logger.info(f"   Silhouette Score: {groups['validation_metrics'].get('silhouette', 'N/A')}")
    
    except Exception as e:
        logger.error(f"â†’ âŒ æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
    logger.info("\n" + "=" * 60)
    logger.info("STEP 4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ï¼ˆPerformance Predictionï¼‰")
    logger.info("=" * 60)
    
    try:
        performance_features = [
            'rd_intensity',
            'firm_size',
            'leverage',
            'firm_age'
        ]
        
        predictions = dm.predict_firm_performance(
            target='roa',
            features=performance_features,
            model_type='ensemble',
            test_size=0.2,
            cv_folds=5,
            save_model=True
        )
        
        best_model = predictions['best_model']
        test_r2 = predictions['all_results'][best_model]['metrics']['test_r2']
        
        logger.info("â†’ âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬å®Œäº†")
        logger.info(f"   Best Model: {best_model}")
        logger.info(f"   Test RÂ²: {test_r2:.4f}")
    
    except Exception as e:
        logger.error(f"â†’ âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—5: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    logger.info("\n" + "=" * 60)
    logger.info("STEP 5: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆFeature Importanceï¼‰")
    logger.info("=" * 60)
    
    try:
        importance = dm.analyze_feature_importance(
            target='roa',
            features=performance_features,
            method='ensemble',
            top_n=10
        )
        
        logger.info("â†’ âœ… ç‰¹å¾´é‡é‡è¦åº¦åˆ†æå®Œäº†")
        logger.info(f"\n   Top 3 Most Important Features:")
        for idx, row in importance.head(3).iterrows():
            logger.info(f"   - {row['feature']}: {row.get('ensemble_importance', row.iloc[1]):.4f}")
    
    except Exception as e:
        logger.error(f"â†’ âŒ ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—6: ç•°å¸¸æ¤œçŸ¥
    logger.info("\n" + "=" * 60)
    logger.info("STEP 6: ç•°å¸¸æ¤œçŸ¥ï¼ˆAnomaly Detectionï¼‰")
    logger.info("=" * 60)
    
    try:
        outliers = dm.detect_strategic_outliers(
            features=['roa', 'rd_intensity', 'leverage'],
            method='ensemble',
            contamination=0.05,
            save_results=True
        )
        
        logger.info("â†’ âœ… ç•°å¸¸æ¤œçŸ¥å®Œäº†")
        logger.info(f"   {len(outliers)} ç¤¾ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ä¼æ¥­ã‚’æ¤œå‡º")
    
    except Exception as e:
        logger.error(f"â†’ âŒ ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—7: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    logger.info("\n" + "=" * 60)
    logger.info("STEP 7: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    logger.info("=" * 60)
    
    try:
        report_path = dm.generate_comprehensive_report()
        
        logger.info("â†’ âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        logger.info(f"   ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    
    except Exception as e:
        logger.error(f"â†’ âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ‰ ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æå®Œäº†ï¼")
    logger.info("=" * 60)
    logger.info(f"\nğŸ“ ã™ã¹ã¦ã®çµæœ: {output_dir}")
    logger.info(f"ğŸ“Š HTMLãƒ¬ãƒãƒ¼ãƒˆ: {output_dir}/datamining_report.html")
    logger.info(f"ğŸ“ˆ å›³è¡¨: {output_dir}/*.png")
    logger.info(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿: {output_dir}/*.xlsx")
    
    print("\n" + "="*60)
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
    print("2. ç”Ÿæˆã•ã‚ŒãŸå›³è¡¨ã‚’ç¢ºèª")
    print("3. DATAMINING_GUIDE.mdã§è©³ç´°ãªä½¿ã„æ–¹ã‚’å­¦ã¶")
    print("4. è‡ªåˆ†ã®ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã§åˆ†æã‚’å®Ÿè¡Œ")
    print("="*60 + "\n")


def run_causal_demo(data_path=None, output_dir='./causal_demo_output/'):
    """
    å› æœæ¨è«–ãƒ‡ãƒ¢å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    EconMLãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å®Ÿè¡Œå¯èƒ½
    """
    logger.info("\n" + "=" * 60)
    logger.info("å› æœæ¨è«–ãƒ‡ãƒ¢ï¼ˆCausal Inference Demoï¼‰")
    logger.info("=" * 60)
    
    try:
        from econml.dml import CausalForestDML
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        if data_path is None:
            df_panel = generate_sample_data(n_firms=150, n_years=10)
        else:
            if data_path.endswith('.dta'):
                df_panel = pd.read_stata(data_path)
            elif data_path.endswith('.csv'):
                df_panel = pd.read_csv(data_path)
        
        # å› æœæ¨è«–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        causal = CausalMLIntegration(
            data=df_panel,
            firm_id='firm_id',
            time_var='year',
            output_dir=output_dir
        )
        
        # Causal Forestï¼ˆç•°è³ªçš„å‡¦ç½®åŠ¹æœï¼‰
        logger.info("\nå®Ÿè¡Œä¸­: Causal Forest...")
        cf_results = causal.causal_forest(
            treatment='ma_dummy',
            outcome='roa_change',
            controls=['firm_size', 'leverage', 'firm_age'],
            heterogeneity_vars=['firm_size', 'rd_intensity'],
            discrete_treatment=True,
            n_estimators=50  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚
        )
        
        logger.info("â†’ âœ… Causal Forestå®Œäº†")
        logger.info(f"   ATE: {cf_results['ate']:.4f}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        causal_report = causal.generate_causal_report()
        logger.info(f"\nğŸ“Š å› æœæ¨è«–ãƒ¬ãƒãƒ¼ãƒˆ: {causal_report}")
    
    except ImportError:
        logger.warning("âŒ EconMLãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        logger.warning("   å› æœæ¨è«–ãƒ‡ãƒ¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.warning("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install econml")
    
    except Exception as e:
        logger.error(f"âŒ å› æœæ¨è«–ãƒ‡ãƒ¢ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
    
    ä½¿ç”¨ä¾‹:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‡ãƒ¢å®Ÿè¡Œ
        python quick_datamining_demo.py
        
        # è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
        python quick_datamining_demo.py --data ./my_data.csv
    """
    
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Strategic Management Data Mining Quick Start'
    )
    parser.add_argument(
        '--data',
        type=str,
        default=None,
        help='Path to your data file (CSV, Stata, Excel)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='./demo_output/',
        help='Output directory for results'
    )
    parser.add_argument(
        '--causal',
        action='store_true',
        help='Run causal inference demo (requires econml)'
    )
    
    args = parser.parse_args()
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œ
    run_quick_demo(
        data_path=args.data,
        output_dir=args.output
    )
    
    # å› æœæ¨è«–ãƒ‡ãƒ¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if args.causal:
        run_causal_demo(
            data_path=args.data,
            output_dir=os.path.join(args.output, 'causal/')
        )
    
    print("\nâœ… All done! Happy researching! ğŸš€\n")
