"""
Strategic Management Research Hub - Comprehensive Data Mining Pipeline
=======================================================================

å®Œå…¨è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚ç ”ç©¶è€…ãŒã‚³ãƒãƒ³ãƒ‰1ã¤ã§
Publication-Ready ãªåˆ†æçµæœã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã«è¨­è¨ˆã€‚

ä¸»è¦æ©Ÿèƒ½ï¼š
1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†å‰å‡¦ç†ã®è‡ªå‹•åŒ–
2. 8ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®çµ±åˆå®Ÿè¡Œ
3. çµæœã®å¯è¦–åŒ–ï¼ˆ30ç¨®é¡ä»¥ä¸Šã®ã‚°ãƒ©ãƒ•ï¼‰
4. HTMLãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ
5. Publication-Ready ãªLaTeXè¡¨ã®å‡ºåŠ›
6. å†ç¾æ€§ã®å®Œå…¨ç¢ºä¿ï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜ï¼‰

ä½¿ç”¨ä¾‹ï¼š
```python
from comprehensive_datamining_pipeline import ComprehensiveDataMiningPipeline

# åˆæœŸåŒ–
pipeline = ComprehensiveDataMiningPipeline(
    data_path='./data/final/analysis_panel.dta',
    config_path='./configs/datamining_config.yaml',
    output_dir='./datamining_results/'
)

# å…¨è‡ªå‹•å®Ÿè¡Œ
pipeline.run_complete_analysis()

# ã¾ãŸã¯æ®µéšçš„å®Ÿè¡Œ
pipeline.load_and_validate_data()
pipeline.run_strategic_group_analysis()
pipeline.run_performance_prediction()
pipeline.run_causal_inference()
pipeline.generate_comprehensive_report()
```

Author: Strategic Management Research Hub v3.1
Version: 3.1
Date: 2025-11-01
License: MIT
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Union, Any
import logging
from pathlib import Path
import warnings
import json
import yaml
from datetime import datetime
import pickle
import sys
from dataclasses import dataclass, asdict
import traceback

# Core ML Libraries
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    silhouette_score, mean_squared_error, r2_score, 
    roc_auc_score, classification_report
)
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.ensemble import (
    RandomForestRegressor, RandomForestClassifier,
    GradientBoostingRegressor, IsolationForest
)

# Advanced ML (if available)
try:
    import xgboost as xgb
    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False

try:
    import lightgbm as lgb
    LGB_AVAILABLE = True
except ImportError:
    LGB_AVAILABLE = False

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

try:
    from econml.dml import DML, LinearDML, CausalForestDML
    from econml.dr import DRLearner
    ECONML_AVAILABLE = True
except ImportError:
    ECONML_AVAILABLE = False

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("Set2")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('datamining_pipeline.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION DATA CLASS
# ============================================================================

@dataclass
class DataMiningConfig:
    """ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­å®š"""
    
    # ãƒ‡ãƒ¼ã‚¿è¨­å®š
    data_path: str
    firm_id: str = 'gvkey'
    time_var: str = 'year'
    output_dir: str = './datamining_output/'
    
    # åˆ†æå¯¾è±¡å¤‰æ•°
    strategic_features: List[str] = None
    performance_target: str = 'roa'
    control_variables: List[str] = None
    
    # æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
    n_clusters: Optional[int] = None  # Noneã®å ´åˆã¯è‡ªå‹•æ±ºå®š
    clustering_method: str = 'kmeans'
    max_clusters: int = 10
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
    prediction_models: List[str] = None
    test_size: float = 0.2
    cv_folds: int = 5
    
    # å› æœæ¨è«–
    treatment_var: str = None
    outcome_var: str = None
    causal_method: str = 'dml'
    
    # ç•°å¸¸æ¤œçŸ¥
    contamination: float = 0.05
    
    # å“è³ªç®¡ç†
    min_observations: int = 100
    max_missing_rate: float = 0.3
    outlier_method: str = 'iqr'
    outlier_threshold: float = 3.0
    
    # å‡ºåŠ›è¨­å®š
    save_models: bool = True
    generate_html_report: bool = True
    generate_latex_tables: bool = True
    figure_format: str = 'png'
    figure_dpi: int = 300
    
    # è¨ˆç®—è¨­å®š
    random_seed: int = 42
    n_jobs: int = -1
    verbose: bool = True
    
    def __post_init__(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š"""
        if self.strategic_features is None:
            self.strategic_features = [
                'rd_intensity', 'capital_intensity', 
                'advertising_intensity', 'international_sales'
            ]
        if self.control_variables is None:
            self.control_variables = [
                'firm_size', 'firm_age', 'leverage', 'sales_growth'
            ]
        if self.prediction_models is None:
            self.prediction_models = [
                'rf', 'gbm', 'xgboost', 'lightgbm', 'ensemble'
            ]
    
    @classmethod
    def from_yaml(cls, yaml_path: str):
        """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        with open(yaml_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)
    
    def to_yaml(self, yaml_path: str):
        """è¨­å®šã‚’YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(asdict(self), f, default_flow_style=False)


# ============================================================================
# MAIN PIPELINE CLASS
# ============================================================================

class ComprehensiveDataMiningPipeline:
    """
    æˆ¦ç•¥çµŒå–¶ç ”ç©¶ã®ãŸã‚ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ã€8ã¤ã®ä¸»è¦åˆ†æã€‘
    1. ãƒ‡ãƒ¼ã‚¿å“è³ªè¨ºæ–­
    2. æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆStrategic Group Analysisï¼‰
    3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ï¼ˆPerformance Predictionï¼‰
    4. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æï¼ˆFeature Importanceï¼‰
    5. ç•°å¸¸æ¤œçŸ¥ï¼ˆAnomaly Detectionï¼‰
    6. å› æœæ¨è«–ï¼ˆCausal Inferenceï¼‰
    7. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆTemporal Pattern Analysisï¼‰
    8. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆComprehensive Reportingï¼‰
    """
    
    def __init__(
        self,
        data_path: str = None,
        config_path: str = None,
        config: DataMiningConfig = None,
        output_dir: str = './datamining_output/'
    ):
        """
        Parameters
        ----------
        data_path : str, optional
            ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        config_path : str, optional
            è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆYAMLï¼‰ã®ãƒ‘ã‚¹
        config : DataMiningConfig, optional
            è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆç›´æ¥æ¸¡ã™å ´åˆï¼‰
        output_dir : str, optional
            å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # è¨­å®šã®èª­ã¿è¾¼ã¿
        if config is not None:
            self.config = config
        elif config_path is not None:
            self.config = DataMiningConfig.from_yaml(config_path)
        elif data_path is not None:
            self.config = DataMiningConfig(data_path=data_path, output_dir=output_dir)
        else:
            raise ValueError("data_path, config_path, ã¾ãŸã¯ config ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'figures').mkdir(exist_ok=True)
        (self.output_dir / 'tables').mkdir(exist_ok=True)
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'logs').mkdir(exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®æ ¼ç´
        self.data = None
        self.data_cleaned = None
        self.results = {}
        self.models = {}
        
        # å®Ÿè¡Œæ™‚é–“ã®è¨˜éŒ²
        self.execution_times = {}
        
        # ãƒ­ã‚°ã®è¨­å®š
        self._setup_logging()
        
        logger.info(f"ComprehensiveDataMiningPipeline åˆæœŸåŒ–å®Œäº†")
        logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_file = self.output_dir / 'logs' / f'pipeline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    # ========================================================================
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†æ¤œè¨¼
    # ========================================================================
    
    def load_and_validate_data(self) -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çš„ãªæ¤œè¨¼
        
        Returns
        -------
        pd.DataFrame
            èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼†æ¤œè¨¼")
        logger.info("=" * 80)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            data_path = Path(self.config.data_path)
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_path}")
            
            if data_path.suffix == '.dta':
                self.data = pd.read_stata(data_path)
            elif data_path.suffix == '.csv':
                self.data = pd.read_csv(data_path)
            elif data_path.suffix == '.parquet':
                self.data = pd.read_parquet(data_path)
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {data_path.suffix}")
            
            logger.info(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {self.data.shape[0]:,} è¡Œ Ã— {self.data.shape[1]:,} åˆ—")
            
            # åŸºæœ¬çš„ãªæ¤œè¨¼
            self._validate_data()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            self.data_cleaned = self._clean_data(self.data.copy())
            
            # çµæœã®ä¿å­˜
            self.results['data_summary'] = {
                'n_observations': len(self.data),
                'n_firms': self.data[self.config.firm_id].nunique(),
                'n_years': self.data[self.config.time_var].nunique(),
                'year_range': [
                    int(self.data[self.config.time_var].min()), 
                    int(self.data[self.config.time_var].max())
                ],
                'variables': list(self.data.columns)
            }
            
            self.execution_times['data_loading'] = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ PHASE 1 å®Œäº† ({self.execution_times['data_loading']:.2f}ç§’)")
            
            return self.data_cleaned
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _validate_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çš„ãªæ¤œè¨¼"""
        logger.info("ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
        
        # å¿…é ˆå¤‰æ•°ã®ç¢ºèª
        required_vars = [self.config.firm_id, self.config.time_var]
        missing_vars = [v for v in required_vars if v not in self.data.columns]
        if missing_vars:
            raise ValueError(f"å¿…é ˆå¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_vars}")
        
        # æœ€å°è¦³æ¸¬æ•°ã®ç¢ºèª
        if len(self.data) < self.config.min_observations:
            logger.warning(
                f"è¦³æ¸¬æ•°ãŒå°‘ãªã™ãã¾ã™: {len(self.data)} < {self.config.min_observations}"
            )
        
        # æ¬ æå€¤ã®ç¢ºèª
        missing_rates = self.data.isnull().sum() / len(self.data)
        high_missing = missing_rates[missing_rates > self.config.max_missing_rate]
        if len(high_missing) > 0:
            logger.warning(f"æ¬ æå€¤ã®å¤šã„å¤‰æ•°: {len(high_missing)} å€‹")
            for var, rate in high_missing.items():
                logger.warning(f"  {var}: {rate:.1%}")
        
        logger.info("âœ“ ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å®Œäº†")
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
        
        df_clean = df.copy()
        
        # 1. å¤–ã‚Œå€¤å‡¦ç†
        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col in [self.config.firm_id, self.config.time_var]:
                continue
            
            if self.config.outlier_method == 'iqr':
                Q1 = df_clean[col].quantile(0.25)
                Q3 = df_clean[col].quantile(0.75)
                IQR = Q3 - Q1
                lower = Q1 - self.config.outlier_threshold * IQR
                upper = Q3 + self.config.outlier_threshold * IQR
                df_clean[col] = df_clean[col].clip(lower, upper)
            
            elif self.config.outlier_method == 'zscore':
                mean = df_clean[col].mean()
                std = df_clean[col].std()
                lower = mean - self.config.outlier_threshold * std
                upper = mean + self.config.outlier_threshold * std
                df_clean[col] = df_clean[col].clip(lower, upper)
        
        # 2. æ¬ æå€¤å‡¦ç†ï¼ˆãƒªã‚¹ãƒˆãƒ¯ã‚¤ã‚ºå‰Šé™¤ï¼‰
        original_len = len(df_clean)
        analysis_vars = (
            [self.config.performance_target] + 
            self.config.strategic_features + 
            self.config.control_variables
        )
        analysis_vars = [v for v in analysis_vars if v in df_clean.columns]
        df_clean = df_clean.dropna(subset=analysis_vars)
        dropped = original_len - len(df_clean)
        
        if dropped > 0:
            logger.info(f"æ¬ æå€¤ã«ã‚ˆã‚‹å‰Šé™¤: {dropped:,} è¡Œ ({dropped/original_len:.1%})")
        
        logger.info(f"âœ“ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {len(df_clean):,} è¡Œ")
        
        return df_clean
    
    # ========================================================================
    # 2. æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
    # ========================================================================
    
    def run_strategic_group_analysis(
        self,
        features: List[str] = None,
        n_clusters: int = None,
        method: str = None
    ) -> Dict[str, Any]:
        """
        æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆStrategic Group Analysisï¼‰
        
        Porter (1980) ã®æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ç†è«–ã«åŸºã¥ãã€é¡ä¼¼ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹
        ä¼æ¥­ç¾¤ã‚’ç‰¹å®šã€‚
        
        Parameters
        ----------
        features : List[str], optional
            ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹æˆ¦ç•¥çš„æ¬¡å…ƒ
        n_clusters : int, optional
            ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰
        method : str, optional
            ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•
        
        Returns
        -------
        Dict[str, Any]
            åˆ†æçµæœ
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 2: æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ")
        logger.info("=" * 80)
        
        if self.data_cleaned is None:
            self.load_and_validate_data()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        features = features or self.config.strategic_features
        n_clusters = n_clusters or self.config.n_clusters
        method = method or self.config.clustering_method
        
        logger.info(f"æˆ¦ç•¥çš„æ¬¡å…ƒ: {features}")
        logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•: {method}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            X = self.data_cleaned[features].dropna()
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š
            if n_clusters is None:
                n_clusters = self._determine_optimal_clusters(
                    X_scaled, 
                    max_k=self.config.max_clusters
                )
                logger.info(f"æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters}")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            if method == 'kmeans':
                model = KMeans(
                    n_clusters=n_clusters, 
                    random_state=self.config.random_seed,
                    n_init=10
                )
            elif method == 'hierarchical':
                model = AgglomerativeClustering(n_clusters=n_clusters)
            elif method == 'dbscan':
                model = DBSCAN(eps=0.5, min_samples=5)
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•: {method}")
            
            cluster_labels = model.fit_predict(X_scaled)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
            cluster_profiles = self._create_cluster_profiles(
                X, cluster_labels, features
            )
            
            # çµæœã®ä¿å­˜
            self.results['strategic_groups'] = {
                'n_clusters': n_clusters,
                'method': method,
                'features': features,
                'cluster_labels': cluster_labels,
                'cluster_profiles': cluster_profiles,
                'silhouette_score': silhouette_score(X_scaled, cluster_labels)
            }
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            if self.config.save_models:
                model_path = self.output_dir / 'models' / 'strategic_groups_model.pkl'
                with open(model_path, 'wb') as f:
                    pickle.dump({'model': model, 'scaler': scaler}, f)
                logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
            
            # å¯è¦–åŒ–
            self._visualize_strategic_groups(X_scaled, cluster_labels, features)
            
            self.execution_times['strategic_groups'] = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ PHASE 2 å®Œäº† ({self.execution_times['strategic_groups']:.2f}ç§’)")
            
            return self.results['strategic_groups']
            
        except Exception as e:
            logger.error(f"æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _determine_optimal_clusters(
        self, 
        X: np.ndarray, 
        max_k: int = 10
    ) -> int:
        """ã‚¨ãƒ«ãƒœãƒ¼æ³•ã¨ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æã§æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ±ºå®š"""
        logger.info("æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¢ç´¢ä¸­...")
        
        silhouette_scores = []
        inertias = []
        
        for k in range(2, max_k + 1):
            kmeans = KMeans(n_clusters=k, random_state=self.config.random_seed, n_init=10)
            labels = kmeans.fit_predict(X)
            silhouette_scores.append(silhouette_score(X, labels))
            inertias.append(kmeans.inertia_)
        
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®k
        optimal_k = np.argmax(silhouette_scores) + 2
        
        logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•° {optimal_k} ã‚’é¸æŠï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {max(silhouette_scores):.3f}ï¼‰")
        
        return optimal_k
    
    def _create_cluster_profiles(
        self, 
        X: pd.DataFrame, 
        labels: np.ndarray, 
        features: List[str]
    ) -> pd.DataFrame:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ"""
        profiles = []
        
        for cluster_id in np.unique(labels):
            mask = labels == cluster_id
            cluster_data = X[mask]
            
            profile = {'cluster': cluster_id, 'size': int(mask.sum())}
            for feat in features:
                profile[f'{feat}_mean'] = cluster_data[feat].mean()
                profile[f'{feat}_std'] = cluster_data[feat].std()
            
            profiles.append(profile)
        
        return pd.DataFrame(profiles)
    
    def _visualize_strategic_groups(
        self, 
        X: np.ndarray, 
        labels: np.ndarray, 
        features: List[str]
    ):
        """æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¯è¦–åŒ–"""
        from sklearn.decomposition import PCA
        
        # PCAã§2æ¬¡å…ƒã«å‰Šæ¸›
        pca = PCA(n_components=2, random_state=self.config.random_seed)
        X_pca = pca.fit_transform(X)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        scatter = ax.scatter(
            X_pca[:, 0], X_pca[:, 1],
            c=labels, cmap='Set2', 
            s=50, alpha=0.6, edgecolors='k'
        )
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        ax.set_title('Strategic Groups (PCA Projection)')
        plt.colorbar(scatter, label='Cluster')
        plt.tight_layout()
        
        fig_path = self.output_dir / 'figures' / f'strategic_groups.{self.config.figure_format}'
        plt.savefig(fig_path, dpi=self.config.figure_dpi, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è¦–åŒ–ä¿å­˜: {fig_path}")
    
    # ========================================================================
    # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
    # ========================================================================
    
    def run_performance_prediction(
        self,
        target: str = None,
        features: List[str] = None,
        models: List[str] = None
    ) -> Dict[str, Any]:
        """
        ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬
        
        è¤‡æ•°ã®MLãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã€‚
        
        Parameters
        ----------
        target : str, optional
            äºˆæ¸¬å¯¾è±¡å¤‰æ•°
        features : List[str], optional
            èª¬æ˜å¤‰æ•°
        models : List[str], optional
            ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆ
        
        Returns
        -------
        Dict[str, Any]
            äºˆæ¸¬çµæœ
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬")
        logger.info("=" * 80)
        
        if self.data_cleaned is None:
            self.load_and_validate_data()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        target = target or self.config.performance_target
        features = features or (self.config.strategic_features + self.config.control_variables)
        models = models or self.config.prediction_models
        
        logger.info(f"äºˆæ¸¬å¯¾è±¡: {target}")
        logger.info(f"èª¬æ˜å¤‰æ•°: {len(features)} å€‹")
        logger.info(f"ãƒ¢ãƒ‡ãƒ«: {models}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            X = self.data_cleaned[features].dropna()
            y = self.data_cleaned.loc[X.index, target]
            
            # Train-Test Split
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.config.test_size,
                random_state=self.config.random_seed
            )
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            model_results = {}
            
            for model_name in models:
                logger.info(f"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­: {model_name}")
                
                if model_name == 'rf':
                    model = RandomForestRegressor(
                        n_estimators=100,
                        random_state=self.config.random_seed,
                        n_jobs=self.config.n_jobs
                    )
                elif model_name == 'gbm':
                    model = GradientBoostingRegressor(
                        n_estimators=100,
                        random_state=self.config.random_seed
                    )
                elif model_name == 'xgboost' and XGB_AVAILABLE:
                    model = xgb.XGBRegressor(
                        n_estimators=100,
                        random_state=self.config.random_seed,
                        n_jobs=self.config.n_jobs
                    )
                elif model_name == 'lightgbm' and LGB_AVAILABLE:
                    model = lgb.LGBMRegressor(
                        n_estimators=100,
                        random_state=self.config.random_seed,
                        n_jobs=self.config.n_jobs
                    )
                else:
                    logger.warning(f"ã‚¹ã‚­ãƒƒãƒ—: {model_name}")
                    continue
                
                # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                model.fit(X_train_scaled, y_train)
                
                # äºˆæ¸¬
                y_pred_train = model.predict(X_train_scaled)
                y_pred_test = model.predict(X_test_scaled)
                
                # è©•ä¾¡
                train_r2 = r2_score(y_train, y_pred_train)
                test_r2 = r2_score(y_test, y_pred_test)
                train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
                test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
                
                model_results[model_name] = {
                    'model': model,
                    'train_r2': train_r2,
                    'test_r2': test_r2,
                    'train_rmse': train_rmse,
                    'test_rmse': test_rmse
                }
                
                logger.info(f"  Train RÂ²: {train_r2:.4f}, Test RÂ²: {test_r2:.4f}")
            
            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
            best_model_name = max(model_results, key=lambda k: model_results[k]['test_r2'])
            logger.info(f"æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
            
            # çµæœã®ä¿å­˜
            self.results['performance_prediction'] = {
                'target': target,
                'features': features,
                'model_results': model_results,
                'best_model': best_model_name,
                'scaler': scaler
            }
            
            self.execution_times['performance_prediction'] = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ PHASE 3 å®Œäº† ({self.execution_times['performance_prediction']:.2f}ç§’)")
            
            return self.results['performance_prediction']
            
        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    # ========================================================================
    # 4. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    # ========================================================================
    
    def run_feature_importance_analysis(
        self,
        target: str = None,
        features: List[str] = None
    ) -> Dict[str, Any]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        
        ã©ã®æˆ¦ç•¥çš„å¤‰æ•°ãŒä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ã‚’åˆ†æã€‚
        
        Parameters
        ----------
        target : str, optional
            äºˆæ¸¬å¯¾è±¡å¤‰æ•°
        features : List[str], optional
            èª¬æ˜å¤‰æ•°
        
        Returns
        -------
        Dict[str, Any]
            ç‰¹å¾´é‡é‡è¦åº¦
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 4: ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
        logger.info("=" * 80)
        
        if 'performance_prediction' not in self.results:
            self.run_performance_prediction(target, features)
        
        try:
            # Random Forestãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
            if 'rf' in self.results['performance_prediction']['model_results']:
                rf_model = self.results['performance_prediction']['model_results']['rf']['model']
                features = self.results['performance_prediction']['features']
                
                importances = rf_model.feature_importances_
                feature_importance_df = pd.DataFrame({
                    'feature': features,
                    'importance': importances
                }).sort_values('importance', ascending=False)
                
                logger.info("Top 5 é‡è¦ç‰¹å¾´é‡:")
                for idx, row in feature_importance_df.head(5).iterrows():
                    logger.info(f"  {row['feature']}: {row['importance']:.4f}")
                
                # å¯è¦–åŒ–
                self._visualize_feature_importance(feature_importance_df)
                
                self.results['feature_importance'] = {
                    'importance_df': feature_importance_df,
                    'top_features': feature_importance_df.head(10)['feature'].tolist()
                }
                
                self.execution_times['feature_importance'] = (datetime.now() - start_time).total_seconds()
                logger.info(f"âœ“ PHASE 4 å®Œäº† ({self.execution_times['feature_importance']:.2f}ç§’)")
                
                return self.results['feature_importance']
            else:
                logger.warning("Random Forestãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}
                
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _visualize_feature_importance(self, importance_df: pd.DataFrame):
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å¯è¦–åŒ–"""
        top_n = min(15, len(importance_df))
        
        fig, ax = plt.subplots(figsize=(10, 8))
        importance_df.head(top_n).plot(
            x='feature', y='importance', kind='barh', ax=ax,
            color='steelblue', edgecolor='black'
        )
        ax.set_xlabel('Feature Importance')
        ax.set_ylabel('Features')
        ax.set_title(f'Top {top_n} Feature Importance (Random Forest)')
        ax.invert_yaxis()
        plt.tight_layout()
        
        fig_path = self.output_dir / 'figures' / f'feature_importance.{self.config.figure_format}'
        plt.savefig(fig_path, dpi=self.config.figure_dpi, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è¦–åŒ–ä¿å­˜: {fig_path}")
    
    # ========================================================================
    # 5. ç•°å¸¸æ¤œçŸ¥
    # ========================================================================
    
    def run_anomaly_detection(
        self,
        features: List[str] = None,
        contamination: float = None
    ) -> Dict[str, Any]:
        """
        ç•°å¸¸æ¤œçŸ¥ï¼ˆAnomaly Detectionï¼‰
        
        æˆ¦ç•¥çš„ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ï¼ˆæ¥µç«¯ã«ç•°ãªã‚‹æˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹ä¼æ¥­ï¼‰ã‚’ç‰¹å®šã€‚
        
        Parameters
        ----------
        features : List[str], optional
            ç•°å¸¸æ¤œçŸ¥ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡
        contamination : float, optional
            ç•°å¸¸å€¤ã®å‰²åˆ
        
        Returns
        -------
        Dict[str, Any]
            ç•°å¸¸æ¤œçŸ¥çµæœ
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 5: ç•°å¸¸æ¤œçŸ¥")
        logger.info("=" * 80)
        
        if self.data_cleaned is None:
            self.load_and_validate_data()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        features = features or (self.config.strategic_features + self.config.control_variables)
        contamination = contamination or self.config.contamination
        
        logger.info(f"ç•°å¸¸æ¤œçŸ¥ç‰¹å¾´é‡: {len(features)} å€‹")
        logger.info(f"ç•°å¸¸å€¤å‰²åˆ: {contamination:.1%}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            X = self.data_cleaned[features].dropna()
            
            # æ¨™æº–åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Isolation Forest
            iso_forest = IsolationForest(
                contamination=contamination,
                random_state=self.config.random_seed,
                n_jobs=self.config.n_jobs
            )
            anomaly_labels = iso_forest.fit_predict(X_scaled)
            anomaly_scores = iso_forest.score_samples(X_scaled)
            
            # ç•°å¸¸å€¤ã®ç‰¹å®š
            is_anomaly = anomaly_labels == -1
            n_anomalies = is_anomaly.sum()
            
            logger.info(f"æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤: {n_anomalies:,} å€‹ ({n_anomalies/len(X):.1%})")
            
            # çµæœã®ä¿å­˜
            anomaly_df = X.copy()
            anomaly_df['is_anomaly'] = is_anomaly
            anomaly_df['anomaly_score'] = anomaly_scores
            
            self.results['anomaly_detection'] = {
                'anomaly_df': anomaly_df,
                'n_anomalies': int(n_anomalies),
                'anomaly_rate': float(n_anomalies / len(X)),
                'top_anomalies': anomaly_df[is_anomaly].nsmallest(10, 'anomaly_score')
            }
            
            # å¯è¦–åŒ–
            self._visualize_anomalies(X_scaled, is_anomaly)
            
            self.execution_times['anomaly_detection'] = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ PHASE 5 å®Œäº† ({self.execution_times['anomaly_detection']:.2f}ç§’)")
            
            return self.results['anomaly_detection']
            
        except Exception as e:
            logger.error(f"ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _visualize_anomalies(self, X: np.ndarray, is_anomaly: np.ndarray):
        """ç•°å¸¸å€¤ã®å¯è¦–åŒ–"""
        from sklearn.decomposition import PCA
        
        # PCAã§2æ¬¡å…ƒã«å‰Šæ¸›
        pca = PCA(n_components=2, random_state=self.config.random_seed)
        X_pca = pca.fit_transform(X)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # æ­£å¸¸å€¤
        ax.scatter(
            X_pca[~is_anomaly, 0], X_pca[~is_anomaly, 1],
            c='lightblue', s=30, alpha=0.5, label='Normal'
        )
        
        # ç•°å¸¸å€¤
        ax.scatter(
            X_pca[is_anomaly, 0], X_pca[is_anomaly, 1],
            c='red', s=100, alpha=0.7, marker='x', label='Anomaly'
        )
        
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
        ax.set_title('Anomaly Detection (PCA Projection)')
        ax.legend()
        plt.tight_layout()
        
        fig_path = self.output_dir / 'figures' / f'anomaly_detection.{self.config.figure_format}'
        plt.savefig(fig_path, dpi=self.config.figure_dpi, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è¦–åŒ–ä¿å­˜: {fig_path}")
    
    # ========================================================================
    # 6. å› æœæ¨è«–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ========================================================================
    
    def run_causal_inference(
        self,
        treatment_var: str = None,
        outcome_var: str = None,
        control_vars: List[str] = None,
        method: str = None
    ) -> Dict[str, Any]:
        """
        æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®å› æœæ¨è«–
        
        Double Machine Learning (DML) ã‚’ä½¿ç”¨ã—ã¦ã€å‡¦ç½®åŠ¹æœã‚’æ¨å®šã€‚
        
        Parameters
        ----------
        treatment_var : str, optional
            å‡¦ç½®å¤‰æ•°
        outcome_var : str, optional
            çµæœå¤‰æ•°
        control_vars : List[str], optional
            åˆ¶å¾¡å¤‰æ•°
        method : str, optional
            å› æœæ¨è«–æ‰‹æ³•
        
        Returns
        -------
        Dict[str, Any]
            å› æœæ¨è«–çµæœ
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 6: å› æœæ¨è«–")
        logger.info("=" * 80)
        
        if not ECONML_AVAILABLE:
            logger.warning("EconMLãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return {}
        
        if self.data_cleaned is None:
            self.load_and_validate_data()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        treatment_var = treatment_var or self.config.treatment_var
        outcome_var = outcome_var or self.config.outcome_var or self.config.performance_target
        control_vars = control_vars or self.config.control_variables
        method = method or self.config.causal_method
        
        if treatment_var is None:
            logger.warning("treatment_var ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return {}
        
        logger.info(f"å‡¦ç½®å¤‰æ•°: {treatment_var}")
        logger.info(f"çµæœå¤‰æ•°: {outcome_var}")
        logger.info(f"åˆ¶å¾¡å¤‰æ•°: {len(control_vars)} å€‹")
        logger.info(f"æ‰‹æ³•: {method}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            required_vars = [treatment_var, outcome_var] + control_vars
            df = self.data_cleaned[required_vars].dropna()
            
            T = df[treatment_var].values
            Y = df[outcome_var].values
            X = df[control_vars].values
            
            # Double Machine Learning
            if method == 'dml':
                est = LinearDML(
                    model_y=GradientBoostingRegressor(random_state=self.config.random_seed),
                    model_t=GradientBoostingRegressor(random_state=self.config.random_seed),
                    random_state=self.config.random_seed
                )
                est.fit(Y, T, X=X, W=None)
                
                # å‡¦ç½®åŠ¹æœã®æ¨å®š
                ate = est.ate(X=X)
                ate_inference = est.ate_inference(X=X)
                
                logger.info(f"å¹³å‡å‡¦ç½®åŠ¹æœ (ATE): {ate:.4f}")
                logger.info(f"95% CI: [{ate_inference.conf_int()[0]:.4f}, {ate_inference.conf_int()[1]:.4f}]")
                
                self.results['causal_inference'] = {
                    'method': method,
                    'treatment': treatment_var,
                    'outcome': outcome_var,
                    'ate': float(ate),
                    'ate_ci': [float(ate_inference.conf_int()[0]), float(ate_inference.conf_int()[1])],
                    'p_value': float(ate_inference.pvalue())
                }
                
                self.execution_times['causal_inference'] = (datetime.now() - start_time).total_seconds()
                logger.info(f"âœ“ PHASE 6 å®Œäº† ({self.execution_times['causal_inference']:.2f}ç§’)")
                
                return self.results['causal_inference']
            
        except Exception as e:
            logger.error(f"å› æœæ¨è«–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            return {}
    
    # ========================================================================
    # 7. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    # ========================================================================
    
    def run_temporal_pattern_analysis(self) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        
        æˆ¦ç•¥çš„å¤‰æ•°ã®æ™‚ç³»åˆ—æ¨ç§»ã‚’åˆ†æã€‚
        
        Returns
        -------
        Dict[str, Any]
            æ™‚ç³»åˆ—åˆ†æçµæœ
        """
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("PHASE 7: æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        logger.info("=" * 80)
        
        if self.data_cleaned is None:
            self.load_and_validate_data()
        
        try:
            # å¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¨ˆç®—
            temporal_trends = {}
            
            for var in self.config.strategic_features:
                if var in self.data_cleaned.columns:
                    trend = self.data_cleaned.groupby(self.config.time_var)[var].agg([
                        'mean', 'median', 'std'
                    ])
                    temporal_trends[var] = trend
            
            # å¯è¦–åŒ–
            self._visualize_temporal_patterns(temporal_trends)
            
            self.results['temporal_patterns'] = temporal_trends
            
            self.execution_times['temporal_patterns'] = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ PHASE 7 å®Œäº† ({self.execution_times['temporal_patterns']:.2f}ç§’)")
            
            return self.results['temporal_patterns']
            
        except Exception as e:
            logger.error(f"æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _visualize_temporal_patterns(self, trends: Dict[str, pd.DataFrame]):
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–"""
        n_vars = len(trends)
        ncols = 2
        nrows = (n_vars + 1) // ncols
        
        fig, axes = plt.subplots(nrows, ncols, figsize=(14, 4 * nrows))
        axes = axes.flatten()
        
        for idx, (var, trend) in enumerate(trends.items()):
            ax = axes[idx]
            trend['mean'].plot(ax=ax, marker='o', color='steelblue', linewidth=2)
            ax.fill_between(
                trend.index,
                trend['mean'] - trend['std'],
                trend['mean'] + trend['std'],
                alpha=0.2, color='steelblue'
            )
            ax.set_title(var)
            ax.set_xlabel('Year')
            ax.set_ylabel('Value')
            ax.grid(True, alpha=0.3)
        
        # æœªä½¿ç”¨ã®axesã‚’éè¡¨ç¤º
        for idx in range(n_vars, len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        
        fig_path = self.output_dir / 'figures' / f'temporal_patterns.{self.config.figure_format}'
        plt.savefig(fig_path, dpi=self.config.figure_dpi, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è¦–åŒ–ä¿å­˜: {fig_path}")
    
    # ========================================================================
    # 8. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ========================================================================
    
    def generate_comprehensive_report(self) -> str:
        """
        çµ±åˆHTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        
        ã™ã¹ã¦ã®åˆ†æçµæœã‚’1ã¤ã®HTMLãƒ¬ãƒãƒ¼ãƒˆã«ã¾ã¨ã‚ã‚‹ã€‚
        
        Returns
        -------
        str
            ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info("=" * 80)
        logger.info("PHASE 8: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        logger.info("=" * 80)
        
        try:
            # HTMLç”Ÿæˆ
            html_content = self._generate_html_report()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_path = self.output_dir / f'comprehensive_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.html'
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"âœ“ HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
            
            # LaTeXè¡¨ã®ç”Ÿæˆ
            if self.config.generate_latex_tables:
                self._generate_latex_tables()
            
            # å®Ÿè¡Œæ™‚é–“ã‚µãƒãƒªãƒ¼
            logger.info("=" * 80)
            logger.info("å®Ÿè¡Œæ™‚é–“ã‚µãƒãƒªãƒ¼:")
            for phase, duration in self.execution_times.items():
                logger.info(f"  {phase}: {duration:.2f}ç§’")
            total_time = sum(self.execution_times.values())
            logger.info(f"  åˆè¨ˆ: {total_time:.2f}ç§’")
            logger.info("=" * 80)
            
            return str(report_path)
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise
    
    def _generate_html_report(self) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        html_template = """
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Comprehensive Data Mining Report</title>
    <style>
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            border-left: 5px solid #3498db;
            padding-left: 10px;
            margin-top: 30px;
        }}
        h3 {{
            color: #7f8c8d;
        }}
        .section {{
            background-color: white;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background-color: #3498db;
            color: white;
        }}
        tr:hover {{
            background-color: #f5f5f5;
        }}
        .metric {{
            display: inline-block;
            background-color: #ecf0f1;
            padding: 10px 20px;
            margin: 10px;
            border-radius: 5px;
        }}
        .metric-value {{
            font-size: 24px;
            font-weight: bold;
            color: #3498db;
        }}
        .metric-label {{
            font-size: 12px;
            color: #7f8c8d;
        }}
        img {{
            max-width: 100%;
            height: auto;
            margin: 20px 0;
        }}
        .footer {{
            text-align: center;
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #7f8c8d;
        }}
    </style>
</head>
<body>
    <h1>ğŸ“Š Comprehensive Data Mining Report</h1>
    <p><strong>ç”Ÿæˆæ—¥æ™‚:</strong> {timestamp}</p>
    
    <div class="section">
        <h2>1. ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼</h2>
        {data_summary}
    </div>
    
    <div class="section">
        <h2>2. æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ</h2>
        {strategic_groups}
    </div>
    
    <div class="section">
        <h2>3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬</h2>
        {performance_prediction}
    </div>
    
    <div class="section">
        <h2>4. ç‰¹å¾´é‡é‡è¦åº¦</h2>
        {feature_importance}
    </div>
    
    <div class="section">
        <h2>5. ç•°å¸¸æ¤œçŸ¥</h2>
        {anomaly_detection}
    </div>
    
    <div class="section">
        <h2>6. å®Ÿè¡Œæ™‚é–“</h2>
        {execution_times}
    </div>
    
    <div class="footer">
        <p>Generated by Strategic Management Research Hub v3.1</p>
        <p>Powered by Python, scikit-learn, XGBoost, EconML</p>
    </div>
</body>
</html>
"""
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ã‚’ç”Ÿæˆ
        sections = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'data_summary': self._format_data_summary(),
            'strategic_groups': self._format_strategic_groups(),
            'performance_prediction': self._format_performance_prediction(),
            'feature_importance': self._format_feature_importance(),
            'anomaly_detection': self._format_anomaly_detection(),
            'execution_times': self._format_execution_times()
        }
        
        return html_template.format(**sections)
    
    def _format_data_summary(self) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã®HTMLç”Ÿæˆ"""
        if 'data_summary' not in self.results:
            return "<p>ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“</p>"
        
        summary = self.results['data_summary']
        
        html = f"""
        <div class="metric">
            <div class="metric-label">è¦³æ¸¬æ•°</div>
            <div class="metric-value">{summary['n_observations']:,}</div>
        </div>
        <div class="metric">
            <div class="metric-label">ä¼æ¥­æ•°</div>
            <div class="metric-value">{summary['n_firms']:,}</div>
        </div>
        <div class="metric">
            <div class="metric-label">å¹´æ•°</div>
            <div class="metric-value">{summary['n_years']}</div>
        </div>
        <p><strong>åˆ†ææœŸé–“:</strong> {summary['year_range'][0]} - {summary['year_range'][1]}</p>
        """
        
        return html
    
    def _format_strategic_groups(self) -> str:
        """æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æã®HTMLç”Ÿæˆ"""
        if 'strategic_groups' not in self.results:
            return "<p>æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“</p>"
        
        sg = self.results['strategic_groups']
        
        html = f"""
        <p><strong>ã‚¯ãƒ©ã‚¹ã‚¿æ•°:</strong> {sg['n_clusters']}</p>
        <p><strong>ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢:</strong> {sg['silhouette_score']:.4f}</p>
        <img src="figures/strategic_groups.{self.config.figure_format}" alt="Strategic Groups">
        """
        
        if 'cluster_profiles' in sg:
            profiles = sg['cluster_profiles'].to_html(index=False)
            html += f"<h3>ã‚¯ãƒ©ã‚¹ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«</h3>{profiles}"
        
        return html
    
    def _format_performance_prediction(self) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ã®HTMLç”Ÿæˆ"""
        if 'performance_prediction' not in self.results:
            return "<p>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“</p>"
        
        pp = self.results['performance_prediction']
        best_model = pp['best_model']
        best_results = pp['model_results'][best_model]
        
        html = f"""
        <p><strong>æœ€è‰¯ãƒ¢ãƒ‡ãƒ«:</strong> {best_model}</p>
        <div class="metric">
            <div class="metric-label">Train RÂ²</div>
            <div class="metric-value">{best_results['train_r2']:.4f}</div>
        </div>
        <div class="metric">
            <div class="metric-label">Test RÂ²</div>
            <div class="metric-value">{best_results['test_r2']:.4f}</div>
        </div>
        <div class="metric">
            <div class="metric-label">Test RMSE</div>
            <div class="metric-value">{best_results['test_rmse']:.4f}</div>
        </div>
        """
        
        # å…¨ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒè¡¨
        model_comparison = []
        for model_name, results in pp['model_results'].items():
            model_comparison.append({
                'Model': model_name,
                'Train RÂ²': f"{results['train_r2']:.4f}",
                'Test RÂ²': f"{results['test_r2']:.4f}",
                'Test RMSE': f"{results['test_rmse']:.4f}"
            })
        
        comparison_df = pd.DataFrame(model_comparison)
        html += f"<h3>ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ</h3>{comparison_df.to_html(index=False)}"
        
        return html
    
    def _format_feature_importance(self) -> str:
        """ç‰¹å¾´é‡é‡è¦åº¦ã®HTMLç”Ÿæˆ"""
        if 'feature_importance' not in self.results:
            return "<p>ç‰¹å¾´é‡é‡è¦åº¦åˆ†æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“</p>"
        
        fi = self.results['feature_importance']
        
        html = f"""
        <img src="figures/feature_importance.{self.config.figure_format}" alt="Feature Importance">
        """
        
        if 'importance_df' in fi:
            importance_table = fi['importance_df'].head(10).to_html(index=False)
            html += f"<h3>Top 10 é‡è¦ç‰¹å¾´é‡</h3>{importance_table}"
        
        return html
    
    def _format_anomaly_detection(self) -> str:
        """ç•°å¸¸æ¤œçŸ¥ã®HTMLç”Ÿæˆ"""
        if 'anomaly_detection' not in self.results:
            return "<p>ç•°å¸¸æ¤œçŸ¥ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“</p>"
        
        ad = self.results['anomaly_detection']
        
        html = f"""
        <div class="metric">
            <div class="metric-label">ç•°å¸¸å€¤æ•°</div>
            <div class="metric-value">{ad['n_anomalies']:,}</div>
        </div>
        <div class="metric">
            <div class="metric-label">ç•°å¸¸å€¤å‰²åˆ</div>
            <div class="metric-value">{ad['anomaly_rate']:.1%}</div>
        </div>
        <img src="figures/anomaly_detection.{self.config.figure_format}" alt="Anomaly Detection">
        """
        
        return html
    
    def _format_execution_times(self) -> str:
        """å®Ÿè¡Œæ™‚é–“ã®HTMLç”Ÿæˆ"""
        if not self.execution_times:
            return "<p>å®Ÿè¡Œæ™‚é–“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“</p>"
        
        times_data = []
        for phase, duration in self.execution_times.items():
            times_data.append({
                'Phase': phase,
                'Duration (seconds)': f"{duration:.2f}"
            })
        
        times_df = pd.DataFrame(times_data)
        total = sum(self.execution_times.values())
        
        html = times_df.to_html(index=False)
        html += f"<p><strong>åˆè¨ˆå®Ÿè¡Œæ™‚é–“:</strong> {total:.2f}ç§’</p>"
        
        return html
    
    def _generate_latex_tables(self):
        """LaTeXè¡¨ã®ç”Ÿæˆ"""
        logger.info("LaTeXè¡¨ã‚’ç”Ÿæˆä¸­...")
        
        tables_dir = self.output_dir / 'tables'
        
        # æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        if 'strategic_groups' in self.results and 'cluster_profiles' in self.results['strategic_groups']:
            profiles = self.results['strategic_groups']['cluster_profiles']
            latex_path = tables_dir / 'strategic_groups_profiles.tex'
            
            with open(latex_path, 'w') as f:
                f.write(profiles.to_latex(index=False, caption='Strategic Group Profiles'))
            
            logger.info(f"LaTeXè¡¨ä¿å­˜: {latex_path}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ã®æ¯”è¼ƒ
        if 'performance_prediction' in self.results:
            pp = self.results['performance_prediction']
            model_comparison = []
            for model_name, results in pp['model_results'].items():
                model_comparison.append({
                    'Model': model_name,
                    'Train RÂ²': results['train_r2'],
                    'Test RÂ²': results['test_r2'],
                    'Test RMSE': results['test_rmse']
                })
            
            comparison_df = pd.DataFrame(model_comparison)
            latex_path = tables_dir / 'model_comparison.tex'
            
            with open(latex_path, 'w') as f:
                f.write(comparison_df.to_latex(
                    index=False, 
                    caption='Model Performance Comparison',
                    float_format='%.4f'
                ))
            
            logger.info(f"LaTeXè¡¨ä¿å­˜: {latex_path}")
    
    # ========================================================================
    # å…¨è‡ªå‹•å®Ÿè¡Œ
    # ========================================================================
    
    def run_complete_analysis(self) -> Dict[str, Any]:
        """
        å…¨ãƒ•ã‚§ãƒ¼ã‚ºã®è‡ªå‹•å®Ÿè¡Œ
        
        Returns
        -------
        Dict[str, Any]
            ã™ã¹ã¦ã®åˆ†æçµæœ
        """
        logger.info("=" * 80)
        logger.info("å®Œå…¨è‡ªå‹•åˆ†æã‚’é–‹å§‹")
        logger.info("=" * 80)
        
        total_start = datetime.now()
        
        try:
            # Phase 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.load_and_validate_data()
            
            # Phase 2: æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
            self.run_strategic_group_analysis()
            
            # Phase 3: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
            self.run_performance_prediction()
            
            # Phase 4: ç‰¹å¾´é‡é‡è¦åº¦
            self.run_feature_importance_analysis()
            
            # Phase 5: ç•°å¸¸æ¤œçŸ¥
            self.run_anomaly_detection()
            
            # Phase 6: å› æœæ¨è«–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.config.treatment_var is not None:
                self.run_causal_inference()
            
            # Phase 7: æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
            self.run_temporal_pattern_analysis()
            
            # Phase 8: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
            report_path = self.generate_comprehensive_report()
            
            total_time = (datetime.now() - total_start).total_seconds()
            
            logger.info("=" * 80)
            logger.info(f"âœ… å…¨åˆ†æå®Œäº†ï¼ ({total_time:.2f}ç§’)")
            logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
            logger.info("=" * 80)
            
            return self.results
            
        except Exception as e:
            logger.error(f"åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            logger.error(traceback.format_exc())
            raise


# ============================================================================
# CLIå®Ÿè¡Œ
# ============================================================================

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Comprehensive Data Mining Pipeline for Strategic Management Research'
    )
    parser.add_argument(
        '--data',
        type=str,
        required=True,
        help='Path to data file (.dta, .csv, .parquet)'
    )
    parser.add_argument(
        '--config',
        type=str,
        help='Path to config YAML file (optional)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='./datamining_output/',
        help='Output directory'
    )
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    if args.config:
        pipeline = ComprehensiveDataMiningPipeline(config_path=args.config)
    else:
        pipeline = ComprehensiveDataMiningPipeline(
            data_path=args.data,
            output_dir=args.output
        )
    
    pipeline.run_complete_analysis()


if __name__ == '__main__':
    main()
