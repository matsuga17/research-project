#!/usr/bin/env python3
"""
data_profiler.py - è‡ªå‹•ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è‡ªç„¶è¨€èªãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚­ãƒ«ç”¨

ä½¿ç”¨æ³•:
    python data_profiler.py <file_path> [--output <output_path>] [--format json|text]
"""

import pandas as pd
import numpy as np
import json
import sys
import argparse
from pathlib import Path
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


def detect_file_type(path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¤œå‡º"""
    suffix = Path(path).suffix.lower()
    type_map = {
        '.csv': 'csv',
        '.tsv': 'tsv',
        '.xlsx': 'excel',
        '.xls': 'excel',
        '.json': 'json',
        '.parquet': 'parquet',
        '.pq': 'parquet',
    }
    return type_map.get(suffix, 'csv')


def load_data(path: str) -> pd.DataFrame:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    file_type = detect_file_type(path)
    
    loaders = {
        'csv': lambda p: pd.read_csv(p, low_memory=False),
        'tsv': lambda p: pd.read_csv(p, sep='\t', low_memory=False),
        'excel': lambda p: pd.read_excel(p),
        'json': lambda p: pd.read_json(p),
        'parquet': lambda p: pd.read_parquet(p),
    }
    
    return loaders[file_type](path)


def analyze_column(series: pd.Series) -> dict:
    """å€‹åˆ¥åˆ—ã®åˆ†æ"""
    result = {
        'dtype': str(series.dtype),
        'count': int(len(series)),
        'missing': int(series.isnull().sum()),
        'missing_pct': round(series.isnull().sum() / len(series) * 100, 2),
        'unique': int(series.nunique()),
        'unique_pct': round(series.nunique() / len(series) * 100, 2),
    }
    
    # æ•°å€¤åˆ—ã®åˆ†æ
    if pd.api.types.is_numeric_dtype(series):
        non_null = series.dropna()
        if len(non_null) > 0:
            result['stats'] = {
                'mean': round(non_null.mean(), 4),
                'std': round(non_null.std(), 4),
                'min': round(non_null.min(), 4),
                'q25': round(non_null.quantile(0.25), 4),
                'median': round(non_null.median(), 4),
                'q75': round(non_null.quantile(0.75), 4),
                'max': round(non_null.max(), 4),
            }
            # å¤–ã‚Œå€¤æ¤œå‡º
            Q1, Q3 = non_null.quantile([0.25, 0.75])
            IQR = Q3 - Q1
            outliers = ((non_null < Q1 - 1.5*IQR) | (non_null > Q3 + 1.5*IQR)).sum()
            result['outliers'] = int(outliers)
            result['outliers_pct'] = round(outliers / len(non_null) * 100, 2)
    
    # ã‚«ãƒ†ã‚´ãƒª/æ–‡å­—åˆ—åˆ—ã®åˆ†æ
    elif pd.api.types.is_object_dtype(series) or pd.api.types.is_categorical_dtype(series):
        value_counts = series.value_counts().head(10)
        result['top_values'] = {
            str(k): int(v) for k, v in value_counts.items()
        }
        # ç©ºæ–‡å­—åˆ—æ¤œå‡º
        if series.dtype == 'object':
            empty_strings = (series == '').sum()
            if empty_strings > 0:
                result['empty_strings'] = int(empty_strings)
    
    # æ—¥æ™‚åˆ—ã®åˆ†æ
    elif pd.api.types.is_datetime64_any_dtype(series):
        non_null = series.dropna()
        if len(non_null) > 0:
            result['date_range'] = {
                'min': str(non_null.min()),
                'max': str(non_null.max()),
                'span_days': int((non_null.max() - non_null.min()).days)
            }
    
    return result


def detect_potential_dates(df: pd.DataFrame) -> list:
    """æ—¥ä»˜ã«å¤‰æ›å¯èƒ½ãªåˆ—ã‚’æ¤œå‡º"""
    potential_dates = []
    date_keywords = ['date', 'time', 'created', 'updated', 'timestamp', 'dt', 'day', 'month', 'year']
    
    for col in df.select_dtypes(include=['object']).columns:
        # åˆ—åã«ã‚ˆã‚‹æ¤œå‡º
        if any(kw in col.lower() for kw in date_keywords):
            potential_dates.append(col)
            continue
        
        # ã‚µãƒ³ãƒ—ãƒ«ã«ã‚ˆã‚‹æ¤œå‡º
        sample = df[col].dropna().head(100)
        if len(sample) > 0:
            try:
                pd.to_datetime(sample, errors='raise')
                potential_dates.append(col)
            except:
                pass
    
    return potential_dates


def detect_quality_issues(df: pd.DataFrame) -> list:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã‚’æ¤œå‡º"""
    issues = []
    
    # é«˜æ¬ æç‡åˆ—
    high_missing = df.columns[df.isnull().mean() > 0.5].tolist()
    if high_missing:
        issues.append({
            'type': 'high_missing',
            'severity': 'warning',
            'columns': high_missing,
            'message': f'{len(high_missing)}åˆ—ã§æ¬ æç‡ãŒ50%ã‚’è¶…ãˆã¦ã„ã¾ã™'
        })
    
    # é‡è¤‡è¡Œ
    dup_count = df.duplicated().sum()
    if dup_count > 0:
        issues.append({
            'type': 'duplicates',
            'severity': 'info',
            'count': int(dup_count),
            'message': f'{dup_count}ä»¶ã®é‡è¤‡è¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ'
        })
    
    # å®šæ•°åˆ—ï¼ˆå˜ä¸€å€¤ã®ã¿ï¼‰
    constant_cols = [col for col in df.columns if df[col].nunique() == 1]
    if constant_cols:
        issues.append({
            'type': 'constant_columns',
            'severity': 'info',
            'columns': constant_cols,
            'message': f'{len(constant_cols)}åˆ—ãŒå®šæ•°ï¼ˆå˜ä¸€å€¤ï¼‰ã§ã™'
        })
    
    # é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£åˆ—
    high_cardinality = []
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) > 0.9:
            high_cardinality.append(col)
    if high_cardinality:
        issues.append({
            'type': 'high_cardinality',
            'severity': 'info',
            'columns': high_cardinality,
            'message': f'{len(high_cardinality)}åˆ—ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ç‡ãŒ90%ã‚’è¶…ãˆã¦ã„ã¾ã™ï¼ˆIDåˆ—ã®å¯èƒ½æ€§ï¼‰'
        })
    
    return issues


def profile_dataframe(df: pd.DataFrame) -> dict:
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ä½“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
    profile = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'rows': len(df),
            'columns': len(df.columns),
            'memory_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),
        },
        'dtypes_summary': df.dtypes.astype(str).value_counts().to_dict(),
        'columns': {},
        'quality_issues': [],
        'potential_date_columns': [],
    }
    
    # å„åˆ—ã®åˆ†æ
    for col in df.columns:
        profile['columns'][col] = analyze_column(df[col])
    
    # å“è³ªå•é¡Œæ¤œå‡º
    profile['quality_issues'] = detect_quality_issues(df)
    
    # æ—¥ä»˜å€™è£œåˆ—
    profile['potential_date_columns'] = detect_potential_dates(df)
    
    # æ•°å€¤åˆ—é–“ã®ç›¸é–¢ï¼ˆä¸Šä½ï¼‰
    numeric_cols = df.select_dtypes(include='number').columns
    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr()
        high_corr = []
        for i in range(len(numeric_cols)):
            for j in range(i+1, len(numeric_cols)):
                c = corr.iloc[i, j]
                if abs(c) > 0.7:
                    high_corr.append({
                        'col1': numeric_cols[i],
                        'col2': numeric_cols[j],
                        'correlation': round(c, 3)
                    })
        if high_corr:
            profile['high_correlations'] = sorted(high_corr, key=lambda x: abs(x['correlation']), reverse=True)[:10]
    
    return profile


def format_text_report(profile: dict) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    lines = []
    lines.append("=" * 60)
    lines.append("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("=" * 60)
    
    # åŸºæœ¬æƒ…å ±
    meta = profile['metadata']
    lines.append(f"\nğŸ“‹ åŸºæœ¬æƒ…å ±")
    lines.append(f"  è¡Œæ•°: {meta['rows']:,}")
    lines.append(f"  åˆ—æ•°: {meta['columns']}")
    lines.append(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {meta['memory_mb']:.2f} MB")
    
    # å‹ã‚µãƒãƒªãƒ¼
    lines.append(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ")
    for dtype, count in profile['dtypes_summary'].items():
        lines.append(f"  {dtype}: {count}åˆ—")
    
    # å“è³ªå•é¡Œ
    if profile['quality_issues']:
        lines.append(f"\nâš ï¸ æ¤œå‡ºã•ã‚ŒãŸå“è³ªå•é¡Œ")
        for issue in profile['quality_issues']:
            severity_icon = 'ğŸ”´' if issue['severity'] == 'warning' else 'ğŸŸ¡'
            lines.append(f"  {severity_icon} {issue['message']}")
    
    # æ—¥ä»˜å€™è£œåˆ—
    if profile['potential_date_columns']:
        lines.append(f"\nğŸ“… æ—¥ä»˜å‹ã«å¤‰æ›å¯èƒ½ãªåˆ—")
        lines.append(f"  {', '.join(profile['potential_date_columns'])}")
    
    # é«˜ç›¸é–¢
    if 'high_correlations' in profile:
        lines.append(f"\nğŸ”— é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.7ï¼‰")
        for hc in profile['high_correlations'][:5]:
            lines.append(f"  {hc['col1']} â†” {hc['col2']}: {hc['correlation']}")
    
    # å„åˆ—ã®è©³ç´°
    lines.append(f"\nğŸ“Š åˆ—è©³ç´°")
    lines.append("-" * 60)
    
    for col_name, col_info in profile['columns'].items():
        lines.append(f"\nã€{col_name}ã€‘")
        lines.append(f"  å‹: {col_info['dtype']}")
        lines.append(f"  æ¬ æ: {col_info['missing']:,} ({col_info['missing_pct']}%)")
        lines.append(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯: {col_info['unique']:,} ({col_info['unique_pct']}%)")
        
        if 'stats' in col_info:
            s = col_info['stats']
            lines.append(f"  çµ±è¨ˆ: å¹³å‡={s['mean']}, ä¸­å¤®å€¤={s['median']}, æ¨™æº–åå·®={s['std']}")
            lines.append(f"        ç¯„å›²=[{s['min']}, {s['max']}], IQR=[{s['q25']}, {s['q75']}]")
            if col_info.get('outliers', 0) > 0:
                lines.append(f"  å¤–ã‚Œå€¤: {col_info['outliers']} ({col_info['outliers_pct']}%)")
        
        if 'top_values' in col_info:
            lines.append(f"  ä¸Šä½å€¤:")
            for val, cnt in list(col_info['top_values'].items())[:5]:
                lines.append(f"    - {val}: {cnt}")
        
        if 'date_range' in col_info:
            dr = col_info['date_range']
            lines.append(f"  æ—¥ä»˜ç¯„å›²: {dr['min']} ~ {dr['max']} ({dr['span_days']}æ—¥)")
    
    lines.append("\n" + "=" * 60)
    lines.append("âœ… ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Œäº†")
    lines.append("=" * 60)
    
    return '\n'.join(lines)


def main():
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°')
    parser.add_argument('file_path', help='åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--format', '-f', choices=['json', 'text'], default='text', help='å‡ºåŠ›å½¢å¼')
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {args.file_path}")
    df = load_data(args.file_path)
    print(f"âœ… {len(df):,}è¡Œ Ã— {len(df.columns)}åˆ— ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
    print("ğŸ” ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ä¸­...")
    profile = profile_dataframe(df)
    
    # å‡ºåŠ›
    if args.format == 'json':
        output = json.dumps(profile, ensure_ascii=False, indent=2)
    else:
        output = format_text_report(profile)
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(output)
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {args.output}")
    else:
        print(output)
    
    return profile


if __name__ == '__main__':
    main()
