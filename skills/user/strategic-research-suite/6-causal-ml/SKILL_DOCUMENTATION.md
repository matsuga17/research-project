---
name: strategic-research-causal-ml
description: Causal inference with machine learning for strategic management research including Causal Forest for heterogeneous treatment effects, Double Machine Learning for high-dimensional confounding, and Synthetic Control Method for comparative case studies.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 7 (Statistical Analysis)
  - statistical-methods: Traditional causal inference
  - automation: ML pipeline automation
---

# Causal ML Toolkit v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

**å› æœæ¨è«–Ã—æ©Ÿæ¢°å­¦ç¿’**ã®æœ€æ–°æ‰‹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚å‡¦ç½®åŠ¹æœã®ç•°è³ªæ€§ã€é«˜æ¬¡å…ƒäº¤çµ¡ã€æ¯”è¼ƒäº‹ä¾‹ç ”ç©¶ã«å¯¾å¿œã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… å‡¦ç½®åŠ¹æœãŒä¼æ¥­ã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼ˆHeterogeneous Treatment Effectsï¼‰
- âœ… çµ±åˆ¶å¤‰æ•°ãŒå¤šã™ãã‚‹ï¼ˆé«˜æ¬¡å…ƒäº¤çµ¡ï¼‰
- âœ… å°‘æ•°å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆã®ã‚¤ãƒ™ãƒ³ãƒˆç ”ç©¶ï¼ˆæ¯”è¼ƒäº‹ä¾‹ï¼‰
- âœ… å¾“æ¥æ‰‹æ³•ï¼ˆIV, PSMï¼‰ãŒæ©Ÿèƒ½ã—ãªã„æ™‚

### å‰ææ¡ä»¶

- å› æœæ¨è«–ã®åŸºç¤ï¼ˆå‡¦ç½®ãƒ»ã‚¢ã‚¦ãƒˆã‚«ãƒ ãƒ»äº¤çµ¡ï¼‰
- æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬ï¼ˆæ±ºå®šæœ¨ã€æ­£å‰‡åŒ–ï¼‰
- Pythonï¼ˆscikit-learn, econmlï¼‰

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

- **åŸºæœ¬å› æœæ¨è«–** â†’ `3-statistical-methods`ï¼ˆIV, PSM, DiDï¼‰
- **ãƒ‡ãƒ¼ã‚¿æº–å‚™** â†’ `1-core-workflow` Phase 6
- **æ¨™æº–åˆ†æ** â†’ `3-statistical-methods`

---

## ğŸ“‹ ç›®æ¬¡

1. [Causal Forest](#1-causal-forest)
2. [Double Machine Learning](#2-double-machine-learning-dml)
3. [Synthetic Control Method](#3-synthetic-control-method)
4. [Quick Reference](#4-quick-reference)

---

## 1. Causal Forest

### 1.1 æ¦‚å¿µ

**Causal Forest**: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§å€‹åˆ¥å‡¦ç½®åŠ¹æœï¼ˆCATEï¼‰ã‚’æ¨å®š

```
CATE(X) = E[Y(1) - Y(0) | X]

Y(1): å‡¦ç½®æ™‚ã®ã‚¢ã‚¦ãƒˆã‚«ãƒ 
Y(0): éå‡¦ç½®æ™‚ã®ã‚¢ã‚¦ãƒˆã‚«ãƒ 
X: ä¼æ¥­ç‰¹æ€§
```

**ã„ã¤ä½¿ã†ã‹**:
- R&DæŠ•è³‡ã®åŠ¹æœãŒä¼æ¥­è¦æ¨¡ã§ç•°ãªã‚‹
- M&Aã®åŠ¹æœãŒç”£æ¥­ã§ç•°ãªã‚‹
- CSRæ´»å‹•ã®åŠ¹æœãŒåœ°åŸŸã§ç•°ãªã‚‹

### 1.2 å®Ÿè£…: EconML

```python
from econml.dml import CausalForestDML
import pandas as pd
import numpy as np

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
# Y: ã‚¢ã‚¦ãƒˆã‚«ãƒ ï¼ˆROAï¼‰
# T: å‡¦ç½®å¤‰æ•°ï¼ˆR&DæŠ•è³‡é«˜=1ï¼‰
# X: åŠ¹æœä¿®æ­£å¤‰æ•°ï¼ˆfirm_size, industryç­‰ï¼‰
# W: äº¤çµ¡å¤‰æ•°ï¼ˆçµ±åˆ¶å¤‰æ•°ï¼‰

df = pd.read_csv('firm_data.csv')

Y = df['roa'].values
T = df['high_rd'].values  # R&Dé«˜=1
X = df[['firm_size', 'firm_age', 'leverage']].values
W = df[['industry', 'year']].values

# Causal Forestæ¨å®š
cf = CausalForestDML(
    model_y=None,  # Auto: Random Forest
    model_t=None,  # Auto: Random Forest
    n_estimators=1000,
    random_state=42
)

cf.fit(Y, T, X=X, W=W)

# å€‹åˆ¥å‡¦ç½®åŠ¹æœï¼ˆCATEï¼‰
cate = cf.effect(X)

print(f"Mean CATE: {cate.mean():.4f}")
print(f"CATE std: {cate.std():.4f}")
print(f"CATE range: [{cate.min():.4f}, {cate.max():.4f}]")

# ä¼æ¥­åˆ¥CATE
df['cate'] = cate
print(df[['firm_id', 'firm_size', 'cate']].head(10))
```

### 1.3 ç•°è³ªæ€§ã®åˆ†æ

```python
# CATEã¨ä¼æ¥­ç‰¹æ€§ã®é–¢ä¿‚
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(df['firm_size'], df['cate'], alpha=0.5)
plt.xlabel('Firm Size (log)')
plt.ylabel('CATE (R&D Effect)')
plt.title('Treatment Effect Heterogeneity by Firm Size')
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

# çµ±è¨ˆçš„æ¤œå®š: CATEåˆ†æ•£ãŒæœ‰æ„ã‹
from scipy import stats

# H0: å…¨ä¼æ¥­ã§å‡¦ç½®åŠ¹æœãŒåŒã˜ï¼ˆåˆ†æ•£=0ï¼‰
# H1: å‡¦ç½®åŠ¹æœãŒç•°è³ªçš„ï¼ˆåˆ†æ•£>0ï¼‰

# Best Linear Projection (BLP) test
blp_model = cf.effect_inference(X)
print(blp_model.summary_frame())
```

### 1.4 æˆ¦ç•¥ç ”ç©¶ã¸ã®å¿œç”¨

**ä»®èª¬**: R&DåŠ¹æœã¯ä¼æ¥­è¦æ¨¡ã§ç•°ãªã‚‹

```python
# å¤§ä¼æ¥­ vs å°ä¼æ¥­ã®CATEæ¯”è¼ƒ
large_firms = df[df['firm_size'] > df['firm_size'].median()]
small_firms = df[df['firm_size'] <= df['firm_size'].median()]

cate_large = large_firms['cate'].mean()
cate_small = small_firms['cate'].mean()

print(f"CATE (Large firms): {cate_large:.4f}")
print(f"CATE (Small firms): {cate_small:.4f}")
print(f"Difference: {cate_large - cate_small:.4f}")

# tæ¤œå®š
t_stat, p_value = stats.ttest_ind(large_firms['cate'], small_firms['cate'])
print(f"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}")
```

---

## 2. Double Machine Learning (DML)

### 2.1 æ¦‚å¿µ

**DML**: é«˜æ¬¡å…ƒäº¤çµ¡ä¸‹ã§ã®å› æœåŠ¹æœæ¨å®š

**å•é¡Œ**: çµ±åˆ¶å¤‰æ•°ãŒ100+ã‚ã‚‹ â†’ é€šå¸¸ã®OLSã¯ç ´ç¶»

**è§£æ±º**: 
1. MLï¼ˆLasso, RFç­‰ï¼‰ã§ Y ~ W, T ~ W ã‚’äºˆæ¸¬
2. æ®‹å·®ã§å› æœåŠ¹æœæ¨å®š

### 2.2 å®Ÿè£…

```python
from econml.dml import LinearDML
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

# é«˜æ¬¡å…ƒçµ±åˆ¶å¤‰æ•°ï¼ˆ100+ï¼‰
W_high_dim = df[[col for col in df.columns if 'control_' in col]].values

# DMLæ¨å®š
dml = LinearDML(
    model_y=RandomForestRegressor(n_estimators=100),
    model_t=RandomForestClassifier(n_estimators=100),
    discrete_treatment=True,
    random_state=42
)

dml.fit(Y, T, X=None, W=W_high_dim)

# å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰
ate = dml.effect().mean()
print(f"Average Treatment Effect: {ate:.4f}")

# ä¿¡é ¼åŒºé–“
ate_inference = dml.effect_inference()
ci = ate_inference.conf_int()
print(f"95% CI: [{ci[0][0]:.4f}, {ci[1][0]:.4f}]")
```

### 2.3 DML vs é€šå¸¸å›å¸°ã®æ¯”è¼ƒ

```python
from sklearn.linear_model import LogisticRegression
from linearmodels.panel import PanelOLS

# é€šå¸¸ã®OLSï¼ˆå‚è€ƒ: é«˜æ¬¡å…ƒã§ãƒã‚¤ã‚¢ã‚¹ï¼‰
# ï¼ˆå®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼‰

print("=== Comparison ===")
print(f"DML ATE: {ate:.4f}")
print("DML handles high-dimensional confounding robustly")
```

---

## 3. Synthetic Control Method

### 3.1 æ¦‚å¿µ

**Synthetic Control**: å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆã®ã€Œåˆæˆå¯¾ç…§ç¾¤ã€ã‚’æ§‹ç¯‰

**ä¾‹**: 
- ä¼æ¥­Aï¼ˆå‡¦ç½®: M&Aå®Ÿæ–½ï¼‰
- ä¼æ¥­B, C, D...ï¼ˆå¯¾ç…§: M&Aæœªå®Ÿæ–½ï¼‰
- åˆæˆA' = 0.3Ã—B + 0.5Ã—C + 0.2Ã—Dï¼ˆå‡¦ç½®å‰ã®Aã«é¡ä¼¼ï¼‰

**ã„ã¤ä½¿ã†ã‹**:
- å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆãŒ1ç¤¾ã®ã¿ï¼ˆã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ï¼‰
- å°‘æ•°ã®å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆï¼ˆ< 10ç¤¾ï¼‰
- DiDã®Parallel Trendsä»®å®šãŒç–‘ã‚ã—ã„

### 3.2 å®Ÿè£…

```python
from sklearn.linear_model import Ridge

def synthetic_control(treated_id, control_ids, df, outcome_var, treatment_year):
    """Synthetic Controlæ¨å®š
    
    Args:
        treated_id: å‡¦ç½®ä¼æ¥­ID
        control_ids: å¯¾ç…§ä¼æ¥­IDã®ãƒªã‚¹ãƒˆ
        df: ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
        outcome_var: ã‚¢ã‚¦ãƒˆã‚«ãƒ å¤‰æ•°å
        treatment_year: å‡¦ç½®å¹´
    
    Returns:
        weights: åˆæˆã‚¦ã‚§ã‚¤ãƒˆ
        synthetic_control: åˆæˆå¯¾ç…§ç¾¤ã®æ™‚ç³»åˆ—
        att: å‡¦ç½®å¾Œã®å¹³å‡å‡¦ç½®åŠ¹æœ
    """
    
    # å‡¦ç½®å‰æœŸé–“
    pre_period = df[df['year'] < treatment_year]
    post_period = df[df['year'] >= treatment_year]
    
    # å‡¦ç½®ä¼æ¥­ã®å‡¦ç½®å‰ã‚¢ã‚¦ãƒˆã‚«ãƒ 
    y_treated_pre = pre_period[pre_period['firm_id'] == treated_id][outcome_var].values
    
    # å¯¾ç…§ä¼æ¥­ã®å‡¦ç½®å‰ã‚¢ã‚¦ãƒˆã‚«ãƒ ï¼ˆè¡Œåˆ—ï¼‰
    X_control_pre = []
    for cid in control_ids:
        y_c = pre_period[pre_period['firm_id'] == cid][outcome_var].values
        X_control_pre.append(y_c)
    X_control_pre = np.array(X_control_pre).T
    
    # ã‚¦ã‚§ã‚¤ãƒˆæ¨å®šï¼ˆRidgeå›å¸°ã€éè² åˆ¶ç´„ï¼‰
    ridge = Ridge(alpha=0.01, fit_intercept=False, positive=True)
    ridge.fit(X_control_pre, y_treated_pre)
    weights = ridge.coef_
    weights = weights / weights.sum()  # æ­£è¦åŒ–
    
    # åˆæˆå¯¾ç…§ç¾¤ã®æ§‹ç¯‰ï¼ˆå…¨æœŸé–“ï¼‰
    synthetic = []
    for year in df['year'].unique():
        df_year = df[df['year'] == year]
        y_controls = [df_year[df_year['firm_id'] == cid][outcome_var].values[0] 
                      for cid in control_ids]
        synthetic.append(np.dot(weights, y_controls))
    
    synthetic = pd.Series(synthetic, index=df['year'].unique())
    
    # å‡¦ç½®å¾Œã®å¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATTï¼‰
    y_treated_post = post_period[post_period['firm_id'] == treated_id][outcome_var].values
    y_synthetic_post = synthetic[synthetic.index >= treatment_year].values
    
    att = (y_treated_post - y_synthetic_post).mean()
    
    return weights, synthetic, att

# ä½¿ç”¨ä¾‹
treated_firm = 'A'
control_firms = ['B', 'C', 'D', 'E', 'F']

weights, synthetic, att = synthetic_control(
    treated_id=treated_firm,
    control_ids=control_firms,
    df=df,
    outcome_var='roa',
    treatment_year=2018
)

print(f"\n=== Synthetic Control Weights ===")
for i, firm in enumerate(control_firms):
    print(f"{firm}: {weights[i]:.3f}")

print(f"\nAverage Treatment Effect on Treated: {att:.4f}")
```

### 3.3 å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

# å‡¦ç½®ä¼æ¥­ã¨åˆæˆå¯¾ç…§ç¾¤ã®æ¯”è¼ƒ
df_treated = df[df['firm_id'] == treated_firm].set_index('year')['roa']

plt.figure(figsize=(12, 6))
plt.plot(df_treated.index, df_treated.values, 'b-', linewidth=2, label='Treated')
plt.plot(synthetic.index, synthetic.values, 'r--', linewidth=2, label='Synthetic Control')
plt.axvline(x=2018, color='gray', linestyle=':', label='Treatment')
plt.xlabel('Year')
plt.ylabel('ROA')
plt.title('Synthetic Control: Treated vs Synthetic')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 3.4 Placebo Testï¼ˆå¦¥å½“æ€§æ¤œè¨¼ï¼‰

```python
def placebo_test(control_ids, df, outcome_var, treatment_year, n_placebo=100):
    """Placebo test: ãƒ©ãƒ³ãƒ€ãƒ ä¼æ¥­ã‚’ã€Œå½å‡¦ç½®ã€"""
    
    placebo_effects = []
    
    for _ in range(n_placebo):
        # ãƒ©ãƒ³ãƒ€ãƒ ã«1ç¤¾ã‚’å½å‡¦ç½®ä¼æ¥­ã«
        pseudo_treated = np.random.choice(control_ids)
        pseudo_controls = [c for c in control_ids if c != pseudo_treated]
        
        _, _, pseudo_att = synthetic_control(
            treated_id=pseudo_treated,
            control_ids=pseudo_controls,
            df=df,
            outcome_var=outcome_var,
            treatment_year=treatment_year
        )
        
        placebo_effects.append(pseudo_att)
    
    return placebo_effects

# å®Ÿè¡Œ
placebo_effects = placebo_test(control_firms, df, 'roa', 2018, n_placebo=100)

# på€¤è¨ˆç®—
p_value = np.mean([abs(pe) >= abs(att) for pe in placebo_effects])
print(f"\nPlacebo test p-value: {p_value:.4f}")

# åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(10, 6))
plt.hist(placebo_effects, bins=30, alpha=0.7, label='Placebo ATTs')
plt.axvline(x=att, color='r', linestyle='--', linewidth=2, label=f'Actual ATT: {att:.4f}')
plt.xlabel('Treatment Effect')
plt.ylabel('Frequency')
plt.title('Placebo Test: Distribution of Pseudo-Treatment Effects')
plt.legend()
plt.show()
```

---

## 4. Quick Reference

### æ‰‹æ³•é¸æŠã‚¬ã‚¤ãƒ‰

| çŠ¶æ³ | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
|------|---------|------|
| å‡¦ç½®åŠ¹æœã®ç•°è³ªæ€§ã‚’èª¿ã¹ãŸã„ | **Causal Forest** | å€‹åˆ¥CATEæ¨å®š |
| çµ±åˆ¶å¤‰æ•°ãŒ100+ | **Double ML** | é«˜æ¬¡å…ƒå¯¾å¿œ |
| å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆãŒ1-10ç¤¾ | **Synthetic Control** | å°‘æ•°ã‚±ãƒ¼ã‚¹å¯¾å¿œ |
| æ¨™æº–çš„ãƒ‘ãƒãƒ«ç ”ç©¶ | `3-statistical-methods` | FE, IV, PSM |

### Causal ML vs å¾“æ¥æ‰‹æ³•

| æ‰‹æ³• | å‡¦ç½®åŠ¹æœ | çµ±åˆ¶å¤‰æ•°æ•° | å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆæ•° |
|------|---------|-----------|--------------|
| **OLS/FE** | å¹³å‡ã®ã¿ | < 20 | ä»»æ„ |
| **IV/PSM** | å¹³å‡ã®ã¿ | < 50 | 50+ |
| **Causal Forest** | ç•°è³ªçš„ï¼ˆå€‹åˆ¥ï¼‰ | < 100 | 100+ |
| **Double ML** | å¹³å‡ã®ã¿ | 100+ | 100+ |
| **Synthetic Control** | å€‹åˆ¥ | ä»»æ„ | 1-10 |

### æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ä¾‹

**ç ”ç©¶1**: R&DæŠ•è³‡åŠ¹æœã®ç•°è³ªæ€§
```python
# Causal Forest â†’ ä¼æ¥­è¦æ¨¡åˆ¥ã®CATE
cf = CausalForestDML()
cf.fit(Y=roa, T=high_rd, X=firm_characteristics, W=controls)
cate_by_size = analyze_heterogeneity(cf, by='firm_size')
```

**ç ”ç©¶2**: è¤‡é›‘ãªäº¤çµ¡æ§‹é€ 
```python
# DML â†’ 100+çµ±åˆ¶å¤‰æ•°
dml = LinearDML(model_y=RandomForest, model_t=RandomForest)
dml.fit(Y=performance, T=csr_initiative, W=high_dim_controls)
```

**ç ”ç©¶3**: Appleç¤¾ã®iPhoneç™ºå£²åŠ¹æœ
```python
# Synthetic Control â†’ å˜ä¸€ã‚±ãƒ¼ã‚¹
weights, synthetic, att = synthetic_control(
    treated_id='Apple',
    control_ids=tech_firms,
    treatment_year=2007
)
```

---

## ç†è«–çš„æ³¨æ„ç‚¹

### Causal Forest
- **ä»®å®š**: Unconfoundednessï¼ˆäº¤çµ¡å¤‰æ•°ã‚’ã™ã¹ã¦è¦³æ¸¬ï¼‰
- **é™ç•Œ**: è¦³æ¸¬ä¸å¯èƒ½ãªäº¤çµ¡ã¯å¯¾å¿œä¸å¯

### Double ML
- **ä»®å®š**: Neyman orthogonalityï¼ˆç›´äº¤æ€§ï¼‰
- **å¼·ã¿**: ãƒ¢ãƒ‡ãƒ«èª¤æŒ‡å®šã«é ‘å¥

### Synthetic Control
- **ä»®å®š**: Convex hullï¼ˆå‡¦ç½®ä¼æ¥­ãŒå¯¾ç…§ç¾¤ã®å‡¸åŒ…å†…ï¼‰
- **é™ç•Œ**: å‡¦ç½®å‰æœŸé–“ãŒçŸ­ã„ã¨ç²¾åº¦ä½ä¸‹

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# EconMLï¼ˆCausal ML toolkitï¼‰
pip install econml

# ä¾å­˜é–¢ä¿‚
pip install scikit-learn pandas numpy scipy matplotlib

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: CausalMLï¼ˆä»£æ›¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
pip install causalml
```

---

## å‚è€ƒæ–‡çŒ®

**Causal Forest**:
- Wager, S., & Athey, S. (2018). "Estimation and inference of heterogeneous treatment effects using random forests." *JASA*, 113(523), 1228-1242.

**Double ML**:
- Chernozhukov, V., et al. (2018). "Double/debiased machine learning for treatment and structural parameters." *Econometrics Journal*, 21(1), C1-C68.

**Synthetic Control**:
- Abadie, A., & Gardeazabal, J. (2003). "The economic costs of conflict: A case study of the Basque Country." *American Economic Review*, 93(1), 113-132.

---

**Version**: 4.0  
**Last Updated**: 2025-11-01  
**Next**: `7-esg-sustainability`, `8-automation` skills
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
