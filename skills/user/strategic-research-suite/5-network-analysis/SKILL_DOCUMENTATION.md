---
name: strategic-research-network-analysis
description: Network analysis toolkit for strategic management research including board interlock networks, strategic alliance networks, and patent citation networks with centrality measures and visualization.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 6 (Variable Construction)
  - data-sources: Network data collection
  - statistical-methods: Network variables in regression
---

# Network Analysis Toolkit v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥ç ”ç©¶ã«ãŠã‘ã‚‹**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†ææ‰‹æ³•**ã‚’æä¾›ã—ã¾ã™ã€‚å–ç· å½¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€æˆ¦ç•¥çš„ææºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ç‰¹è¨±å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰ã¨åˆ†æã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… ä¼æ¥­é–“é–¢ä¿‚ã‚’å¤‰æ•°åŒ–ã—ãŸã„æ™‚
- âœ… Board Interlockï¼ˆå–ç· å½¹å…¼ä»»ï¼‰ã®å½±éŸ¿ã‚’ç ”ç©¶
- âœ… Strategic Allianceï¼ˆæˆ¦ç•¥çš„ææºï¼‰ã®åŠ¹æœã‚’åˆ†æ
- âœ… çŸ¥è­˜ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼ï¼ˆç‰¹è¨±å¼•ç”¨ï¼‰ã‚’æ¸¬å®š

### å‰ææ¡ä»¶

- PythonåŸºç¤ï¼ˆpandas, networkxï¼‰
- ã‚°ãƒ©ãƒ•ç†è«–ã®åŸºæœ¬æ¦‚å¿µ
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–ã®ç†è§£

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

- **å¤‰æ•°æ§‹ç¯‰** â†’ `1-core-workflow` Phase 6
- **çµ±è¨ˆåˆ†æ** â†’ `3-statistical-methods`
- **ãƒ‡ãƒ¼ã‚¿åé›†** â†’ `2-data-sources`

---

## ğŸ“‹ ç›®æ¬¡

1. [Board Interlock Network](#1-board-interlock-network)
2. [Strategic Alliance Network](#2-strategic-alliance-network)
3. [Patent Citation Network](#3-patent-citation-network)
4. [Quick Reference](#4-quick-reference)

---

## 1. Board Interlock Network

### 1.1 æ¦‚å¿µ

**Board Interlock**: è¤‡æ•°ä¼æ¥­ã®å–ç· å½¹ã‚’å…¼ä»»ã™ã‚‹äººç‰©ã‚’é€šã˜ãŸä¼æ¥­é–“ãƒªãƒ³ã‚¯

**ç†è«–çš„æ ¹æ‹ **:
- Resource Dependence Theory: å¤–éƒ¨è³‡æºã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
- Social Network Theory: æƒ…å ±ãƒ»çŸ¥è­˜ã®æµé€š
- Institutional Theory: æ…£è¡Œã®æ¨¡å€£ãƒ»æ‹¡æ•£

### 1.2 ãƒ‡ãƒ¼ã‚¿åé›†

```python
import pandas as pd
import networkx as nx

# å–ç· å½¹ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: EDINETã‹ã‚‰ï¼‰
directors = pd.DataFrame({
    'director_id': [1, 1, 2, 2, 3],
    'firm_id': ['A', 'B', 'B', 'C', 'C'],
    'director_name': ['Tanaka', 'Tanaka', 'Suzuki', 'Suzuki', 'Sato'],
    'year': [2020, 2020, 2020, 2020, 2020]
})

print(directors)
```

### 1.3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰

```python
def build_board_network(director_df, year):
    """å–ç· å½¹å…¼ä»»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰
    
    Args:
        director_df: DataFrame with columns ['director_id', 'firm_id', 'year']
        year: Target year
        
    Returns:
        networkx.Graph: ä¼æ¥­é–“ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã‚°ãƒ©ãƒ•
        
    Raises:
        ValueError: Required columns missing
    """
    # Input validation
    required_cols = ['director_id', 'firm_id', 'year']
    missing_cols = set(required_cols) - set(director_df.columns)
    
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")
    
    # æŒ‡å®šå¹´ã®ãƒ‡ãƒ¼ã‚¿
    df_year = director_df[director_df['year'] == year]
    
    if len(df_year) == 0:
        print(f"Warning: No data for year {year}. Returning empty graph.")
        return nx.Graph()
    
    try:
        # Bipartite graph: å–ç· å½¹ - ä¼æ¥­
        B = nx.Graph()
        
        for _, row in df_year.iterrows():
            # Check for null values
            if pd.isna(row['director_id']) or pd.isna(row['firm_id']):
                continue
                
            B.add_node(row['director_id'], bipartite=0)  # å–ç· å½¹
            B.add_node(row['firm_id'], bipartite=1)      # ä¼æ¥­
            B.add_edge(row['director_id'], row['firm_id'])
        
        # Projection: ä¼æ¥­é–“ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        firm_nodes = {n for n, d in B.nodes(data=True) if d.get('bipartite') == 1}
        
        if len(firm_nodes) < 2:
            print(f"Warning: Only {len(firm_nodes)} firm(s) found. Network may be sparse.")
        
        G = nx.bipartite.weighted_projected_graph(B, firm_nodes)
        
        print(f"Network built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        
        return G
        
    except Exception as e:
        print(f"Error building network: {type(e).__name__}: {e}")
        return nx.Graph()

# å®Ÿè¡Œ
G_board = build_board_network(directors, 2020)
print(f"Nodes: {G_board.number_of_nodes()}, Edges: {G_board.number_of_edges()}")
```

### 1.4 ä¸­å¿ƒæ€§æŒ‡æ¨™

```python
# Degree Centrality: ç›´æ¥ãƒªãƒ³ã‚¯æ•°
degree_cent = nx.degree_centrality(G_board)

# Betweenness Centrality: åª’ä»‹æ€§
between_cent = nx.betweenness_centrality(G_board)

# Eigenvector Centrality: å½±éŸ¿åŠ›
eigen_cent = nx.eigenvector_centrality(G_board, max_iter=1000)

# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
centrality_df = pd.DataFrame({
    'firm_id': list(degree_cent.keys()),
    'degree_centrality': list(degree_cent.values()),
    'betweenness_centrality': list(between_cent.values()),
    'eigenvector_centrality': list(eigen_cent.values())
})

print(centrality_df)
```

### 1.5 å¤‰æ•°æ§‹ç¯‰

```python
# Panel dataç”¨ã«å¤‰æ•°è¿½åŠ 
df_firms = df_firms.merge(centrality_df, on='firm_id', how='left')

# æ¬ æå€¤å‡¦ç†ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœªå‚åŠ ä¼æ¥­ = 0ï¼‰
df_firms[['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']] = \
    df_firms[['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']].fillna(0)
```

### 1.6 å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

# Spring layout
pos = nx.spring_layout(G_board, seed=42)

# ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º = Degree Centrality
node_sizes = [degree_cent[node] * 5000 for node in G_board.nodes()]

# æç”»
nx.draw_networkx(
    G_board, pos, 
    node_size=node_sizes,
    node_color='lightblue',
    with_labels=True,
    font_size=8
)
plt.title('Board Interlock Network')
plt.axis('off')
plt.show()
```

---

## 2. Strategic Alliance Network

### 2.1 æ¦‚å¿µ

**Strategic Alliance**: ä¼æ¥­é–“ã®æˆ¦ç•¥çš„ææºï¼ˆåˆå¼ã€R&Då”åŠ›ã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ç­‰ï¼‰

**ç†è«–çš„æ ¹æ‹ **:
- Transaction Cost Economics: å¸‚å ´ vs éšå±¤ã®ä¸­é–“å½¢æ…‹
- Resource-Based View: è£œå®Œçš„è³‡æºã®ç²å¾—
- Learning Theory: çŸ¥è­˜ç§»è»¢

### 2.2 ãƒ‡ãƒ¼ã‚¿åé›†

```python
# ææºãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: SDC, LexisNexisï¼‰
alliances = pd.DataFrame({
    'firm_a': ['A', 'A', 'B', 'C'],
    'firm_b': ['B', 'C', 'C', 'D'],
    'alliance_type': ['R&D', 'JV', 'Marketing', 'R&D'],
    'year': [2020, 2020, 2020, 2020]
})
```

### 2.3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰

```python
def build_alliance_network(alliance_df, year):
    """ææºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    df_year = alliance_df[alliance_df['year'] == year]
    
    G = nx.Graph()
    
    for _, row in df_year.iterrows():
        G.add_edge(row['firm_a'], row['firm_b'], 
                   alliance_type=row['alliance_type'])
    
    return G

G_alliance = build_alliance_network(alliances, 2020)
```

### 2.4 ææºãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæŒ‡æ¨™

```python
def calculate_alliance_metrics(G, firm_id):
    """ææºãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæŒ‡æ¨™ã‚’è¨ˆç®—
    
    Args:
        G: NetworkX graph
        firm_id: Target firm ID
        
    Returns:
        dict: ææºæŒ‡æ¨™ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    """
    if G is None or G.number_of_nodes() == 0:
        print("Warning: Empty graph provided.")
        return {
            'alliance_count': 0,
            'partner_diversity': 0,
            'network_constraint': None
        }
    
    if firm_id not in G.nodes():
        return {
            'alliance_count': 0,
            'partner_diversity': 0,
            'network_constraint': None
        }
    
    try:
        # ææºæ•°
        alliance_count = G.degree(firm_id)
        
        # Partner Diversity
        partners = list(G.neighbors(firm_id))
        partner_diversity = len(partners)
        
        # Network Constraint (Burt's Structural Holes)
        try:
            constraint = nx.constraint(G, firm_id)
        except:
            constraint = None
        
        return {
            'alliance_count': alliance_count,
            'partner_diversity': partner_diversity,
            'network_constraint': constraint
        }
        
    except Exception as e:
        print(f"Error calculating alliance metrics for {firm_id}: {type(e).__name__}: {e}")
        return {
            'alliance_count': 0,
            'partner_diversity': 0,
            'network_constraint': None
        }

# å…¨ä¼æ¥­ã®æŒ‡æ¨™è¨ˆç®—
metrics = []
for firm in G_alliance.nodes():
    m = calculate_alliance_metrics(G_alliance, firm)
    m['firm_id'] = firm
    metrics.append(m)

alliance_metrics = pd.DataFrame(metrics)
print(alliance_metrics)
```

---

## 3. Patent Citation Network

### 3.3 æ¦‚å¿µ

**Patent Citation**: ç‰¹è¨±AãŒç‰¹è¨±Bã‚’å¼•ç”¨ â†’ çŸ¥è­˜ãƒ•ãƒ­ãƒ¼

**ç†è«–çš„æ ¹æ‹ **:
- Knowledge-Based View: çŸ¥è­˜ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼
- Innovation Theory: æŠ€è¡“è»Œé“ã®è¿½è·¡
- Absorptive Capacity: å¤–éƒ¨çŸ¥è­˜ã®å¸å

### 3.2 ãƒ‡ãƒ¼ã‚¿åé›†

```python
# ç‰¹è¨±å¼•ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆä¾‹: USPTO, JPOï¼‰
citations = pd.DataFrame({
    'citing_patent': ['P1', 'P2', 'P3', 'P4'],
    'cited_patent': ['P0', 'P0', 'P1', 'P2'],
    'citing_firm': ['A', 'B', 'A', 'C'],
    'cited_firm': ['X', 'X', 'A', 'B'],
    'year': [2020, 2020, 2020, 2020]
})
```

### 3.3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰

```python
def build_citation_network(citation_df, year):
    """ç‰¹è¨±å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ï¼ˆæœ‰å‘ã‚°ãƒ©ãƒ•ï¼‰"""
    df_year = citation_df[citation_df['year'] == year]
    
    G = nx.DiGraph()
    
    for _, row in df_year.iterrows():
        # ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        if row['citing_firm'] != row['cited_firm']:  # è‡ªå·±å¼•ç”¨é™¤å¤–
            G.add_edge(row['citing_firm'], row['cited_firm'])
    
    return G

G_patent = build_citation_network(citations, 2020)
```

### 3.4 çŸ¥è­˜ãƒ•ãƒ­ãƒ¼æŒ‡æ¨™

```python
def calculate_knowledge_flow(G, firm_id):
    """çŸ¥è­˜ãƒ•ãƒ­ãƒ¼æŒ‡æ¨™ã‚’è¨ˆç®—
    
    Args:
        G: NetworkX DiGraph (æœ‰å‘ã‚°ãƒ©ãƒ•)
        firm_id: Target firm ID
        
    Returns:
        dict: çŸ¥è­˜ãƒ•ãƒ­ãƒ¼æŒ‡æ¨™ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    """
    if G is None or G.number_of_nodes() == 0:
        print("Warning: Empty graph provided.")
        return {
            'knowledge_inflow': 0,
            'knowledge_outflow': 0,
            'knowledge_diversity': 0
        }
    
    if not isinstance(G, nx.DiGraph):
        print("Warning: Graph is not directed. Converting to DiGraph.")
        G = G.to_directed()
    
    if firm_id not in G.nodes():
        return {
            'knowledge_inflow': 0,
            'knowledge_outflow': 0,
            'knowledge_diversity': 0
        }
    
    try:
        # Inflow: å½“è©²ä¼æ¥­ãŒä»–ç¤¾ã‚’å¼•ç”¨ï¼ˆout-degreeï¼‰
        inflow = G.out_degree(firm_id)
        
        # Outflow: ä»–ç¤¾ãŒå½“è©²ä¼æ¥­ã‚’å¼•ç”¨ï¼ˆin-degreeï¼‰
        outflow = G.in_degree(firm_id)
        
        # Diversity: å¼•ç”¨å…ƒä¼æ¥­ã®å¤šæ§˜æ€§
        sources = list(G.predecessors(firm_id))
        diversity = len(set(sources))
        
        return {
            'knowledge_inflow': inflow,
            'knowledge_outflow': outflow,
            'knowledge_diversity': diversity
        }
        
    except Exception as e:
        print(f"Error calculating knowledge flow for {firm_id}: {type(e).__name__}: {e}")
        return {
            'knowledge_inflow': 0,
            'knowledge_outflow': 0,
            'knowledge_diversity': 0
        }

# è¨ˆç®—
kf_metrics = []
for firm in G_patent.nodes():
    m = calculate_knowledge_flow(G_patent, firm)
    m['firm_id'] = firm
    kf_metrics.append(m)

kf_df = pd.DataFrame(kf_metrics)
print(kf_df)
```

### 3.5 Self-Citationç‡

```python
def calculate_self_citation_rate(citation_df, firm_id, year):
    """è‡ªå·±å¼•ç”¨ç‡ã‚’è¨ˆç®—"""
    df_firm = citation_df[
        (citation_df['citing_firm'] == firm_id) &
        (citation_df['year'] == year)
    ]
    
    if len(df_firm) == 0:
        return 0
    
    self_citations = len(df_firm[df_firm['cited_firm'] == firm_id])
    total_citations = len(df_firm)
    
    return self_citations / total_citations

# ä¾‹
rate = calculate_self_citation_rate(citations, 'A', 2020)
print(f"Self-citation rate: {rate:.2%}")
```

---

## 4. Quick Reference

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŒ‡æ¨™ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | æ„å‘³ | è§£é‡ˆ |
|------|------|------|
| **Degree Centrality** | ç›´æ¥ãƒªãƒ³ã‚¯æ•° | é«˜ã„ = å¤šãã®ä¼æ¥­ã¨æ¥ç¶š |
| **Betweenness Centrality** | åª’ä»‹æ€§ | é«˜ã„ = ãƒ–ãƒ­ãƒ¼ã‚«ãƒ¼çš„ä½ç½® |
| **Eigenvector Centrality** | å½±éŸ¿åŠ› | é«˜ã„ = é‡è¦ä¼æ¥­ã¨æ¥ç¶š |
| **Network Constraint** | åˆ¶ç´„åº¦ | ä½ã„ = Structural Holes |
| **Clustering Coefficient** | ã‚¯ãƒ©ã‚¹ã‚¿æ€§ | é«˜ã„ = å¯†ãªã‚°ãƒ«ãƒ¼ãƒ— |

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥å¤‰æ•°

| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ | ä¸»è¦å¤‰æ•° | ç†è«–çš„æ„å‘³ |
|------------|---------|-----------|
| **Board Interlock** | Degree, Betweenness | æƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹ã€å½±éŸ¿åŠ› |
| **Alliance** | Alliance Count, Diversity | è³‡æºã‚¢ã‚¯ã‚»ã‚¹ã€å­¦ç¿’æ©Ÿä¼š |
| **Patent Citation** | Inflow, Outflow | çŸ¥è­˜å¸åã€çŸ¥è­˜æ‹¡æ•£ |

### æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ä¾‹

**ä»®èª¬ä¾‹1**: Board Interlock â†’ Strategic Change
```python
# H: å–ç· å½¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§ãŒé«˜ã„ä¼æ¥­ã¯ã€æˆ¦ç•¥å¤‰æ›´ãŒæ—©ã„
model = PanelOLS.from_formula(
    'strategy_change ~ degree_centrality + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

**ä»®èª¬ä¾‹2**: Alliance Diversity â†’ Innovation
```python
# H: ææºå…ˆã®å¤šæ§˜æ€§ãŒé«˜ã„ä¼æ¥­ã¯ã€ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆæœãŒé«˜ã„
model = PanelOLS.from_formula(
    'patent_count ~ partner_diversity + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

**ä»®èª¬ä¾‹3**: Knowledge Inflow â†’ Performance
```python
# H: å¤–éƒ¨çŸ¥è­˜ã®æµå…¥ãŒå¤šã„ä¼æ¥­ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé«˜ã„
model = PanelOLS.from_formula(
    'roa ~ knowledge_inflow + knowledge_outflow + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install networkx pandas numpy matplotlib
```

---

## å‚è€ƒæ–‡çŒ®

- Borgatti, S. P., & Foster, P. C. (2003). "The network paradigm in organizational research." *Journal of Management*, 29(6), 991-1013.
- Gulati, R. (1999). "Network location and learning." *Strategic Management Journal*, 20(5), 397-420.
- Brass, D. J., et al. (2004). "Taking stock of networks and organizations." *Academy of Management Journal*, 47(6), 795-817.

---

**Version**: 4.0  
**Last Updated**: 2025-11-01  
**Next**: `6-causal-ml`, `7-esg-sustainability` skills
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01

## 5. å‹•çš„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼ˆæ™‚ç³»åˆ—å¤‰åŒ–ï¼‰

### 5.1 æ¦‚å¿µ

**Dynamic Network Analysis**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’è¿½è·¡

**ç ”ç©¶ã§ã®æ´»ç”¨**:
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®‰å®šæ€§ã®æ¸¬å®š
- æ–°è¦tieå½¢æˆãƒ‘ã‚¿ãƒ¼ãƒ³
- Centralityå¤‰åŒ–ã¨ä¼æ¥­æˆ¦ç•¥ã®é–¢ä¿‚
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€²åŒ–ã®äºˆæ¸¬

### 5.2 æ™‚ç³»åˆ—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰

```python
import networkx as nx
import pandas as pd

def build_temporal_networks(director_df, year_range):
    """è¤‡æ•°å¹´åº¦ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰
    
    Args:
        director_df: Director data with 'year' column
        year_range: List of years to analyze
        
    Returns:
        dict: {year: NetworkX graph}
    """
    networks = {}
    
    for year in year_range:
        G = build_board_network(director_df, year)
        networks[year] = G
        
        print(f"{year}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    
    return networks

# ä½¿ç”¨ä¾‹: 2018-2023ã®6å¹´é–“
temporal_networks = build_temporal_networks(
    directors,
    year_range=range(2018, 2024)
)

# çµæœä¾‹:
# 2018: 245 nodes, 389 edges
# 2019: 251 nodes, 412 edges
# 2020: 248 nodes, 398 edges
# 2021: 253 nodes, 425 edges
# 2022: 256 nodes, 441 edges
# 2023: 259 nodes, 456 edges
```

---

### 5.3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰åŒ–ã®æ¸¬å®š

#### 5.3.1 Jaccardä¿‚æ•°ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®‰å®šæ€§ï¼‰

```python
def calculate_network_stability(G1, G2):
    """2æ™‚ç‚¹é–“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®‰å®šæ€§ã‚’æ¸¬å®š
    
    Args:
        G1: Network at time t
        G2: Network at time t+1
        
    Returns:
        dict: Stability metrics
    """
    edges_t1 = set(G1.edges())
    edges_t2 = set(G2.edges())
    
    # Jaccard similarity
    intersection = edges_t1 & edges_t2
    union = edges_t1 | edges_t2
    
    jaccard = len(intersection) / len(union) if union else 0
    
    # Edge persistence rate
    persistence = len(intersection) / len(edges_t1) if edges_t1 else 0
    
    # New tie formation rate
    new_ties = edges_t2 - edges_t1
    new_tie_rate = len(new_ties) / len(edges_t2) if edges_t2 else 0
    
    # Lost tie rate
    lost_ties = edges_t1 - edges_t2
    lost_tie_rate = len(lost_ties) / len(edges_t1) if edges_t1 else 0
    
    return {
        'jaccard_similarity': jaccard,
        'edge_persistence_rate': persistence,
        'new_tie_formation_rate': new_tie_rate,
        'lost_tie_rate': lost_tie_rate,
        'new_ties_count': len(new_ties),
        'lost_ties_count': len(lost_ties)
    }

# å¹´åº¦é–“ã®å®‰å®šæ€§ã‚’è¨ˆç®—
stability_metrics = []

for year in range(2018, 2023):
    G_t = temporal_networks[year]
    G_t1 = temporal_networks[year + 1]
    
    stability = calculate_network_stability(G_t, G_t1)
    stability['year_from'] = year
    stability['year_to'] = year + 1
    
    stability_metrics.append(stability)

df_stability = pd.DataFrame(stability_metrics)
print(df_stability)

# çµæœä¾‹:
#    year_from  year_to  jaccard  persistence  new_tie_rate  lost_tie_rate
# 0       2018     2019    0.756        0.823         0.167          0.177
# 1       2019     2020    0.742        0.809         0.191          0.191
# 2       2020     2021    0.768        0.831         0.169          0.169
# 3       2021     2022    0.771        0.835         0.165          0.165
# 4       2022     2023    0.779        0.841         0.159          0.159

# ç™ºè¦‹: Jaccardä¿‚æ•°ãŒé«˜ã„ï¼ˆ0.75+ï¼‰â†’ Board Interlock networkã¯æ¯”è¼ƒçš„å®‰å®š
```

---

#### 5.3.2 Centralityå¤‰åŒ–ç‡

```python
def calculate_centrality_change(temporal_networks, year_range):
    """CentralityæŒ‡æ¨™ã®çµŒå¹´å¤‰åŒ–ã‚’è¨ˆç®—
    
    Returns:
        DataFrame: Panel data with centrality changes
    """
    records = []
    
    for year in year_range:
        G = temporal_networks[year]
        
        # Centralityè¨ˆç®—
        degree_cent = nx.degree_centrality(G)
        between_cent = nx.betweenness_centrality(G)
        
        try:
            eigen_cent = nx.eigenvector_centrality(G, max_iter=1000)
        except:
            eigen_cent = {node: 0 for node in G.nodes()}
        
        for node in G.nodes():
            records.append({
                'firm_id': node,
                'year': year,
                'degree_centrality': degree_cent.get(node, 0),
                'betweenness_centrality': between_cent.get(node, 0),
                'eigenvector_centrality': eigen_cent.get(node, 0)
            })
    
    df_panel = pd.DataFrame(records)
    df_panel = df_panel.sort_values(['firm_id', 'year'])
    
    # Year-over-year change
    for metric in ['degree_centrality', 'betweenness_centrality', 'eigenvector_centrality']:
        df_panel[f'{metric}_change'] = df_panel.groupby('firm_id')[metric].diff()
        df_panel[f'{metric}_pct_change'] = df_panel.groupby('firm_id')[metric].pct_change()
    
    return df_panel

# å®Ÿè¡Œ
df_centrality_panel = calculate_centrality_change(
    temporal_networks,
    year_range=range(2018, 2024)
)

print(df_centrality_panel[['firm_id', 'year', 'degree_centrality', 'degree_centrality_change']].head(10))

# çµæœä¾‹:
#   firm_id  year  degree_centrality  degree_centrality_change
# 0    AAPL  2018              0.145                      NaN
# 1    AAPL  2019              0.152                    0.007
# 2    AAPL  2020              0.148                   -0.004
# 3    AAPL  2021              0.156                    0.008
# 4    AAPL  2022              0.161                    0.005
```

---

#### 5.3.3 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯†åº¦ã®å¤‰åŒ–

```python
def calculate_network_density_trend(temporal_networks, year_range):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯†åº¦ã®çµŒå¹´å¤‰åŒ–"""
    
    density_metrics = []
    
    for year in year_range:
        G = temporal_networks[year]
        
        density = nx.density(G)
        avg_degree = sum(dict(G.degree()).values()) / G.number_of_nodes() if G.number_of_nodes() > 0 else 0
        
        # Clustering coefficient
        try:
            clustering = nx.average_clustering(G)
        except:
            clustering = 0
        
        # Connected components
        num_components = nx.number_connected_components(G)
        largest_component_size = len(max(nx.connected_components(G), key=len)) if G.number_of_nodes() > 0 else 0
        
        density_metrics.append({
            'year': year,
            'nodes': G.number_of_nodes(),
            'edges': G.number_of_edges(),
            'density': density,
            'avg_degree': avg_degree,
            'clustering_coef': clustering,
            'num_components': num_components,
            'largest_component_size': largest_component_size,
            'largest_component_pct': largest_component_size / G.number_of_nodes() * 100 if G.number_of_nodes() > 0 else 0
        })
    
    return pd.DataFrame(density_metrics)

df_density = calculate_network_density_trend(temporal_networks, range(2018, 2024))

print(df_density)

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Density
axes[0, 0].plot(df_density['year'], df_density['density'], marker='o')
axes[0, 0].set_title('Network Density')
axes[0, 0].set_ylabel('Density')

# Average Degree
axes[0, 1].plot(df_density['year'], df_density['avg_degree'], marker='s', color='orange')
axes[0, 1].set_title('Average Degree')
axes[0, 1].set_ylabel('Avg Degree')

# Clustering Coefficient
axes[1, 0].plot(df_density['year'], df_density['clustering_coef'], marker='^', color='green')
axes[1, 0].set_title('Clustering Coefficient')
axes[1, 0].set_ylabel('Clustering')
axes[1, 0].set_xlabel('Year')

# Largest Component %
axes[1, 1].plot(df_density['year'], df_density['largest_component_pct'], marker='d', color='red')
axes[1, 1].set_title('Largest Component Size')
axes[1, 1].set_ylabel('% of Network')
axes[1, 1].set_xlabel('Year')

plt.tight_layout()
plt.savefig('network_evolution.png', dpi=300)
plt.show()
```

---

### 5.4 å‹•çš„å¯è¦–åŒ–

```python
def create_network_animation(temporal_networks, year_range, output_file='network_evolution.gif'):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ™‚ç³»åˆ—ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
    
    Requires: matplotlib, imageio
    """
    import matplotlib.pyplot as plt
    import imageio
    import os
    
    # Consistent layoutï¼ˆå…¨å¹´åº¦ã§åŒã˜ä½ç½®ï¼‰
    all_nodes = set()
    for G in temporal_networks.values():
        all_nodes.update(G.nodes())
    
    # Kamada-Kawai layoutï¼ˆå®‰å®šçš„ï¼‰
    G_combined = nx.Graph()
    G_combined.add_nodes_from(all_nodes)
    for G in temporal_networks.values():
        G_combined.add_edges_from(G.edges())
    
    pos = nx.kamada_kawai_layout(G_combined)
    
    # å„å¹´åº¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    filenames = []
    
    for year in year_range:
        G = temporal_networks[year]
        
        plt.figure(figsize=(10, 8))
        
        # Node size = Degree Centrality
        degree_cent = nx.degree_centrality(G)
        node_sizes = [degree_cent.get(node, 0) * 3000 for node in G.nodes()]
        
        # Draw
        nx.draw_networkx_nodes(
            G, pos,
            node_size=node_sizes,
            node_color='lightblue',
            alpha=0.7
        )
        
        nx.draw_networkx_edges(
            G, pos,
            alpha=0.3,
            width=0.5
        )
        
        # Labelsï¼ˆä¸»è¦ãƒãƒ¼ãƒ‰ã®ã¿ï¼‰
        top_nodes = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:10]
        labels = {node: node for node, _ in top_nodes}
        nx.draw_networkx_labels(G, pos, labels, font_size=8)
        
        plt.title(f'Board Interlock Network - {year}', fontsize=16)
        plt.axis('off')
        
        # Save frame
        filename = f'frame_{year}.png'
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.close()
        
        filenames.append(filename)
    
    # Create GIF
    images = [imageio.imread(filename) for filename in filenames]
    imageio.mimsave(output_file, images, duration=1.0)  # 1ç§’/ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    # Clean up
    for filename in filenames:
        os.remove(filename)
    
    print(f"Animation saved: {output_file}")

# å®Ÿè¡Œ
create_network_animation(temporal_networks, range(2018, 2024))
```

---

### 5.5 æˆ¦ç•¥ç ”ç©¶ã§ã®æ´»ç”¨

#### ä»®èª¬1: Centralityå¤‰åŒ– â†’ æˆ¦ç•¥å¤‰æ›´

```python
from linearmodels.panel import PanelOLS

# Centrality change dataã¨strategy change dataã‚’ãƒãƒ¼ã‚¸
df_merged = df_centrality_panel.merge(df_strategy, on=['firm_id', 'year'])

# Panel regression
model = PanelOLS.from_formula(
    'strategy_change ~ degree_centrality_change + betweenness_centrality_change + controls + EntityEffects + TimeEffects',
    data=df_merged.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)

print(model.summary)

# æœŸå¾…: Centralityæ€¥å¢— â†’ Strategic repositioning
```

#### ä»®èª¬2: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®‰å®šæ€§ â†’ Performance volatility

```python
# ä¼æ¥­åˆ¥ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®‰å®šæ€§ï¼ˆå¹³å‡Jaccardï¼‰
firm_stability = []

for firm_id in df_centrality_panel['firm_id'].unique():
    # ãã®ä¼æ¥­ãŒé–¢ä¸ã™ã‚‹edgeã®å®‰å®šæ€§
    # ï¼ˆç°¡ç•¥ç‰ˆ: ä¼æ¥­ã®Centralityå¤‰å‹•æ€§ã§ä»£ç”¨ï¼‰
    
    firm_data = df_centrality_panel[df_centrality_panel['firm_id'] == firm_id]
    
    if len(firm_data) > 2:
        cent_volatility = firm_data['degree_centrality'].std()
        
        firm_stability.append({
            'firm_id': firm_id,
            'centrality_volatility': cent_volatility
        })

df_firm_stability = pd.DataFrame(firm_stability)

# Merge with performance data
df_analysis = df_firm_stability.merge(df_performance, on='firm_id')

# Correlation
print(df_analysis[['centrality_volatility', 'roa_volatility']].corr())

# ä»®èª¬: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½ç½®ãŒä¸å®‰å®š â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚‚ä¸å®‰å®š
```

---

### 5.6 Network Event Analysis

```python
def detect_network_events(temporal_networks, year_range, threshold=0.1):
    """é‡è¦ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º
    
    Args:
        threshold: Centralityå¤‰åŒ–ã®é–¾å€¤ï¼ˆ10%ä»¥ä¸Šã§ã€Œé‡è¦ã€ï¼‰
        
    Returns:
        DataFrame: Detected events
    """
    events = []
    
    df_cent = calculate_centrality_change(temporal_networks, year_range)
    
    for firm_id in df_cent['firm_id'].unique():
        firm_data = df_cent[df_cent['firm_id'] == firm_id].copy()
        firm_data = firm_data.sort_values('year')
        
        for idx in range(1, len(firm_data)):
            row = firm_data.iloc[idx]
            
            # Degree centralityã®å¤§å¹…å¤‰åŒ–
            if abs(row['degree_centrality_pct_change']) > threshold:
                event_type = 'Centrality Surge' if row['degree_centrality_pct_change'] > 0 else 'Centrality Drop'
                
                events.append({
                    'firm_id': firm_id,
                    'year': row['year'],
                    'event_type': event_type,
                    'centrality_change_pct': row['degree_centrality_pct_change'] * 100,
                    'centrality_before': firm_data.iloc[idx-1]['degree_centrality'],
                    'centrality_after': row['degree_centrality']
                })
    
    return pd.DataFrame(events)

# ã‚¤ãƒ™ãƒ³ãƒˆæ¤œå‡º
df_events = detect_network_events(temporal_networks, range(2018, 2024), threshold=0.15)

print(f"Detected {len(df_events)} significant network events")
print(df_events.sort_values('centrality_change_pct', ascending=False).head(10))

# ã“ã‚Œã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆã¨ä¼æ¥­æˆ¦ç•¥ãƒ»æ¥­ç¸¾ã®é–¢ä¿‚ã‚’åˆ†æ
# ä¾‹: Centralityæ€¥å¢—ã®ç¿Œå¹´ã«M&Aã‚„æ–°è¦äº‹æ¥­å‚å…¥ãŒå¤šã„ã‹ï¼Ÿ
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
