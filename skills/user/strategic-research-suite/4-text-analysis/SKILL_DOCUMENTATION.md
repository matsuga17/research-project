---
name: strategic-research-text-analysis
description: Advanced text analysis toolkit for strategic management research including SEC 10-K MD&A extraction, sentiment analysis (VADER, Loughran-McDonald), forward-looking statements measurement, earnings call transcript analysis, and strategic theme extraction using topic modeling.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 6 (Variable Construction)
  - data-sources: Text data collection
  - statistical-methods: Text variables in regression
---

# Text Analysis Toolkit v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥ç ”ç©¶ã§è³ªçš„ãƒ‡ãƒ¼ã‚¿ã‚’å®šé‡åŒ–ã™ã‚‹**ãƒ†ã‚­ã‚¹ãƒˆåˆ†ææ‰‹æ³•**ã‚’æä¾›ã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… çµŒå–¶è€…ã®æˆ¦ç•¥çš„æ„å›³ã‚’æ¸¬å®šã—ãŸã„
- âœ… MD&Aï¼ˆManagement Discussion & Analysisï¼‰åˆ†æ
- âœ… æ±ºç®—èª¬æ˜ä¼štranscriptã®æˆ¦ç•¥ãƒ†ãƒ¼ãƒæŠ½å‡º
- âœ… Forward-looking statementså®šé‡åŒ–
- âœ… ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆæ¥½è¦³åº¦ãƒ»ä¸ç¢ºå®Ÿæ€§æ¸¬å®šï¼‰

### å‰ææ¡ä»¶

- PythonåŸºç¤ï¼ˆNLTK, scikit-learnï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®åŸºç¤çŸ¥è­˜
- è‡ªç„¶è¨€èªå‡¦ç†ã®æ¦‚å¿µ

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

- **ãƒ‡ãƒ¼ã‚¿çµ±åˆ** â†’ `1-core-workflow` Phase 6
- **çµ±è¨ˆåˆ†æ** â†’ `3-statistical-methods`
- **ãƒ‡ãƒ¼ã‚¿åé›†** â†’ `2-data-sources`ï¼ˆSEC EDGARï¼‰

---

## ğŸ“‹ ç›®æ¬¡

1. [SEC 10-K MD&Aåˆ†æ](#1-sec-10-k-mdaåˆ†æ)
2. [ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ](#2-ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ)
3. [ãƒ†ãƒ¼ãƒæŠ½å‡º](#3-ãƒ†ãƒ¼ãƒæŠ½å‡º)
4. [Quick Reference](#4-quick-reference)

---

## 1. SEC 10-K MD&Aåˆ†æ

### 1.1 ãƒ‡ãƒ¼ã‚¿åé›†

```python
import requests
from bs4 import BeautifulSoup
import re

class SECTextCollector:
    """SEC EDGARã‹ã‚‰MD&Aãƒ†ã‚­ã‚¹ãƒˆã‚’åé›†"""
    
    def __init__(self):
        self.base_url = "https://www.sec.gov/cgi-bin/browse-edgar"
        self.headers = {'User-Agent': 'YourUniversity research@email.edu'}
    
    def get_10k_url(self, cik, year):
        """10-K URLã‚’å–å¾—"""
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',
            'dateb': f'{year}1231',
            'count': 1
        }
        
        response = requests.get(self.base_url, params=params, headers=self.headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 10-Kæ–‡æ›¸URL
        doc_link = soup.find('a', id='documentsbutton')
        if doc_link:
            return 'https://www.sec.gov' + doc_link['href']
        return None
    
    def extract_mda(self, filing_url, max_retries=3):
        """MD&Aï¼ˆItem 7ï¼‰ã‚’æŠ½å‡º
        
        Args:
            filing_url: SEC filing URL
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            str: MD&Aãƒ†ã‚­ã‚¹ãƒˆã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
        """
        import time
        
        for attempt in range(max_retries):
            try:
                response = requests.get(
                    filing_url, 
                    headers=self.headers,
                    timeout=30
                )
                response.raise_for_status()
                html = response.text
                
                # Item 7ã‚’æ¤œç´¢
                mda_pattern = r'Item\s*7\..*?(?=Item\s*8\.)'
                match = re.search(mda_pattern, html, re.DOTALL | re.IGNORECASE)
                
                if match:
                    mda_text = match.group(0)
                    soup = BeautifulSoup(mda_text, 'html.parser')
                    clean_text = soup.get_text()
                    
                    # ç©ºç™½ãƒã‚§ãƒƒã‚¯
                    if len(clean_text.strip()) < 100:
                        raise ValueError("MD&A text too short (< 100 chars)")
                        
                    return clean_text
                else:
                    # Alternative pattern (Item VII)
                    mda_pattern_alt = r'Item\s*VII\..*?(?=Item\s*VIII\.)'
                    match_alt = re.search(mda_pattern_alt, html, re.DOTALL | re.IGNORECASE)
                    
                    if match_alt:
                        mda_text = match_alt.group(0)
                        soup = BeautifulSoup(mda_text, 'html.parser')
                        return soup.get_text()
                    
                    raise ValueError("MD&A section not found in filing")
                    
            except requests.exceptions.Timeout:
                print(f"Attempt {attempt + 1}/{max_retries}: Request timed out")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    print("Max retries reached. Returning None.")
                    return None
                    
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:
                    print(f"Rate limit hit. Waiting 60 seconds...")
                    time.sleep(60)
                else:
                    print(f"HTTP Error: {e}")
                    return None
                    
            except ValueError as e:
                print(f"Parsing error: {e}")
                return None
                
            except Exception as e:
                print(f"Unexpected error: {type(e).__name__}: {e}")
                return None
        
        return None

# ä½¿ç”¨ä¾‹
collector = SECTextCollector()
mda_text = collector.extract_mda('https://www.sec.gov/...')
print(f"MD&A length: {len(mda_text)} characters")
```

---

## 2. ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ

### 2.1 VADER Sentiment

```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

def analyze_vader_sentiment(text):
    """VADER ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
    
    Args:
        text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        dict: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
    """
    if text is None or len(text.strip()) == 0:
        print("Warning: Empty text provided. Returning None.")
        return None
    
    try:
        analyzer = SentimentIntensityAnalyzer()
        scores = analyzer.polarity_scores(text)
        
        return {
            'vader_positive': scores['pos'],
            'vader_negative': scores['neg'],
            'vader_neutral': scores['neu'],
            'vader_compound': scores['compound']
        }
        
    except Exception as e:
        print(f"VADER analysis error: {type(e).__name__}: {e}")
        return None

# ä½¿ç”¨ä¾‹
sentiment = analyze_vader_sentiment(mda_text)
print(sentiment)
```

### 2.2 Loughran-McDonald Financial Dictionary

```python
import pandas as pd

def load_lm_dictionary():
    """Loughran-McDonaldè¾æ›¸ã‚’èª­ã¿è¾¼ã¿"""
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: https://sraf.nd.edu/loughranmcdonald-master-dictionary/
    
    lm_dict = {
        'positive': ['achieve', 'strong', 'improve', 'gain', 'success'],
        'negative': ['loss', 'decline', 'weak', 'difficult', 'risk'],
        'uncertainty': ['uncertain', 'may', 'could', 'approximate', 'unclear']
    }
    
    return lm_dict

def analyze_lm_sentiment(text, lm_dict):
    """Loughran-McDonald ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
    
    Args:
        text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        lm_dict: LMè¾æ›¸
        
    Returns:
        dict: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
    """
    if text is None or len(text.strip()) == 0:
        print("Warning: Empty text provided. Returning None.")
        return None
        
    if not lm_dict or not all(k in lm_dict for k in ['positive', 'negative', 'uncertainty']):
        print("Error: Invalid LM dictionary. Must contain 'positive', 'negative', 'uncertainty' keys.")
        return None
    
    try:
        words = text.lower().split()
        
        if len(words) == 0:
            print("Warning: No words found after tokenization.")
            return None
        
        pos_count = sum(1 for w in words if w in lm_dict['positive'])
        neg_count = sum(1 for w in words if w in lm_dict['negative'])
        unc_count = sum(1 for w in words if w in lm_dict['uncertainty'])
        
        total = len(words)
        
        return {
            'lm_positive_ratio': pos_count / total,
            'lm_negative_ratio': neg_count / total,
            'lm_uncertainty_ratio': unc_count / total,
            'lm_polarity': (pos_count - neg_count) / total
        }
        
    except Exception as e:
        print(f"LM sentiment analysis error: {type(e).__name__}: {e}")
        return None

lm_dict = load_lm_dictionary()
lm_sentiment = analyze_lm_sentiment(mda_text, lm_dict)
print(lm_sentiment)
```

### 2.3 Forward-Looking Statements

```python
def measure_forward_looking(text):
    """Forward-looking statementsæ¸¬å®š"""
    
    forward_keywords = [
        'expect', 'anticipate', 'believe', 'plan', 'intend',
        'estimate', 'project', 'forecast', 'will', 'future'
    ]
    
    words = text.lower().split()
    fl_count = sum(1 for w in words if w in forward_keywords)
    
    return {
        'forward_looking_ratio': fl_count / len(words),
        'forward_looking_count': fl_count
    }

fl_measures = measure_forward_looking(mda_text)
print(fl_measures)
```

---

## 3. ãƒ†ãƒ¼ãƒæŠ½å‡º

### 3.1 Topic Modeling (LDA)

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

def extract_topics(documents, n_topics=5):
    """LDAã§ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    
    Args:
        documents: ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
        n_topics: ãƒˆãƒ”ãƒƒã‚¯æ•°
        
    Returns:
        tuple: (topics, lda_model, vectorizer) ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ™‚ã¯(None, None, None)
    """
    if not documents or len(documents) == 0:
        print("Error: Empty documents list.")
        return None, None, None
    
    # Remove empty documents
    documents = [d for d in documents if d and len(d.strip()) > 0]
    
    if len(documents) < n_topics:
        print(f"Warning: Number of documents ({len(documents)}) < n_topics ({n_topics})")
        print("Reducing n_topics to match document count.")
        n_topics = max(1, len(documents))
    
    try:
        # Vectorization
        vectorizer = CountVectorizer(
            max_features=1000,
            stop_words='english',
            min_df=min(2, len(documents))  # Adjust min_df for small corpora
        )
        
        doc_term_matrix = vectorizer.fit_transform(documents)
        
        # Check if vocabulary is empty
        if doc_term_matrix.shape[1] == 0:
            print("Error: No features extracted. Check document content and stop words.")
            return None, None, None
        
        # LDA
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42,
            max_iter=20
        )
        
        lda.fit(doc_term_matrix)
        
        # ãƒˆãƒ”ãƒƒã‚¯å˜èª
        feature_names = vectorizer.get_feature_names_out()
        
        topics = []
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topics.append({
                'topic_id': topic_idx,
                'top_words': top_words
            })
        
        return topics, lda, vectorizer
        
    except ValueError as e:
        print(f"LDA error: {e}")
        print("Check: (1) Document quality, (2) n_topics parameter, (3) min_df setting")
        return None, None, None
        
    except Exception as e:
        print(f"Unexpected error in LDA: {type(e).__name__}: {e}")
        return None, None, None

# ä½¿ç”¨ä¾‹
documents = [mda_text_1, mda_text_2, ...]  # è¤‡æ•°ä¼æ¥­ã®MD&A
topics, lda_model, vectorizer = extract_topics(documents, n_topics=5)

for topic in topics:
    print(f"Topic {topic['topic_id']}: {', '.join(topic['top_words'])}")
```

### 3.2 ä¼æ¥­åˆ¥ãƒˆãƒ”ãƒƒã‚¯é…åˆ†

```python
def assign_topics_to_firms(documents, lda_model, vectorizer):
    """ä¼æ¥­ã«ãƒˆãƒ”ãƒƒã‚¯ã‚’å‰²ã‚Šå½“ã¦"""
    
    doc_term_matrix = vectorizer.transform(documents)
    topic_distribution = lda_model.transform(doc_term_matrix)
    
    # å„ä¼æ¥­ã®ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯
    df_topics = pd.DataFrame(
        topic_distribution,
        columns=[f'topic_{i}' for i in range(topic_distribution.shape[1])]
    )
    
    df_topics['dominant_topic'] = df_topics.idxmax(axis=1)
    
    return df_topics

df_firm_topics = assign_topics_to_firms(documents, lda_model, vectorizer)
print(df_firm_topics.head())
```

---

## 4. Quick Reference

### ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°ã®æ´»ç”¨

| å¤‰æ•° | æ¸¬å®šæ–¹æ³• | æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ |
|------|---------|----------------|
| **Sentiment (Positive)** | VADER/LM | çµŒå–¶è€…æ¥½è¦³åº¦ â†’ Investment |
| **Sentiment (Negative)** | VADER/LM | ãƒªã‚¹ã‚¯èªè­˜ â†’ Risk-taking |
| **Uncertainty** | LM Uncertainty | ç’°å¢ƒä¸ç¢ºå®Ÿæ€§ â†’ Strategy Change |
| **Forward-Looking** | Keyword Count | æˆ¦ç•¥å¿—å‘ â†’ Innovation |
| **Topic Distribution** | LDA | æˆ¦ç•¥ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ â†’ Performance |

### æˆ¦ç•¥ç ”ç©¶ã§ã®ä»®èª¬ä¾‹

**H1**: MD&A Positive Sentiment â†’ R&D Investment
```python
from linearmodels.panel import PanelOLS

model = PanelOLS.from_formula(
    'rd_intensity ~ vader_positive + controls + EntityEffects + TimeEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

**H2**: Forward-Looking Statements â†’ Innovation
```python
model = PanelOLS.from_formula(
    'patent_count ~ forward_looking_ratio + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

---

## 5. æ±ºç®—èª¬æ˜ä¼šTranscriptåˆ†æ

### 5.1 æ¦‚å¿µ

**Earnings Call Transcript**: å››åŠæœŸæ±ºç®—ç™ºè¡¨å¾Œã®é›»è©±ä¼šè­°ã®æ–‡å­—èµ·ã“ã—

**ç ”ç©¶ã§ã®æ´»ç”¨**:
- çµŒå–¶è€…ã®æˆ¦ç•¥èª¬æ˜ã®è©³ç´°åº¦
- Q&Aã§ã®ãƒˆãƒ¼ãƒ³ï¼ˆè‡ªä¿¡åº¦ã€ä¸ç¢ºå®Ÿæ€§ï¼‰
- ã‚¢ãƒŠãƒªã‚¹ãƒˆã®è³ªå•å†…å®¹ï¼ˆæƒ…å ±éå¯¾ç§°æ€§ã®æŒ‡æ¨™ï¼‰
- Forward-looking statementsã®å…·ä½“æ€§

### 5.2 ãƒ‡ãƒ¼ã‚¿åé›†

#### Seeking Alpha

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

class EarningsCallCollector:
    """æ±ºç®—èª¬æ˜ä¼štranscriptã‚’åé›†"""
    
    def __init__(self, api_key=None):
        self.api_key = api_key
        self.headers = {
            'User-Agent': 'YourUniversity research@email.edu'
        }
    
    def get_transcript_seeking_alpha(self, ticker, year, quarter, max_retries=3):
        """Seeking Alphaã‹ã‚‰Transcriptå–å¾—
        
        Args:
            ticker: Stock ticker (e.g., 'AAPL')
            year: Year (e.g., 2023)
            quarter: Quarter (1-4)
            
        Returns:
            dict: Transcript dataã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
        """
        import time
        
        quarter_map = {1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4'}
        url = f"https://seekingalpha.com/symbol/{ticker}/earnings/earnings-call-transcripts/{year}-{quarter_map[quarter]}"
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                transcript_div = soup.find('div', {'data-test-id': 'content-container'})
                
                if not transcript_div:
                    return None
                
                full_text = transcript_div.get_text(separator='\n', strip=True)
                
                return {
                    'ticker': ticker,
                    'year': year,
                    'quarter': quarter,
                    'full_text': full_text,
                    'char_length': len(full_text)
                }
                
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    return None
            except:
                return None
        
        return None
```

### 5.3 ç™ºè¨€è€…åˆ†é›¢

```python
def parse_transcript_sections(full_text):
    """Transcriptã‚’ç™ºè¨€è€…åˆ¥ã«åˆ†é›¢"""
    import re
    
    sections = {
        'prepared_remarks': '',
        'qa_section': '',
        'management_qa': '',
        'analyst_questions': ''
    }
    
    # Prepared Remarks
    prepared_match = re.search(
        r'Prepared Remarks(.*?)Question-and-Answer',
        full_text,
        re.DOTALL | re.IGNORECASE
    )
    
    if prepared_match:
        sections['prepared_remarks'] = prepared_match.group(1).strip()
    
    # Q&A
    qa_match = re.search(
        r'Question-and-Answer Session(.*?)$',
        full_text,
        re.DOTALL | re.IGNORECASE
    )
    
    if qa_match:
        sections['qa_section'] = qa_match.group(1).strip()
    
    return sections
```

### 5.4 Q&Aãƒˆãƒ¼ãƒ³åˆ†æ

```python
def analyze_qa_tone(qa_text, lm_dict):
    """Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒˆãƒ¼ãƒ³ã‚’åˆ†æ"""
    
    if not qa_text:
        return None
    
    words = qa_text.lower().split()
    total = len(words)
    
    confidence_keywords = ['confident', 'strong', 'optimistic', 'growth']
    uncertainty_keywords = ['uncertain', 'risk', 'challenging', 'difficult']
    
    confidence = sum(1 for w in words if w in confidence_keywords)
    uncertainty = sum(1 for w in words if w in uncertainty_keywords)
    
    return {
        'confidence_ratio': confidence / total,
        'uncertainty_ratio': uncertainty / total,
        'net_tone': (confidence - uncertainty) / total
    }
```

### 5.5 æˆ¦ç•¥ç ”ç©¶ã§ã®æ´»ç”¨

**ä»®èª¬**: Q&Aè‡ªä¿¡åº¦ â†’ æ¬¡æœŸæ¥­ç¸¾

```python
from linearmodels.panel import PanelOLS

model = PanelOLS.from_formula(
    'roa_next ~ qa_confidence + qa_uncertainty + controls + EntityEffects',
    data=df.set_index(['firm_id', 'quarter'])
).fit(cov_type='clustered', cluster_entity=True)
```

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install vaderSentiment beautifulsoup4 scikit-learn nltk requests pandas linearmodels
```

---

## å‚è€ƒæ–‡çŒ®

- Loughran, T., & McDonald, B. (2011). "When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks." *The Journal of Finance*, 66(1), 35-65.
- Mayew, W. J., & Venkatachalam, M. (2012). "The power of voice: Managerial affective states and future firm performance." *The Journal of Finance*, 67(1), 1-43.

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
---

## Troubleshooting

### ğŸ”´ Problem 1: Data Collection Failure

**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01

## 6. å®Ÿè£…ä¾‹ãƒ»ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£

### 6.1 Case Study: Apple MD&Aåˆ†æï¼ˆ2020-2023ï¼‰

**ç ”ç©¶è³ªå•**: Appleã®MD&Aã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã¯ã€å®Ÿéš›ã®R&DæŠ•è³‡ã¨é–¢é€£ã™ã‚‹ã‹ï¼Ÿ

#### Step 1: ãƒ‡ãƒ¼ã‚¿åé›†

```python
# Apple 10-Kåé›†ï¼ˆ2020-2023ï¼‰
collector = SECTextCollector()

apple_mda_data = []

for year in range(2020, 2024):
    # 10-K URLå–å¾—
    filing_url = collector.get_10k_url('0000320193', year)  # Apple CIK
    
    if filing_url:
        # MD&AæŠ½å‡º
        mda_text = collector.extract_mda(filing_url)
        
        if mda_text:
            apple_mda_data.append({
                'company': 'Apple',
                'year': year,
                'mda_text': mda_text,
                'mda_length': len(mda_text)
            })
            
            print(f"{year}: {len(mda_text):,} characters")

# çµæœä¾‹:
# 2020: 45,231 characters
# 2021: 48,102 characters
# 2022: 51,450 characters
# 2023: 49,887 characters
```

#### Step 2: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ

```python
# VADER & Loughran-McDonald
lm_dict = load_lm_dictionary()

for item in apple_mda_data:
    # VADER
    vader_sentiment = analyze_vader_sentiment(item['mda_text'])
    item.update(vader_sentiment)
    
    # LM
    lm_sentiment = analyze_lm_sentiment(item['mda_text'], lm_dict)
    item.update(lm_sentiment)
    
    # Forward-looking
    fl_measures = measure_forward_looking(item['mda_text'])
    item.update(fl_measures)

df_apple = pd.DataFrame(apple_mda_data)

print(df_apple[['year', 'vader_compound', 'lm_polarity', 'forward_looking_ratio']])

# çµæœä¾‹:
#    year  vader_compound  lm_polarity  forward_looking_ratio
# 0  2020          0.6543       0.0234                 0.0156
# 1  2021          0.7102       0.0289                 0.0178
# 2  2022          0.5891       0.0198                 0.0134
# 3  2023          0.6745       0.0267                 0.0165
```

#### Step 3: è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸

```python
# Compustatã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
apple_financial = pd.DataFrame({
    'year': [2020, 2021, 2022, 2023],
    'revenue': [274515, 365817, 394328, 383285],  # Million USD
    'rd_expense': [18752, 21914, 26251, 29915],    # Million USD
    'roa': [0.177, 0.269, 0.283, 0.265]
})

# R&D intensityè¨ˆç®—
apple_financial['rd_intensity'] = apple_financial['rd_expense'] / apple_financial['revenue']

# Merge
df_complete = df_apple.merge(apple_financial, on='year')

print(df_complete[['year', 'vader_compound', 'rd_intensity', 'roa']])

# çµæœä¾‹:
#    year  vader_compound  rd_intensity       roa
# 0  2020          0.6543        0.0683    0.177
# 1  2021          0.7102        0.0599    0.269
# 2  2022          0.5891        0.0666    0.283
# 3  2023          0.6745        0.0781    0.265
```

#### Step 4: ç›¸é–¢åˆ†æ

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Correlation
corr = df_complete[['vader_compound', 'lm_polarity', 'forward_looking_ratio', 
                     'rd_intensity', 'roa']].corr()

print("\nCorrelation Matrix:")
print(corr)

# Visualization
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.scatter(df_complete['vader_compound'], df_complete['rd_intensity'])
plt.xlabel('VADER Sentiment')
plt.ylabel('R&D Intensity')
plt.title('Sentiment vs R&D Intensity')

plt.subplot(1, 2, 2)
plt.plot(df_complete['year'], df_complete['vader_compound'], marker='o', label='Sentiment')
plt.plot(df_complete['year'], df_complete['rd_intensity'] * 10, marker='s', label='R&D Intensity (Ã—10)')
plt.xlabel('Year')
plt.legend()
plt.title('Time Series')
plt.tight_layout()
plt.savefig('apple_sentiment_analysis.png', dpi=300)
plt.show()

# ç™ºè¦‹:
# - VADER compound ã¨ R&D intensity: r = 0.42 (æ­£ã®ç›¸é–¢)
# - Forward-looking ratio ã¨ R&D intensity: r = 0.58 (ä¸­ç¨‹åº¦ã®æ­£ç›¸é–¢)
# â†’ ä»®èª¬æ”¯æŒ: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªMD&A â†’ é«˜ã„R&DæŠ•è³‡
```

---

### 6.2 Case Study: Techä¼æ¥­3ç¤¾æ¯”è¼ƒï¼ˆAAPL, MSFT, GOOGLï¼‰

**ç ”ç©¶è³ªå•**: Techä¼æ¥­ã®æˆ¦ç•¥å¿—å‘ï¼ˆInnovationè¨€åŠï¼‰ã¯ã€å®Ÿéš›ã®ç‰¹è¨±å‡ºé¡˜æ•°ã¨é–¢é€£ã™ã‚‹ã‹ï¼Ÿ

#### Step 1: 3ç¤¾ã®MD&Aåé›†

```python
companies = [
    {'ticker': 'AAPL', 'cik': '0000320193', 'name': 'Apple'},
    {'ticker': 'MSFT', 'cik': '0000789019', 'name': 'Microsoft'},
    {'ticker': 'GOOGL', 'cik': '0001652044', 'name': 'Alphabet'}
]

all_mda_data = []

for company in companies:
    for year in range(2020, 2024):
        filing_url = collector.get_10k_url(company['cik'], year)
        
        if filing_url:
            mda_text = collector.extract_mda(filing_url)
            
            if mda_text:
                all_mda_data.append({
                    'ticker': company['ticker'],
                    'company': company['name'],
                    'year': year,
                    'mda_text': mda_text
                })

print(f"Collected {len(all_mda_data)} MD&As from 3 companies")
# çµæœ: Collected 12 MD&As from 3 companies
```

#### Step 2: Innovationè¨€åŠé »åº¦

```python
def count_innovation_mentions(text):
    """Innovationé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    innovation_keywords = [
        'innovation', 'innovate', 'innovative',
        'r&d', 'research', 'development',
        'patent', 'intellectual property',
        'new product', 'breakthrough', 'cutting-edge'
    ]
    
    text_lower = text.lower()
    words = text_lower.split()
    
    innovation_count = sum(1 for w in words if any(kw in w for kw in innovation_keywords))
    
    return {
        'innovation_count': innovation_count,
        'innovation_ratio': innovation_count / len(words)
    }

# Innovationè¨€åŠã‚’è¨ˆç®—
for item in all_mda_data:
    innovation_metrics = count_innovation_mentions(item['mda_text'])
    item.update(innovation_metrics)

df_tech = pd.DataFrame(all_mda_data)

# ä¼æ¥­åˆ¥å¹³å‡
innovation_by_company = df_tech.groupby('ticker')['innovation_ratio'].mean().sort_values(ascending=False)

print("\nInnovation Mention Ratio (Average 2020-2023):")
print(innovation_by_company)

# çµæœä¾‹:
# GOOGL    0.0089
# MSFT     0.0067
# AAPL     0.0054
```

#### Step 3: ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ

```python
# ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ï¼ˆUSPTO or Google Patentsã‹ã‚‰å–å¾—ï¼‰
patent_data = pd.DataFrame({
    'ticker': ['AAPL', 'AAPL', 'AAPL', 'AAPL',
               'MSFT', 'MSFT', 'MSFT', 'MSFT',
               'GOOGL', 'GOOGL', 'GOOGL', 'GOOGL'],
    'year': [2020, 2021, 2022, 2023] * 3,
    'patent_count': [2840, 2914, 3012, 3145,    # Apple
                     2905, 3100, 3250, 3410,    # Microsoft
                     3150, 3280, 3420, 3550]    # Google
})

# Merge
df_merged = df_tech.merge(patent_data, on=['ticker', 'year'])

print(df_merged[['ticker', 'year', 'innovation_ratio', 'patent_count']].head(9))

# ä¼æ¥­å†…ç›¸é–¢ï¼ˆLaggedï¼‰
for ticker in ['AAPL', 'MSFT', 'GOOGL']:
    df_firm = df_merged[df_merged['ticker'] == ticker].copy()
    
    # Lead patent_count (æ¬¡å¹´åº¦ã®ç‰¹è¨±)
    df_firm['patent_next_year'] = df_firm['patent_count'].shift(-1)
    
    if len(df_firm) > 2:
        corr = df_firm['innovation_ratio'].corr(df_firm['patent_next_year'])
        print(f"{ticker}: Innovation Ratio vs Next Year Patent Count = {corr:.3f}")

# çµæœä¾‹:
# AAPL: Innovation Ratio vs Next Year Patent Count = 0.782
# MSFT: Innovation Ratio vs Next Year Patent Count = 0.891
# GOOGL: Innovation Ratio vs Next Year Patent Count = 0.756
# â†’ å¼·ã„æ­£ã®ç›¸é–¢: Innovationè¨€åŠ â†’ ç¿Œå¹´ã®ç‰¹è¨±å‡ºé¡˜å¢—åŠ 
```

#### Step 4: å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for idx, ticker in enumerate(['AAPL', 'MSFT', 'GOOGL']):
    df_firm = df_merged[df_merged['ticker'] == ticker]
    
    ax = axes[idx]
    ax2 = ax.twinx()
    
    # Innovation ratio (Left axis)
    ax.plot(df_firm['year'], df_firm['innovation_ratio'] * 100, 
            marker='o', color='blue', label='Innovation Ratio (%)')
    ax.set_xlabel('Year')
    ax.set_ylabel('Innovation Ratio (%)', color='blue')
    ax.tick_params(axis='y', labelcolor='blue')
    
    # Patent count (Right axis)
    ax2.plot(df_firm['year'], df_firm['patent_count'], 
             marker='s', color='red', label='Patent Count')
    ax2.set_ylabel('Patent Count', color='red')
    ax2.tick_params(axis='y', labelcolor='red')
    
    ax.set_title(f'{ticker}')
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('tech_innovation_patents.png', dpi=300)
plt.show()
```

---

### 6.3 å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

#### 1. ãƒ‡ãƒ¼ã‚¿åé›†åŠ¹ç‡åŒ–

```python
from tqdm import tqdm
import time

def batch_collect_mda(company_list, year_range, delay=5):
    """è¤‡æ•°ä¼æ¥­ãƒ»å¹´åº¦ã®MD&Aã‚’ä¸€æ‹¬åé›†"""
    
    collector = SECTextCollector()
    results = []
    
    total = len(company_list) * len(year_range)
    
    with tqdm(total=total, desc="Collecting MD&As") as pbar:
        for company in company_list:
            for year in year_range:
                try:
                    filing_url = collector.get_10k_url(company['cik'], year)
                    
                    if filing_url:
                        mda_text = collector.extract_mda(filing_url)
                        
                        if mda_text:
                            results.append({
                                'ticker': company['ticker'],
                                'year': year,
                                'mda_text': mda_text
                            })
                    
                    time.sleep(delay)  # SEC rate limitå¯¾ç­–
                    
                except Exception as e:
                    print(f"Error: {company['ticker']} {year}: {e}")
                
                pbar.update(1)
    
    return pd.DataFrame(results)

# ä½¿ç”¨ä¾‹
companies = [
    {'ticker': 'AAPL', 'cik': '0000320193'},
    {'ticker': 'MSFT', 'cik': '0000789019'},
    {'ticker': 'GOOGL', 'cik': '0001652044'}
]

df_mda = batch_collect_mda(companies, range(2020, 2024), delay=5)
```

#### 2. çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

```python
import pickle
import os

def cache_sentiment_results(df, cache_file='sentiment_cache.pkl'):
    """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    
    if os.path.exists(cache_file):
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿
        with open(cache_file, 'rb') as f:
            cached_df = pickle.load(f)
        print(f"Loaded {len(cached_df)} cached results")
        return cached_df
    
    # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æå®Ÿè¡Œ
    lm_dict = load_lm_dictionary()
    
    for idx, row in df.iterrows():
        vader = analyze_vader_sentiment(row['mda_text'])
        lm = analyze_lm_sentiment(row['mda_text'], lm_dict)
        
        df.at[idx, 'vader_compound'] = vader['vader_compound']
        df.at[idx, 'lm_polarity'] = lm['lm_polarity']
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    with open(cache_file, 'wb') as f:
        pickle.dump(df, f)
    
    print(f"Cached {len(df)} results")
    return df

# ä½¿ç”¨ä¾‹
df_with_sentiment = cache_sentiment_results(df_mda)
```

#### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def robust_text_analysis_pipeline(df, output_file='text_analysis_results.csv'):
    """ã‚¨ãƒ©ãƒ¼ã«å¼·ã„ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    results = []
    errors = []
    
    for idx, row in df.iterrows():
        try:
            # VADER
            vader = analyze_vader_sentiment(row['mda_text'])
            
            if vader is None:
                raise ValueError("VADER analysis failed")
            
            # LM
            lm_dict = load_lm_dictionary()
            lm = analyze_lm_sentiment(row['mda_text'], lm_dict)
            
            if lm is None:
                raise ValueError("LM analysis failed")
            
            # Success
            result = {
                'ticker': row['ticker'],
                'year': row['year'],
                **vader,
                **lm,
                'status': 'success'
            }
            
            results.append(result)
            
        except Exception as e:
            # Error handling
            error_record = {
                'ticker': row['ticker'],
                'year': row['year'],
                'error': str(e),
                'status': 'failed'
            }
            
            errors.append(error_record)
            print(f"Error: {row['ticker']} {row['year']}: {e}")
    
    # çµæœã‚’ä¿å­˜
    df_results = pd.DataFrame(results)
    df_results.to_csv(output_file, index=False)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
    if errors:
        df_errors = pd.DataFrame(errors)
        df_errors.to_csv('errors.csv', index=False)
        print(f"\n{len(errors)} errors occurred. See errors.csv")
    
    print(f"\nSuccessfully processed: {len(results)}/{len(df)} ({len(results)/len(df)*100:.1f}%)")
    
    return df_results

# ä½¿ç”¨ä¾‹
df_results = robust_text_analysis_pipeline(df_mda)
```

---

### 6.4 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**ãƒ†ã‚¹ãƒˆç’°å¢ƒ**: MacBook Pro M1, 16GB RAM

| ã‚¿ã‚¹ã‚¯ | ä»¶æ•° | æ‰€è¦æ™‚é–“ | å‚™è€ƒ |
|--------|------|---------|------|
| MD&AæŠ½å‡º | 100ç¤¾Ã—4å¹´ | 45åˆ† | SEC rate limit: 5ç§’/request |
| VADERåˆ†æ | 400ä»¶ | 2.3åˆ† | å¹³å‡5,000èª/MD&A |
| LMåˆ†æ | 400ä»¶ | 1.8åˆ† | è¾æ›¸ãƒ™ãƒ¼ã‚¹ |
| LDA (n_topics=5) | 400ä»¶ | 8.7åˆ† | scikit-learn |
| å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | 100ç¤¾Ã—4å¹´ | 58åˆ† | ãƒ‡ãƒ¼ã‚¿åé›†å«ã‚€ |

**æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ**:
- SECåé›†: multiprocessingä¸å¯ï¼ˆrate limitï¼‰ã€sequentialã§5ç§’å¾…æ©Ÿ
- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ: multiprocessingå¯èƒ½ â†’ 4ã‚³ã‚¢ã§3å€é«˜é€ŸåŒ–
- LDA: ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚ã‚Šã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¯Daskä½¿ç”¨

---

**Version**: 4.0  
**Last Updated**: 2025-11-01

## 7. FAQï¼ˆã‚ˆãã‚ã‚‹è³ªå•ï¼‰

### Q1: ã©ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆè¾æ›¸ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

**A**: ç”¨é€”ã«ã‚ˆã‚‹ï¼š

**VADER**:
- âœ… ä½¿ç”¨å ´é¢: ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã€ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆ
- âœ… é•·æ‰€: æ„Ÿå˜†ç¬¦ãƒ»å¤§æ–‡å­—ã‚’è€ƒæ…®ã€é€Ÿã„
- âŒ çŸ­æ‰€: è²¡å‹™ç”¨èªã«å¼±ã„

**Loughran-McDonald**:
- âœ… ä½¿ç”¨å ´é¢: è²¡å‹™å ±å‘Šæ›¸ï¼ˆ10-K, MD&Aï¼‰
- âœ… é•·æ‰€: è²¡å‹™æ–‡è„ˆç‰¹åŒ–ã€å­¦è¡“ç ”ç©¶ã§æ¨™æº–
- âŒ çŸ­æ‰€: ä¸€èˆ¬ãƒ†ã‚­ã‚¹ãƒˆã«ã¯ä¸å‘ã

**æ¨å¥¨**: ä¸¡æ–¹ä½¿ç”¨ã—ã¦æ¯”è¼ƒ
```python
# ä¸¡æ–¹è¨ˆç®—ã—ã¦ç›¸é–¢ç¢ºèª
df['vader_compound'] = ...
df['lm_polarity'] = ...

print(df[['vader_compound', 'lm_polarity']].corr())
# ç›¸é–¢ãŒé«˜ã„ï¼ˆr>0.7ï¼‰â†’ é ‘å¥
# ç›¸é–¢ãŒä½ã„ï¼ˆr<0.3ï¼‰â†’ æ–‡è„ˆä¾å­˜æ€§é«˜ã„
```

---

### Q2: MD&AãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ï¼Ÿ

**A**: è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼š

```python
patterns = [
    r'Item\s*7\..*?(?=Item\s*8\.)',       # Standard
    r'Item\s*VII\..*?(?=Item\s*VIII\.)',  # Roman numerals
    r'Management.*?Discussion.*?(?=Item\s*8)', # Alternative
]

for pattern in patterns:
    match = re.search(pattern, html, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(0)
```

**åˆ¥è§£**: SECã®IXBRLãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨
```python
# XBRLå½¢å¼ã®10-Kã‹ã‚‰æ§‹é€ åŒ–æŠ½å‡º
from sec_edgar_downloader import Downloader

dl = Downloader()
dl.get("10-K", "AAPL", after="2023-01-01", before="2023-12-31")
```

---

### Q3: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã®è§£é‡ˆã¯ï¼Ÿ

**A**: æ¥­ç•Œãƒ»æ™‚æœŸã§æ¨™æº–åŒ–ãŒå¿…è¦ï¼š

**çµ¶å¯¾å€¤ã§ã¯ãªãç›¸å¯¾å€¤**:
```python
# Industry-adjusted sentiment
df['sentiment_adj'] = df.groupby('industry')['vader_compound'].transform(
    lambda x: (x - x.mean()) / x.std()
)

# Year-adjusted (ãƒã‚¯ãƒ­çµŒæ¸ˆç’°å¢ƒã‚’é™¤å»)
df['sentiment_year_adj'] = df.groupby('year')['vader_compound'].transform(
    lambda x: (x - x.mean()) / x.std()
)
```

**å…¸å‹çš„ãªåˆ†å¸ƒ**:
- VADER compound: -0.2 ã€œ +0.8ï¼ˆMD&Aã¯ãƒã‚¸ãƒ†ã‚£ãƒ–åå‘ï¼‰
- LM polarity: -0.05 ã€œ +0.05ï¼ˆä¸­ç«‹çš„ï¼‰

---

### Q4: ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§æœ€é©ãªãƒˆãƒ”ãƒƒã‚¯æ•°ã¯ï¼Ÿ

**A**: Perplexityãƒ»Coherenceã§è©•ä¾¡ï¼š

```python
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt

perplexities = []
coherences = []
topic_range = range(2, 21)

for n_topics in topic_range:
    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(doc_term_matrix)
    
    perplexity = lda.perplexity(doc_term_matrix)
    perplexities.append(perplexity)
    
    # Coherenceè¨ˆç®—ï¼ˆgensimãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨æ¨å¥¨ï¼‰
    # coherence = calculate_coherence(lda, texts)
    # coherences.append(coherence)

# Elbow methodã§é¸æŠ
plt.plot(topic_range, perplexities, marker='o')
plt.xlabel('Number of Topics')
plt.ylabel('Perplexity')
plt.title('Optimal Topic Number')
plt.show()

# æ¨å¥¨: 5-10ãƒˆãƒ”ãƒƒã‚¯ï¼ˆMD&Aç ”ç©¶ã§ã¯ï¼‰
```

---

### Q5: ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¯ã©ã“ã¾ã§å¿…è¦ï¼Ÿ

**A**: åˆ†æç›®çš„ã«ã‚ˆã‚‹ï¼š

**ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ**:
- âœ… å¿…è¦: å°æ–‡å­—åŒ–ã€HTMLã‚¿ã‚°é™¤å»
- âŒ ä¸è¦: Stemmingï¼ˆèªå¹¹æŠ½å‡ºï¼‰ã€Stop wordsé™¤å»ï¼ˆ"not"ç­‰ã®å¦å®šèªãŒé‡è¦ï¼‰

```python
def preprocess_for_sentiment(text):
    # Minimal preprocessing
    text = text.lower()
    text = BeautifulSoup(text, 'html.parser').get_text()
    text = re.sub(r'\s+', ' ', text)  # é€£ç¶šç©ºç™½é™¤å»
    return text
```

**ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°**:
- âœ… å¿…è¦: Stemming/Lemmatizationã€Stop wordsé™¤å»ã€n-gramæ¤œå‡º

```python
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

def preprocess_for_topics(text):
    # Aggressive preprocessing
    text = text.lower()
    tokens = word_tokenize(text)
    
    # Stop wordsé™¤å»
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    
    return ' '.join(tokens)
```

---

### Q6: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ”¹å–„ã™ã‚‹ã«ã¯ï¼Ÿ

**A**: æ®µéšçš„æœ€é©åŒ–ï¼š

**Level 1: Vectorization**
```python
# Bad (Loop)
for i, text in enumerate(texts):
    df.at[i, 'sentiment'] = analyze_vader_sentiment(text)['vader_compound']

# Good (Apply)
df['sentiment'] = df['text'].apply(
    lambda x: analyze_vader_sentiment(x)['vader_compound']
)
```

**Level 2: Multiprocessing**
```python
from multiprocessing import Pool

def parallel_sentiment_analysis(texts, n_cores=4):
    with Pool(processes=n_cores) as pool:
        results = pool.map(analyze_vader_sentiment, texts)
    return results

# 4ã‚³ã‚¢ã§3-4å€é«˜é€ŸåŒ–
```

**Level 3: Batch Processing**
```python
# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã¯chunkã§å‡¦ç†
chunk_size = 1000

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    chunk['sentiment'] = chunk['text'].apply(analyze_vader_sentiment)
    chunk.to_csv('output.csv', mode='a', header=False, index=False)
```

---

### Q7: è¤‡æ•°å¹´åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¯”è¼ƒã™ã‚‹ã«ã¯ï¼Ÿ

**A**: ãƒ‘ãƒãƒ«æ§‹é€ ã§å¤‰åŒ–ã‚’è¿½è·¡ï¼š

```python
# Year-over-year change
df = df.sort_values(['ticker', 'year'])
df['sentiment_change'] = df.groupby('ticker')['vader_compound'].diff()

# Cumulative change
df['sentiment_cumulative'] = df.groupby('ticker')['vader_compound'].cumsum()

# Volatility
df['sentiment_volatility'] = df.groupby('ticker')['vader_compound'].transform(
    lambda x: x.rolling(window=3).std()
)

# ä»®èª¬: Sentiment volatilityãŒé«˜ã„ â†’ Strategic changeç¢ºç‡â†‘
```

---

### Q8: æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æã¯å¯èƒ½ï¼Ÿ

**A**: å¯èƒ½ã ãŒã€å°‚ç”¨ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ï¼š

**å½¢æ…‹ç´ è§£æ**:
```python
import MeCab

mecab = MeCab.Tagger("-Owakati")

def tokenize_japanese(text):
    return mecab.parse(text).strip().split()

# ä½¿ç”¨ä¾‹
text = "å½“ç¤¾ã®æ¥­ç¸¾ã¯é †èª¿ã«æ¨ç§»ã—ã¦ã„ã¾ã™"
tokens = tokenize_japanese(text)
# ['å½“ç¤¾', 'ã®', 'æ¥­ç¸¾', 'ã¯', 'é †èª¿', 'ã«', 'æ¨ç§»', 'ã—', 'ã¦', 'ã„', 'ã¾ã™']
```

**æ—¥æœ¬èªã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆè¾æ›¸**:
```python
# æ—¥æœ¬èªè©•ä¾¡æ¥µæ€§è¾æ›¸ï¼ˆæ±åŒ—å¤§å­¦ï¼‰
# http://www.cl.ecei.tohoku.ac.jp/index.php?Open%20Resources%2FJapanese%20Sentiment%20Polarity%20Dictionary

jp_sentiment_dict = {
    'è‰¯ã„': 1.0,
    'æ‚ªã„': -1.0,
    'é †èª¿': 0.8,
    'å›°é›£': -0.6,
    # ...
}

def analyze_japanese_sentiment(text):
    tokens = tokenize_japanese(text)
    scores = [jp_sentiment_dict.get(token, 0) for token in tokens]
    return sum(scores) / len(tokens) if tokens else 0
```

**æ¨å¥¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
- MeCab: å½¢æ…‹ç´ è§£æ
- oseti: æ—¥æœ¬èªã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
- ginza: spaCyãƒ™ãƒ¼ã‚¹ã®æ—¥æœ¬èªNLP

---

### Q9: ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®çµæœãŒçµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„å ´åˆã¯ï¼Ÿ

**A**: ä»¥ä¸‹ã‚’ç¢ºèªï¼š

**1. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**
```python
# å°‘ãªãã¨ã‚‚30-50ç¤¾Ã—3-5å¹´ãŒæ¨å¥¨
print(f"Sample size: {len(df)} (N firms: {df['ticker'].nunique()}, T: {df['year'].nunique()})")

# Power analysis
from statsmodels.stats.power import TTestIndPower

power_analysis = TTestIndPower()
required_n = power_analysis.solve_power(effect_size=0.3, alpha=0.05, power=0.8)
print(f"Required sample size for 80% power: {required_n}")
```

**2. å¤‰æ•°ã®å¤‰å‹•**
```python
# ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã®æ¨™æº–åå·®ãŒå°ã•ã„ â†’ åŠ¹æœæ¤œå‡ºå›°é›£
print(df['vader_compound'].describe())

# å¤–ã‚Œå€¤é™¤å»å¾Œã®åˆ†å¸ƒç¢ºèª
df_clean = df[(df['vader_compound'] > -0.5) & (df['vader_compound'] < 1.0)]
```

**3. Lagæ§‹é€ **
```python
# å³æ™‚åŠ¹æœã§ã¯ãªãã€é…ã‚ŒåŠ¹æœã‚’æ¤œè¨¼
df['sentiment_lag1'] = df.groupby('ticker')['vader_compound'].shift(1)
df['sentiment_lag2'] = df.groupby('ticker')['vader_compound'].shift(2)

model = PanelOLS.from_formula(
    'roa ~ sentiment_lag1 + sentiment_lag2 + controls + EntityEffects + TimeEffects',
    data=df.set_index(['ticker', 'year'])
).fit()
```

**4. éç·šå½¢é–¢ä¿‚**
```python
# Quadratic termè¿½åŠ 
df['sentiment_squared'] = df['vader_compound'] ** 2

# Inverted-U shapeï¼ˆé€†Uå­—ï¼‰ã‚’æ¤œè¨¼
model = PanelOLS.from_formula(
    'roa ~ vader_compound + sentiment_squared + controls + EntityEffects',
    data=df.set_index(['ticker', 'year'])
).fit()

# sentiment_squared ãŒè² ã§æœ‰æ„ â†’ æœ€é©ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæ°´æº–ãŒå­˜åœ¨
```

---

### Q10: ç ”ç©¶å€«ç†ãƒ»è‘—ä½œæ¨©ã®æ³¨æ„ç‚¹ã¯ï¼Ÿ

**A**: ä»¥ä¸‹ã‚’éµå®ˆï¼š

**ãƒ‡ãƒ¼ã‚¿åé›†**:
- âœ… SEC EDGAR: ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã€è‡ªç”±ã«ä½¿ç”¨å¯
- âš ï¸ Seeking Alpha: Terms of Serviceç¢ºèªã€å•†ç”¨åˆ©ç”¨åˆ¶é™ã‚ã‚Š
- âŒ Paywallè¨˜äº‹: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç¦æ­¢

**å¼•ç”¨**:
```python
# åˆ†æçµæœã®ã¿å ±å‘Šã€å…¨æ–‡è»¢è¼‰ã¯ä¸å¯

# Good
"Apple's MD&A shows positive sentiment (VADER=0.67) in 2023."

# Bad
"Apple states: [10-Kã‹ã‚‰æ•°ç™¾èªã‚’ãã®ã¾ã¾å¼•ç”¨]"
```

**ãƒ‡ãƒ¼ã‚¿å…±æœ‰**:
- âœ… ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã€å¤‰æ•°: å…±æœ‰å¯
- âŒ ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ: è‘—ä½œæ¨©å•é¡Œã®å¯èƒ½æ€§ã€è¦ç¢ºèª

**IRBæ‰¿èª**:
- ãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆ10-Kï¼‰: IRBä¸è¦ï¼ˆä¸€èˆ¬çš„ï¼‰
- Earnings callï¼ˆéŸ³å£°ãƒ»transcriptã®éå…¬é–‹éƒ¨åˆ†ï¼‰: IRBç¢ºèªæ¨å¥¨

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
