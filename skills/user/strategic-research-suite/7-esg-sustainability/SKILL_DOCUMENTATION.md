---
name: strategic-research-esg-sustainability
description: ESG and sustainability data sources for strategic management research including MSCI, CDP, Refinitiv, EPA, EU ETS with variable construction for carbon intensity, ESG scores, and disclosure quality.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 2 (Data Source Discovery), Phase 6 (Variable Construction)
  - data-sources: Additional data sources
  - statistical-methods: ESG variables in regression
---

# ESG Sustainability Data v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

**ESG/ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ç ”ç©¶**ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨å¤‰æ•°æ§‹ç¯‰æ‰‹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… ESGæˆ¦ç•¥ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚ã‚’ç ”ç©¶
- âœ… ç’°å¢ƒè¦åˆ¶ã®å½±éŸ¿ã‚’åˆ†æ
- âœ… CSRæ´»å‹•ã®åŠ¹æœã‚’æ¸¬å®š
- âœ… ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£é–‹ç¤ºã®æ±ºå®šè¦å› ã‚’èª¿æŸ»

### å‰ææ¡ä»¶

- ESGæ¦‚å¿µã®åŸºç¤ç†è§£
- ç’°å¢ƒãƒ»ç¤¾ä¼šãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹æŒ‡æ¨™ã®çŸ¥è­˜
- PythonåŸºç¤ï¼ˆAPIã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼‰

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

- **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹** â†’ `2-data-sources`ï¼ˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆï¼‰
- **ç†è«–** â†’ `_shared/THEORY_FRAMEWORKS.md`ï¼ˆStakeholder Theoryç­‰ï¼‰
- **çµ±è¨ˆåˆ†æ** â†’ `3-statistical-methods`

---

## ğŸ“‹ ç›®æ¬¡

1. [ESGãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#1-esgãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
2. [ç’°å¢ƒãƒ‡ãƒ¼ã‚¿](#2-ç’°å¢ƒãƒ‡ãƒ¼ã‚¿)
3. [ESGå¤‰æ•°æ§‹ç¯‰](#3-esgå¤‰æ•°æ§‹ç¯‰)
4. [Quick Reference](#4-quick-reference)

---

## 1. ESGãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 1.1 Premium ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

**MSCI ESG Ratings**
- **è²»ç”¨**: é«˜é¡ï¼ˆæ©Ÿé–¢å¥‘ç´„ï¼‰
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¸Šå ´ä¼æ¥­
- **è©•ä¾¡**: AAA-CCCï¼ˆ7æ®µéšï¼‰
- **å¼·ã¿**: æ¥­ç•Œæ¨™æº–ã€å­¦è¡“ç ”ç©¶ã§åºƒãä½¿ç”¨

**Refinitiv ESG Scores**
- **è²»ç”¨**: é«˜é¡ï¼ˆWRDSçµŒç”±å¯èƒ½ï¼‰
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: 9,000+ä¼æ¥­
- **ã‚¹ã‚³ã‚¢**: 0-100
- **å¼·ã¿**: è©³ç´°ãªé …ç›®åˆ¥ã‚¹ã‚³ã‚¢

**Sustainalytics ESG Risk Ratings**
- **è²»ç”¨**: é«˜é¡
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: 13,000+ä¼æ¥­
- **è©•ä¾¡**: Low-Severeï¼ˆ5æ®µéšï¼‰

**Bloomberg ESG Data**
- **è²»ç”¨**: Bloomberg Terminal
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: åŒ…æ‹¬çš„
- **å¼·ã¿**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°

### 1.2 Free/ä½ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

**CDP (Carbon Disclosure Project)**
```python
import requests
import pandas as pd

# CDP APIï¼ˆè¦ç™»éŒ²ã€ç„¡æ–™ï¼‰
def get_cdp_data(company_id, year, max_retries=3):
    """CDPæ°—å€™å¤‰å‹•ãƒ‡ãƒ¼ã‚¿å–å¾—
    
    Args:
        company_id: CDP company ID
        year: Target year
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        
    Returns:
        dict: CDPãƒ‡ãƒ¼ã‚¿ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯None
    """
    import os
    import time
    
    api_key = os.getenv('CDP_API_KEY', 'YOUR_CDP_API_KEY')
    
    if api_key == 'YOUR_CDP_API_KEY':
        print("Warning: CDP_API_KEY not set. Set with: export CDP_API_KEY='your_key'")
        print("Register at: https://www.cdp.net/")
        return None
    
    url = f"https://api.cdp.net/{year}/companies/{company_id}/climate"
    headers = {"Authorization": f"Bearer {api_key}"}
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                return {
                    'company_id': company_id,
                    'year': year,
                    'carbon_emissions': data.get('scope1_emissions', None),
                    'energy_consumption': data.get('total_energy', None),
                    'cdp_score': data.get('climate_score', None)
                }
            elif response.status_code == 404:
                print(f"Company {company_id} not found in CDP database for {year}")
                return None
            elif response.status_code == 401:
                print("Authentication failed. Check your CDP API key.")
                return None
            elif response.status_code == 429:
                print(f"Rate limit hit. Waiting 60 seconds...")
                time.sleep(60)
            else:
                print(f"HTTP Error {response.status_code}: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"Attempt {attempt + 1}/{max_retries}: Request timed out")
            if attempt < max_retries - 1:
                time.sleep(5)
            else:
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"Network error: {type(e).__name__}: {e}")
            return None
            
        except ValueError as e:
            print(f"JSON parsing error: {e}")
            return None
    
    return None

# ä½¿ç”¨ä¾‹
cdp_data = get_cdp_data('COMP001', 2023)
print(cdp_data)
```

**GRI (Global Reporting Initiative)**
- **è²»ç”¨**: ç„¡æ–™
- **å†…å®¹**: ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£å ±å‘Šæ›¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **URL**: https://database.globalreporting.org/

**EPA TRI (Toxic Release Inventory, ç±³å›½)**
```python
# EPA TRI API
def get_epa_tri_data(facility_id, year, max_retries=3):
    """EPAæœ‰å®³ç‰©è³ªæ’å‡ºãƒ‡ãƒ¼ã‚¿å–å¾—
    
    Args:
        facility_id: EPA facility ID
        year: Reporting year
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        
    Returns:
        DataFrame: EPA TRIãƒ‡ãƒ¼ã‚¿ã€ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºDataFrame
    """
    import time
    
    url = "https://data.epa.gov/efservice/tri_facility"
    
    params = {
        'FACILITY_ID': facility_id,
        'REPORTING_YEAR': year,
        'output': 'JSON'
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if not data:
                print(f"No data found for facility {facility_id} in year {year}")
                return pd.DataFrame()
            
            df = pd.DataFrame(data)
            print(f"Retrieved {len(df)} records for facility {facility_id}")
            return df
            
        except requests.exceptions.Timeout:
            print(f"Attempt {attempt + 1}/{max_retries}: Request timed out")
            if attempt < max_retries - 1:
                time.sleep(5)
            else:
                print("Max retries reached. Returning empty DataFrame.")
                return pd.DataFrame()
                
        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: {e}")
            return pd.DataFrame()
            
        except ValueError as e:
            print(f"JSON parsing error: {e}")
            return pd.DataFrame()
            
        except Exception as e:
            print(f"Unexpected error: {type(e).__name__}: {e}")
            return pd.DataFrame()
    
    return pd.DataFrame()

# ä½¿ç”¨ä¾‹
tri_data = get_epa_tri_data('12345XMPLF', 2022)
print(tri_data[['CHEMICAL_NAME', 'TOTAL_RELEASES']].head())
```

**EU ETS (Emissions Trading System, æ¬§å·)**
- **è²»ç”¨**: ç„¡æ–™
- **å†…å®¹**: EUæ’å‡ºæ¨©å–å¼•ãƒ‡ãƒ¼ã‚¿
- **URL**: https://ec.europa.eu/clima/ets/

---

## 2. ç’°å¢ƒãƒ‡ãƒ¼ã‚¿

### 2.1 Carbon Emissionsï¼ˆç‚­ç´ æ’å‡ºé‡ï¼‰

**Scopeå®šç¾©**:
- **Scope 1**: ç›´æ¥æ’å‡ºï¼ˆå·¥å ´ã€è»Šä¸¡ï¼‰
- **Scope 2**: é–“æ¥æ’å‡ºï¼ˆè³¼å…¥é›»åŠ›ï¼‰
- **Scope 3**: ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æ’å‡º

**ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰**:
```python
def calculate_carbon_intensity(df):
    """ç‚­ç´ é›†ç´„åº¦ã‚’è¨ˆç®—"""
    
    # Carbon Intensity = CO2æ’å‡ºé‡ / å£²ä¸Šé«˜
    df['carbon_intensity'] = df['scope1_emissions'] / df['revenue']
    
    # Log transformationï¼ˆåˆ†å¸ƒã®æ­£è¦åŒ–ï¼‰
    df['carbon_intensity_log'] = np.log(df['carbon_intensity'] + 1)
    
    return df

# ä½¿ç”¨ä¾‹
df = calculate_carbon_intensity(df)
```

### 2.2 Environmental Performance Indicators

```python
def construct_environmental_vars(df):
    """ç’°å¢ƒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¤‰æ•°æ§‹ç¯‰"""
    
    # Energy Efficiency
    df['energy_efficiency'] = df['revenue'] / df['total_energy_consumption']
    
    # Waste Reduction Rate
    df['waste_reduction'] = (df['waste_t_minus1'] - df['waste_t']) / df['waste_t_minus1']
    
    # Water Consumption per Revenue
    df['water_intensity'] = df['water_consumption'] / df['revenue']
    
    # Renewable Energy Ratio
    df['renewable_ratio'] = df['renewable_energy'] / df['total_energy']
    
    return df

df = construct_environmental_vars(df)
```

---

## 3. ESGå¤‰æ•°æ§‹ç¯‰

### 3.1 ESG Composite Score

```python
def create_esg_score(df):
    """ESGç·åˆã‚¹ã‚³ã‚¢ä½œæˆï¼ˆè¤‡æ•°ã‚½ãƒ¼ã‚¹ã®çµ±åˆï¼‰"""
    
    # æ¨™æº–åŒ–ï¼ˆ0-100ã‚¹ã‚±ãƒ¼ãƒ«ã«ï¼‰
    from sklearn.preprocessing import MinMaxScaler
    
    scaler = MinMaxScaler(feature_range=(0, 100))
    
    # å„æŸ±ã‚’æ¨™æº–åŒ–
    df['e_score_std'] = scaler.fit_transform(df[['environmental_score']])
    df['s_score_std'] = scaler.fit_transform(df[['social_score']])
    df['g_score_std'] = scaler.fit_transform(df[['governance_score']])
    
    # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆç­‰é‡ã¿ï¼‰
    df['esg_score'] = (df['e_score_std'] + df['s_score_std'] + df['g_score_std']) / 3
    
    # ä»£æ›¿: ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
    from sklearn.decomposition import PCA
    
    pca = PCA(n_components=1)
    df['esg_score_pca'] = pca.fit_transform(df[['e_score_std', 's_score_std', 'g_score_std']])
    
    return df

df = create_esg_score(df)
```

### 3.2 ESG Disclosure Quality

```python
def measure_disclosure_quality(df):
    """ESGé–‹ç¤ºå“è³ªã‚’æ¸¬å®š"""
    
    # é–‹ç¤ºé …ç›®æ•°ï¼ˆä¾‹: GRIåŸºæº–ï¼‰
    gri_indicators = [
        'ghg_emissions', 'energy_consumption', 'water_usage',
        'waste_generated', 'employee_diversity', 'board_diversity'
    ]
    
    # é–‹ç¤ºç‡
    df['disclosure_rate'] = df[gri_indicators].notna().sum(axis=1) / len(gri_indicators)
    
    # é–‹ç¤ºã®è©³ç´°åº¦ï¼ˆæ–‡å­—æ•°ã€ç°¡æ˜“ç‰ˆï¼‰
    # å®Ÿéš›ã¯ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    df['disclosure_detail'] = df['esg_report_length'] / 1000  # KBå˜ä½
    
    # ç¬¬ä¸‰è€…ä¿è¨¼ã®æœ‰ç„¡
    df['assurance'] = df['third_party_assurance'].astype(int)
    
    return df

df = measure_disclosure_quality(df)
```

### 3.3 ESG Controversy Score

```python
def calculate_controversy_score(df, controversies_df):
    """ESGè«–äº‰ã‚¹ã‚³ã‚¢ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ã‚¤ãƒ™ãƒ³ãƒˆï¼‰
    
    Args:
        df: ä¼æ¥­ãƒ‡ãƒ¼ã‚¿
        controversies_df: è«–äº‰ãƒ‡ãƒ¼ã‚¿ï¼ˆfirm_id, year, controversy_type, severityï¼‰
    
    Returns:
        df: controversy_scoreã‚’è¿½åŠ 
    """
    
    # å„ä¼æ¥­ãƒ»å¹´ã®è«–äº‰ä»¶æ•°
    controversy_counts = controversies_df.groupby(['firm_id', 'year']).size().reset_index(name='controversy_count')
    
    # é‡å¤§åº¦åŠ é‡
    severity_weights = {'Low': 1, 'Medium': 3, 'High': 5, 'Severe': 10}
    controversies_df['severity_score'] = controversies_df['severity'].map(severity_weights)
    
    controversy_severity = controversies_df.groupby(['firm_id', 'year'])['severity_score'].sum().reset_index()
    
    # ãƒãƒ¼ã‚¸
    df = df.merge(controversy_counts, on=['firm_id', 'year'], how='left')
    df = df.merge(controversy_severity, on=['firm_id', 'year'], how='left')
    
    df['controversy_count'] = df['controversy_count'].fillna(0)
    df['severity_score'] = df['severity_score'].fillna(0)
    
    return df

df = calculate_controversy_score(df, controversies_df)
```

---

## 4. Quick Reference

### ESGãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¯”è¼ƒ

| ã‚½ãƒ¼ã‚¹ | è²»ç”¨ | ã‚«ãƒãƒ¬ãƒƒã‚¸ | å“è³ª | å­¦è¡“åˆ©ç”¨ |
|--------|------|-----------|------|---------|
| **MSCI** | é«˜é¡ | ã‚°ãƒ­ãƒ¼ãƒãƒ« | â­â­â­â­â­ | æœ€å¤š |
| **Refinitiv** | é«˜é¡ | ã‚°ãƒ­ãƒ¼ãƒãƒ« | â­â­â­â­â­ | å¤š |
| **CDP** | ç„¡æ–™ | ã‚°ãƒ­ãƒ¼ãƒãƒ« | â­â­â­â­ | ä¸­ |
| **EPA TRI** | ç„¡æ–™ | ç±³å›½ | â­â­â­â­ | ä¸­ |
| **EU ETS** | ç„¡æ–™ | æ¬§å· | â­â­â­â­ | ä¸­ |

### ä¸»è¦ESGå¤‰æ•°

| å¤‰æ•° | å®šç¾© | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ |
|------|------|-------------|
| **Carbon Intensity** | CO2æ’å‡ºé‡/å£²ä¸Šé«˜ | CDP, EPA |
| **ESG Score** | ç·åˆã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰ | MSCI, Refinitiv |
| **Disclosure Quality** | é–‹ç¤ºé …ç›®ç‡ | GRI, CDP |
| **Controversy Score** | ãƒã‚¬ãƒ†ã‚£ãƒ–äº‹è±¡ã®é‡å¤§åº¦ | RepRisk, Refinitiv |
| **Board Diversity** | å¥³æ€§å–ç· å½¹æ¯”ç‡ | æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ |

### æˆ¦ç•¥ç ”ç©¶ã§ã®ä»®èª¬ä¾‹

**H1**: ESG â†’ Financial Performance
```python
model = PanelOLS.from_formula(
    'roa ~ esg_score + controls + EntityEffects + TimeEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

**H2**: Environmental Regulation â†’ Innovation
```python
# EU ETSå°å…¥ï¼ˆ2005å¹´ï¼‰ã®åŠ¹æœ
model = PanelOLS.from_formula(
    'patent_count ~ eu_ets_regulated + post_2005 + eu_ets_regulated:post_2005 + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

**H3**: Disclosure Quality â†’ Cost of Capital
```python
model = PanelOLS.from_formula(
    'wacc ~ disclosure_quality + esg_score + controls + EntityEffects + TimeEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

### ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**Stakeholder Theory**:
- ESGæ´»å‹• â†’ ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼é–¢ä¿‚æ”¹å–„ â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š

**Legitimacy Theory**:
- ESGé–‹ç¤º â†’ ç¤¾ä¼šçš„æ­£å½“æ€§ç²å¾— â†’ è³‡æºã‚¢ã‚¯ã‚»ã‚¹

**Resource-Based View**:
- ESGèƒ½åŠ› â†’ ç‹¬è‡ªã®çµ„ç¹”è³‡æº â†’ ç«¶äº‰å„ªä½

â†’ è©³ç´°: `_shared/THEORY_FRAMEWORKS.md`

---

## ãƒ‡ãƒ¼ã‚¿åé›†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. è¤‡æ•°ã‚½ãƒ¼ã‚¹ã®çµ±åˆ
```python
# MSCI + CDP + è²¡å‹™ãƒ‡ãƒ¼ã‚¿
df_integrated = df_financial.merge(
    df_msci, on=['firm_id', 'year'], how='left'
).merge(
    df_cdp, on=['firm_id', 'year'], how='left'
)

# æ¬ æå€¤ã®è£œå®Œï¼ˆéšå±¤çš„ï¼‰
df_integrated['esg_score'] = df_integrated['msci_esg'].fillna(df_integrated['cdp_score'])
```

### 2. æ™‚å·®åŠ¹æœã®è€ƒæ…®
```python
# ESGæŠ•è³‡ã®åŠ¹æœã¯2-3å¹´å¾Œã«ç¾ã‚Œã‚‹
df['esg_score_lag2'] = df.groupby('firm_id')['esg_score'].shift(2)

model = PanelOLS.from_formula(
    'roa ~ esg_score_lag2 + controls + EntityEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', cluster_entity=True)
```

### 3. å†…ç”Ÿæ€§ã¸ã®å¯¾å‡¦
```python
# ESG Score â†’ Performance ã®å†…ç”Ÿæ€§
# é€†å› æœ: Performance â†’ ESGæŠ•è³‡å¢—åŠ 

# å¯¾ç­–: Instrumental Variable
# IVå€™è£œ: ç”£æ¥­å¹³å‡ESGã‚¹ã‚³ã‚¢ã€åœ°åŸŸç’°å¢ƒè¦åˆ¶å³æ ¼åº¦

from linearmodels.iv import IV2SLS

iv_model = IV2SLS.from_formula(
    'roa ~ [esg_score ~ industry_avg_esg] + controls + EntityEffects + TimeEffects',
    data=df.set_index(['firm_id', 'year'])
).fit(cov_type='clustered', clusters=df['firm_id'])
```

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install pandas numpy requests scikit-learn
```

---

## å‚è€ƒæ–‡çŒ®

**ESGç ”ç©¶**:
- Friede, G., Busch, T., & Bassen, A. (2015). "ESG and financial performance: aggregated evidence from more than 2000 empirical studies." *Journal of Sustainable Finance & Investment*, 5(4), 210-233.

**é–‹ç¤ºç ”ç©¶**:
- Dhaliwal, D. S., et al. (2011). "Voluntary nonfinancial disclosure and the cost of equity capital." *The Accounting Review*, 86(1), 59-100.

---

**Version**: 4.0  
**Last Updated**: 2025-11-01  
**Next**: `8-automation` skill
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
