---
name: strategic-research-core-workflow
description: Comprehensive Phase 1-8 research workflow for strategic management and organizational studies, covering research design, data collection, panel dataset construction, variable operationalization, statistical analysis, and documentation for top-tier journal publication.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - data-sources: Phase 2 data source discovery and collection
  - statistical-methods: Phase 7 advanced statistical analysis
  - text-analysis: Qualitative data quantification
  - network-analysis: Inter-organizational relationship analysis
  - causal-ml: Causal inference and heterogeneous treatment effects
  - esg-sustainability: ESG/CSR strategy research
  - automation: End-to-end pipeline automation
---

# Strategic Research Core Workflow v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥çµŒå–¶ãƒ»çµ„ç¹”ç ”ç©¶ã®ãŸã‚ã®**Phase 1-8ç ”ç©¶ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**ã‚’æä¾›ã—ã¾ã™ã€‚Research Questionè¨­å®šã‹ã‚‰ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æŠ•ç¨¿ã¾ã§ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… ã™ã¹ã¦ã®å®šé‡ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®èµ·ç‚¹
- âœ… ç ”ç©¶è¨­è¨ˆã®å…¨ä½“åƒã‚’ç†è§£ã—ãŸã„
- âœ… åˆã‚ã¦æˆ¦ç•¥ç ”ç©¶ãƒ»çµ„ç¹”ç ”ç©¶ã«å–ã‚Šçµ„ã‚€
- âœ… Phaseåˆ¥ã®ä½œæ¥­å†…å®¹ã¨é †åºã‚’ç¢ºèªã—ãŸã„
- âœ… ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆSMJ, AMJ, OS, JOMï¼‰åŸºæº–ã‚’æº€ãŸã—ãŸã„

### å‰ææ¡ä»¶

**å¿…é ˆçŸ¥è­˜**:
- PythonåŸºç¤ï¼ˆpandas, numpyï¼‰
- ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æ¦‚å¿µ
- çµ±è¨ˆåˆ†æã®åŸºç¤ï¼ˆå›å¸°åˆ†æã€på€¤ã€ä¿¡é ¼åŒºé–“ï¼‰

**æ¨å¥¨çŸ¥è­˜**:
- æˆ¦ç•¥çµŒå–¶ãƒ»çµ„ç¹”ç†è«–ã®åŸºç¤
- å­¦è¡“è«–æ–‡ã®èª­ã¿æ–¹ãƒ»æ›¸ãæ–¹
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œï¼ˆSQLåŸºç¤ï¼‰

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

**Phaseåˆ¥æ¨å¥¨ã‚¹ã‚­ãƒ«**:
- **Phase 1**: æœ¬ã‚¹ã‚­ãƒ« + `_shared/THEORY_FRAMEWORKS.md`
- **Phase 2**: `2-data-sources`ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¢ç´¢ï¼‰
- **Phase 3**: `2-data-sources` + `8-automation`ï¼ˆåé›†æˆ¦ç•¥ï¼‰
- **Phase 4**: æœ¬ã‚¹ã‚­ãƒ« + `8-automation`ï¼ˆPanelæ§‹ç¯‰ï¼‰
- **Phase 5**: `_shared/quality-checklist.md`ï¼ˆQAï¼‰
- **Phase 6**: `_shared/common-definitions.md` + å°‚é–€ã‚¹ã‚­ãƒ«
  - ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•° â†’ `4-text-analysis`
  - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰æ•° â†’ `5-network-analysis`
  - ESGå¤‰æ•° â†’ `7-esg-sustainability`
- **Phase 7**: `3-statistical-methods`ï¼ˆçµ±è¨ˆåˆ†æï¼‰
- **Phase 8**: `8-automation`ï¼ˆå†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰

---

## ğŸ“‹ ç›®æ¬¡

1. [Phase 1: Research Design](#phase-1-research-design)
2. [Phase 2: Data Source Discovery](#phase-2-data-source-discovery)
3. [Phase 3: Data Collection Strategy](#phase-3-data-collection-strategy)
4. [Phase 4: Panel Dataset Construction](#phase-4-panel-dataset-construction)
5. [Phase 5: Quality Assurance](#phase-5-quality-assurance)
6. [Phase 6: Variable Construction](#phase-6-variable-construction)
7. [Phase 7: Statistical Analysis](#phase-7-statistical-analysis)
8. [Phase 8: Documentation & Reproducibility](#phase-8-documentation--reproducibility)
9. [Quick Start Guide](#quick-start-guide)
10. [Common Pitfalls](#common-pitfalls)
11. [FAQ](#faq)

---

## Phase 1: Research Design

### 1.1 Research Question (RQ) è¨­å®š

**è‰¯ã„RQã®5æ¡ä»¶**:

1. **Theoretical Gap**: æ—¢å­˜ç ”ç©¶ã§æœªè§£æ±ºã®å•é¡Œãƒ»çŸ›ç›¾ã‚’æ˜ç¢ºã«æŒ‡æ‘˜
2. **Practical Relevance**: çµŒå–¶å®Ÿå‹™ã¸ã®ç¤ºå”†ãŒæ˜ç¢º
3. **Empirically Testable**: ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼å¯èƒ½ï¼ˆæŠ½è±¡çš„æ¦‚å¿µã‚’å›é¿ï¼‰
4. **Specific & Focused**: å¤‰æ•°é–“ã®é–¢ä¿‚ãŒæ˜ç¢ºï¼ˆã€Œå½±éŸ¿ã™ã‚‹ã€ã ã‘ã§ã¯ä¸ååˆ†ï¼‰
5. **Boundary Conditions**: ã©ã®ã‚ˆã†ãªçŠ¶æ³ã§æˆç«‹ã™ã‚‹ã‹ãŒæ˜ç¢º

**æ‚ªã„RQä¾‹**:
```
âŒ ã€Œã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¯ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿã€

å•é¡Œç‚¹:
- å…ˆè¡Œç ”ç©¶ã§æ—¢ã«ç¢ºç«‹æ¸ˆã¿ï¼ˆTheoretical Gapãªã—ï¼‰
- ã€Œã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã€ã€Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ãŒæ›–æ˜§
- æ¡ä»¶ãƒ»ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒä¸æ˜ç¢º
```

**è‰¯ã„RQä¾‹**:
```
âœ… ã€Œç’°å¢ƒä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ä¸‹ã§ã€æ¢ç´¢çš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆExplorationï¼‰ã¯
çŸ­æœŸçš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æãªã†ãŒã€ã©ã®ã‚ˆã†ãªçµ„ç¹”çš„æ¡ä»¶ï¼ˆOrganizational Slack,
Absorptive Capacityï¼‰ã®ä¸‹ã§é•·æœŸçš„ã«å›å¾©ã™ã‚‹ã‹ï¼Ÿã€

å„ªã‚Œã¦ã„ã‚‹ç‚¹:
- Theoretical Gap: March (1991)ã®Exploration-Exploitationç†è«–ã¨
  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ™‚é–“çš„ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®çµ±åˆ
- æ¡ä»¶ä»˜ãé–¢ä¿‚: ç’°å¢ƒä¸ç¢ºå®Ÿæ€§ã¨ã„ã†å¢ƒç•Œæ¡ä»¶
- èª¿æ•´å¤‰æ•°: Slack, Absorptive Capacity
- æ™‚é–“çš„è¦–ç‚¹: çŸ­æœŸvsé•·æœŸ
```

### 1.2 ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é¸æŠ

â†’ **è©³ç´°**: `_shared/THEORY_FRAMEWORKS.md`

**ä¸»è¦ç†è«–8ã¤**:

| ç†è«– | é©ç”¨å ´é¢ | ä¸»è¦æ–‡çŒ® |
|------|---------|---------|
| **RBV** | ä¼æ¥­å†…éƒ¨è³‡æºã¨ç«¶äº‰å„ªä½ | Barney (1991) |
| **Dynamic Capabilities** | ç’°å¢ƒå¤‰åŒ–ã¸ã®é©å¿œ | Teece et al. (1997) |
| **Institutional Theory** | çµ„ç¹”ã¨ç’°å¢ƒã®åŒå‹åŒ– | DiMaggio & Powell (1983) |
| **TCE** | Make-or-buyæ±ºå®š | Williamson (1985) |
| **Agency Theory** | çµŒå–¶è€…-æ ªä¸»åˆ©å®³å¯¾ç«‹ | Jensen & Meckling (1976) |
| **Stakeholder Theory** | CSR, ESG | Freeman (1984) |
| **IO** | ç”£æ¥­æ§‹é€ ã¨åç›Šæ€§ | Porter (1980) |
| **KBV** | çŸ¥è­˜å‰µé€ ãƒ»ç§»è»¢ | Grant (1996) |

**é¸æŠåŸºæº–**:
1. **RQé©åˆæ€§**: RQãŒèª¬æ˜ã—ã‚ˆã†ã¨ã™ã‚‹ç¾è±¡ã«æœ€ã‚‚é©ã—ãŸç†è«–
2. **å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ **: ãªãœãã®é–¢ä¿‚ãŒç”Ÿã˜ã‚‹ã‹ã‚’æ˜ç¢ºã«èª¬æ˜ã§ãã‚‹
3. **ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ä½¿ç”¨é »åº¦**: SMJ, AMJ, OSã§ã®ä½¿ç”¨å®Ÿç¸¾

**è¤‡æ•°ç†è«–ã®çµ±åˆ**:
```python
# ä¾‹: RBV + Dynamic Capabilities
# RQãŒã€Œè³‡æºã®è“„ç©ï¼ˆRBVï¼‰ã¨ç’°å¢ƒå¤‰åŒ–ã¸ã®é©å¿œï¼ˆDCï¼‰ã®ç›¸äº’ä½œç”¨ã€ã®å ´åˆ

ç†è«–çš„è«–ç†:
1. RBV: R&DæŠ•è³‡ã¯æ¨¡å€£å›°é›£ãªæŠ€è¡“è³‡æºã‚’è“„ç©ï¼ˆé™çš„è¦–ç‚¹ï¼‰
2. Dynamic Capabilities: ç’°å¢ƒå¤‰åŒ–æ™‚ã«R&Dè³‡æºã‚’å†é…ç½®ã™ã‚‹èƒ½åŠ›ï¼ˆå‹•çš„è¦–ç‚¹ï¼‰
3. çµ±åˆ: R&Dè³‡æºÃ—å†é…ç½®èƒ½åŠ›ã®äº¤äº’ä½œç”¨ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ±ºå®š
```

### 1.3 ä»®èª¬å°å‡º

**ä»®èª¬æ§‹ç¯‰ã®3ã‚¹ãƒ†ãƒƒãƒ—**:

#### Step 1: ç†è«–çš„æ ¹æ‹ ã®æ˜ç¤º
```
ç†è«–: RBV (Resource-Based View)
å‘½é¡Œ: æ¨¡å€£å›°é›£ãªè³‡æºã¯æŒç¶šçš„ç«¶äº‰å„ªä½ã®æºæ³‰
é©ç”¨: R&DæŠ•è³‡ã¯æŠ€è¡“çŸ¥è­˜ã¨ã„ã†æ¨¡å€£å›°é›£è³‡æºã‚’å‰µå‡º
```

#### Step 2: å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç‰¹å®š
```
R&DæŠ•è³‡ â†’ æŠ€è¡“çŸ¥è­˜è“„ç© â†’ ç‰¹è¨±å–å¾— â†’ ç«¶äº‰å„ªä½ â†’ è¶…éåˆ©æ½¤ â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š

å„ã‚¹ãƒ†ãƒƒãƒ—ã®ç†è«–çš„èª¬æ˜:
- R&Dâ†’çŸ¥è­˜: Learning-by-doing (Cohen & Levinthal, 1990)
- çŸ¥è­˜â†’ç‰¹è¨±: Appropriability mechanisms (Teece, 1986)
- ç‰¹è¨±â†’ç«¶äº‰å„ªä½: Imitation barriers (Rumelt, 1984)
```

#### Step 3: å¢ƒç•Œæ¡ä»¶ãƒ»èª¿æ•´åŠ¹æœã®ç‰¹å®š
```
H1 (Main Effect): R&DæŠ•è³‡å¼·åº¦ã¯ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ­£ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹

H2 (Moderator: Industry Maturity):
ç”£æ¥­æˆç†Ÿåº¦ãŒé«˜ã„å ´åˆã€H1ã®æ­£ã®é–¢ä¿‚ã¯å¼±ã¾ã‚‹

ç†è«–çš„æ ¹æ‹ :
- æˆç†Ÿç”£æ¥­ã§ã¯æŠ€è¡“é©æ–°ã®ä½™åœ°ãŒé™å®šçš„ï¼ˆKlepper, 1996ï¼‰
- Diminishing returnsãŒæ—©æœŸã«ç™ºç”Ÿ
```

**å®Ÿè£…ä¾‹: ä»®èª¬ãƒªã‚¹ãƒˆ**
```markdown
### ä»®èª¬

**H1 (Main Effect)**: R&DæŠ•è³‡å¼·åº¦ã¯ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆROAï¼‰ã«æ­£ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹

**H2 (Moderator)**: ç”£æ¥­æˆç†Ÿåº¦ã¯H1ã®é–¢ä¿‚ã‚’è² ã«èª¿æ•´ã™ã‚‹
- H2a: ç”£æ¥­æˆç†Ÿåº¦ãŒä½ã„å ´åˆã€H1ã®æ­£ã®é–¢ä¿‚ã¯å¼·ã„
- H2b: ç”£æ¥­æˆç†Ÿåº¦ãŒé«˜ã„å ´åˆã€H1ã®æ­£ã®é–¢ä¿‚ã¯å¼±ã„

**H3 (Mediator)**: R&DæŠ•è³‡å¼·åº¦ã¯ç‰¹è¨±å–å¾—æ•°ã‚’åª’ä»‹ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã™ã‚‹
- H3a: R&DæŠ•è³‡å¼·åº¦ â†’ ç‰¹è¨±å–å¾—æ•°ï¼ˆæ­£ï¼‰
- H3b: ç‰¹è¨±å–å¾—æ•° â†’ ROAï¼ˆæ­£ï¼‰
- H3c: H3aÃ—H3b: åª’ä»‹åŠ¹æœãŒæœ‰æ„
```

### 1.4 å¤‰æ•°å®šç¾©

â†’ **è©³ç´°**: `_shared/common-definitions.md`

**å¿…é ˆå¤‰æ•°ã‚¿ã‚¤ãƒ—**:

| ã‚¿ã‚¤ãƒ— | ä¾‹ | æ¸¬å®šæ–¹æ³• |
|--------|-----|---------|
| **å¾“å±å¤‰æ•° (DV)** | ROA, Tobin's Q, ROE | è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®— |
| **ç‹¬ç«‹å¤‰æ•° (IV)** | R&D Intensity, Patent Count | è²¡å‹™ãƒ»ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ |
| **èª¿æ•´å¤‰æ•° (Moderator)** | Industry Maturity, Slack | ç”£æ¥­ãƒ»çµ„ç¹”å¤‰æ•° |
| **åª’ä»‹å¤‰æ•° (Mediator)** | Patent Count, Absorptive Capacity | ä¸­é–“å¤‰æ•° |
| **çµ±åˆ¶å¤‰æ•° (CV)** | Firm Size, Leverage, Industry, Year | äº¤çµ¡å› å­ |

**Pythonã§ã®å¤‰æ•°æ§‹ç¯‰**:
```python
import pandas as pd
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('compustat_sample.csv')

# 1. å¾“å±å¤‰æ•°: ROA
df['roa'] = df['net_income'] / df['total_assets']

# 2. ç‹¬ç«‹å¤‰æ•°: R&D Intensity
df['rd_intensity'] = df['rd_expense'] / df['revenue']

# æ¬ æå€¤å‡¦ç†: R&Dæœªå ±å‘Šä¼æ¥­ã¯0ã¨ä»®å®šï¼ˆæ¥­ç•Œæ¨™æº–ï¼‰
df['rd_intensity'] = df['rd_intensity'].fillna(0)

# 3. èª¿æ•´å¤‰æ•°: Industry Maturity (ç”£æ¥­å¹³å‡å¹´é½¢)
df['industry_maturity'] = df.groupby('sic_code')['firm_age'].transform('mean')

# 4. çµ±åˆ¶å¤‰æ•°
# Firm Size (log)
df['firm_size'] = np.log(df['total_assets'])

# Leverage
df['leverage'] = df['total_debt'] / df['total_assets']

# Firm Age (log)
df['firm_age'] = df['year'] - df['founding_year']
df['firm_age_log'] = np.log(df['firm_age'] + 1)  # +1 to avoid log(0)

# 5. ãƒ©ã‚°å¤‰æ•°ï¼ˆ1å¹´ãƒ©ã‚°ï¼‰
df = df.sort_values(['firm_id', 'year'])
df['rd_intensity_lag1'] = df.groupby('firm_id')['rd_intensity'].shift(1)

print(df[['firm_id', 'year', 'roa', 'rd_intensity', 'firm_size']].head(10))
```

### 1.5 ã‚µãƒ³ãƒ—ãƒ«è¨­è¨ˆ

â†’ **è©³ç´°**: `_shared/common-definitions.md#minimum-sample-sizes`

**æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**:

| ç ”ç©¶ã‚¿ã‚¤ãƒ— | æœ€å° firm-years | æ¨å¥¨ firm-years | å…¸å‹çš„æœŸé–“ |
|-----------|----------------|----------------|-----------|
| **åŸºæœ¬ãƒ‘ãƒãƒ«å›å¸°** | 500 | 1,000+ | 5-10å¹´ |
| **ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ« (SMJ, AMJ, OS)** | 1,500 | 2,000+ | 10-15å¹´ |
| **IV/PSM (å†…ç”Ÿæ€§å¯¾ç­–)** | 1,000 | 2,000+ | 10å¹´+ |
| **Event Study (DiD)** | 100 events | 200+ events | äº‹è±¡ä¾å­˜ |

**ã‚µãƒ³ãƒ—ãƒ«é¸æŠåŸºæº–**:

```python
# å…¸å‹çš„ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
sample = df[
    # 1. æœŸé–“
    (df['year'] >= 2010) & (df['year'] <= 2020) &
    
    # 2. ç”£æ¥­: è£½é€ æ¥­ï¼ˆSIC 2000-3999ï¼‰
    (df['sic_code'] >= 2000) & (df['sic_code'] <= 3999) &
    
    # 3. ãƒ‡ãƒ¼ã‚¿å“è³ª
    (df['total_assets'] > 0) &
    (df['revenue'] > 0) &
    (df['net_income'].notna()) &
    
    # 4. å¤–ã‚Œå€¤é™¤å¤–: Winsorization
    (df['roa'] >= df['roa'].quantile(0.01)) &
    (df['roa'] <= df['roa'].quantile(0.99))
].copy()

print(f"Sample size: {len(sample)} firm-years")
print(f"Unique firms: {sample['firm_id'].nunique()}")
print(f"Years: {sample['year'].min()}-{sample['year'].max()}")
print(f"Industries (SIC2): {sample['sic_code'].apply(lambda x: x//100).nunique()}")

# ã‚µãƒ³ãƒ—ãƒ«è¨˜è¿°çµ±è¨ˆ
sample[['roa', 'rd_intensity', 'firm_size', 'leverage']].describe()
```

**Survival Biaså¯¾ç­–**:
```python
# ç”Ÿå­˜ãƒã‚¤ã‚¢ã‚¹ãƒã‚§ãƒƒã‚¯: å…¨æœŸé–“å­˜ç¶šä¼æ¥­ã®ã¿ã‹ï¼Ÿ
full_panel_firms = sample.groupby('firm_id')['year'].count()
expected_years = sample['year'].max() - sample['year'].min() + 1

print(f"Expected years per firm: {expected_years}")
print(f"Mean years per firm: {full_panel_firms.mean():.2f}")
print(f"% of firms with full panel: {(full_panel_firms == expected_years).mean()*100:.1f}%")

# æ¨å¥¨: Unbalanced panelã‚’è¨±å®¹ï¼ˆç”Ÿå­˜ãƒã‚¤ã‚¢ã‚¹è»½æ¸›ï¼‰
```

---

## Phase 2: Data Source Discovery

### 2.1 ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ

â†’ **è©³ç´°**: `2-data-sources` skill

**åœ°åŸŸåˆ¥ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**:

| åœ°åŸŸ | è²¡å‹™ãƒ‡ãƒ¼ã‚¿ | æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ | å½¹å“¡ãƒ‡ãƒ¼ã‚¿ |
|------|----------|----------|-----------|
| **åŒ—ç±³** | Compustat â­â­â­â­â­ | CRSP â­â­â­â­â­ | ExecuComp â­â­â­â­â­ |
| **æ¬§å·** | Orbis â­â­â­â­ | Datastream â­â­â­â­ | Orbis â­â­â­ |
| **æ—¥æœ¬** | EDINET â­â­â­â­â­ | JPX â­â­â­â­ | EDINET â­â­â­â­ |
| **éŸ“å›½** | DART â­â­â­â­ | KRX â­â­â­â­ | DART â­â­â­ |
| **ä¸­å›½** | CNINFO â­â­â­ | Tushare â­â­â­ | - |

**ç„¡æ–™ vs æœ‰æ–™**:

```python
# åŒ—ç±³ä¼æ¥­ç ”ç©¶
if has_wrds_access:
    # æœ‰æ–™: WRDSï¼ˆæ¨å¥¨: ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æ°´æº–ï¼‰
    data_source = "Compustat + CRSP via WRDS"
else:
    # ç„¡æ–™: SEC EDGARï¼ˆå“è³ªã¯åŠ£ã‚‹ãŒå¯èƒ½ï¼‰
    data_source = "SEC EDGAR 10-K filings"
    print("Warning: Manual extraction required, quality may vary")

# æ—¥æœ¬ä¼æ¥­ç ”ç©¶
data_source = "EDINET API (å®Œå…¨ç„¡æ–™)"  # æ¨å¥¨: å…¬å¼APIã€é«˜å“è³ª
```

### 2.2 å¤‰æ•°-ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å¯¾å¿œè¡¨

| å¤‰æ•° | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | å–å¾—é›£æ˜“åº¦ |
|------|------------|----------|
| **ROA** | Compustat/EDINET | â­â˜†â˜†â˜†â˜† å®¹æ˜“ |
| **Tobin's Q** | Compustat + CRSP | â­â­â˜†â˜†â˜† ä¸­ |
| **R&D Intensity** | Compustat | â­â˜†â˜†â˜†â˜† å®¹æ˜“ |
| **Patent Count** | USPTO/JPO | â­â­â­â˜†â˜† ä¸­é«˜ |
| **Board Interlock** | Proxy Statement/EDINET | â­â­â­â­â˜† é«˜ |
| **MD&A Sentiment** | SEC 10-K | â­â­â­â­â˜† é«˜ |
| **ESG Score** | MSCI/CDP | â­â­â­â˜†â˜† ä¸­é«˜ |

---

## Phase 3: Data Collection Strategy

### 3.1 åé›†è¨ˆç”»ç«‹æ¡ˆ

**åé›†ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ **:
```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ›´å³ç¦ï¼‰
â”‚   â”œâ”€â”€ processed/        # å‰å‡¦ç†æ¸ˆã¿
â”‚   â””â”€â”€ final/            # åˆ†æç”¨æœ€çµ‚ç‰ˆ
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ 01_collect.py     # ãƒ‡ãƒ¼ã‚¿åé›†
â”‚   â”œâ”€â”€ 02_clean.py       # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
â”‚   â”œâ”€â”€ 03_merge.py       # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸
â”‚   â””â”€â”€ 04_variables.py   # å¤‰æ•°æ§‹ç¯‰
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_dictionary.md # å¤‰æ•°è¾æ›¸
â””â”€â”€ README.md
```

### 3.2 ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè£…

â†’ **è©³ç´°å®Ÿè£…**: `2-data-sources` skill

**åŒ—ç±³ä¼æ¥­ï¼ˆWRDSçµŒç”±ï¼‰**:
```python
import wrds

# WRDSæ¥ç¶š
db = wrds.Connection()

# Compustatè²¡å‹™ãƒ‡ãƒ¼ã‚¿
compustat = db.raw_sql("""
    SELECT gvkey, datadate, fyear, sich, 
           at AS total_assets,
           sale AS revenue,
           ni AS net_income,
           xrd AS rd_expense,
           dltt AS long_term_debt,
           dlc AS current_debt
    FROM comp.funda
    WHERE indfmt='INDL' AND datafmt='STD' AND popsrc='D' AND consol='C'
        AND fyear BETWEEN 2010 AND 2020
        AND sich BETWEEN 2000 AND 3999
""")

# CRSPæ ªä¾¡ãƒ‡ãƒ¼ã‚¿
crsp = db.raw_sql("""
    SELECT permno, date, prc, shrout
    FROM crsp.dsf
    WHERE date BETWEEN '2010-01-01' AND '2020-12-31'
""")

db.close()
```

**æ—¥æœ¬ä¼æ¥­ï¼ˆEDINET APIï¼‰**:
```python
# â†’ è©³ç´°å®Ÿè£…ã¯ 2-data-sources skillå‚ç…§
from edinet_collector import EDINETCollector

collector = EDINETCollector()
df_japan = collector.collect_sample(
    start_date='2010-01-01',
    end_date='2020-12-31',
    doc_type='æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸'
)
```

### 3.3 ãƒªã‚¹ã‚¯ç®¡ç†

| ãƒªã‚¹ã‚¯ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– |
|--------|---------|------|
| **APIåˆ¶é™** | é«˜ | time.sleep(), ãƒãƒƒãƒå‡¦ç† |
| **æ¬ æå€¤** | é«˜ | å¤šé‡ä»£å…¥æ³•, é™¤å¤–åŸºæº–æ˜ç¢ºåŒ– |
| **ãƒ‡ãƒ¼ã‚¿å½¢å¼ä¸æ•´åˆ** | ä¸­ | æ¨™æº–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ |
| **ã‚¢ã‚¯ã‚»ã‚¹æœŸé™åˆ‡ã‚Œ** | ä½ | ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ |

---

## Phase 4: Panel Dataset Construction

### 4.1 Panelæ§‹é€ ã®ç†è§£

**Panel Dataæ§‹é€ **:
```
       firm_id  year  roa  rd_intensity  firm_size
0      1001     2010  0.05  0.03          8.5
1      1001     2011  0.06  0.04          8.6
2      1001     2012  0.04  0.03          8.7
...
1000   2500     2020  0.08  0.05          9.2
```

**ç‰¹å¾´**:
- **Cross-sectional dimension**: è¤‡æ•°ä¼æ¥­ï¼ˆfirm_idï¼‰
- **Time-series dimension**: è¤‡æ•°å¹´ï¼ˆyearï¼‰
- **Balanced vs Unbalanced**: å…¨ä¼æ¥­ãŒå…¨æœŸé–“å­˜åœ¨ã™ã‚‹ã‹

### 4.2 MultiIndexè¨­å®š

```python
import pandas as pd

# Panelæ§‹é€ è¨­å®š
df_panel = df.set_index(['firm_id', 'year']).sort_index()

print(df_panel.head())
print(f"Panel shape: {df_panel.shape}")
print(f"Firms: {df_panel.index.get_level_values('firm_id').nunique()}")
print(f"Years: {df_panel.index.get_level_values('year').nunique()}")

# Balanced panelç¢ºèª
firms_per_year = df_panel.groupby('year').size()
print("\nObservations per year:")
print(firms_per_year)
```

### 4.3 è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸

**ãƒãƒ¼ã‚¸æˆ¦ç•¥**:

```python
# 1. Compustatè²¡å‹™ãƒ‡ãƒ¼ã‚¿
df_financial = compustat[['gvkey', 'fyear', 'at', 'sale', 'ni', 'xrd']].copy()
df_financial = df_financial.rename(columns={'gvkey': 'firm_id', 'fyear': 'year'})

# 2. CRSPæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ â†’ å¹´æ¬¡å¹³å‡
df_price = crsp.groupby(['permno', crsp['date'].dt.year])['prc'].mean().reset_index()
df_price = df_price.rename(columns={'permno': 'firm_id', 'date': 'year'})

# 3. Patent ãƒ‡ãƒ¼ã‚¿
df_patent = patents[['assignee_id', 'year', 'patent_count']].copy()
df_patent = df_patent.rename(columns={'assignee_id': 'firm_id'})

# ãƒãƒ¼ã‚¸
df_merged = df_financial.merge(
    df_price, on=['firm_id', 'year'], how='left'
).merge(
    df_patent, on=['firm_id', 'year'], how='left'
)

# æ¬ æå€¤ç¢ºèª
print("Missing values:")
print(df_merged.isnull().sum())
```

**ãƒãƒ¼ã‚¸è¨ºæ–­**:
```python
# ãƒãƒ¼ã‚¸æˆåŠŸç‡
print(f"Financial data: {len(df_financial)} obs")
print(f"Price data: {len(df_price)} obs")
print(f"Merged data: {len(df_merged)} obs")
print(f"Match rate: {len(df_merged) / len(df_financial) * 100:.1f}%")

# æœªãƒãƒƒãƒä¼æ¥­ã®ç¢ºèª
unmatched_firms = set(df_financial['firm_id']) - set(df_merged['firm_id'])
print(f"Unmatched firms: {len(unmatched_firms)}")
```

---

## Phase 5: Quality Assurance

### 5.1 QAãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

â†’ **è©³ç´°**: `_shared/quality-checklist.md`

**å¿…é ˆãƒã‚§ãƒƒã‚¯**:
- [ ] **æ¬ æå€¤**: å„å¤‰æ•°ã®æ¬ æç‡ < 10%
- [ ] **å¤–ã‚Œå€¤**: Winsorization (1%, 99%)å®Ÿæ–½
- [ ] **é‡è¤‡**: firm_id Ã— year ã®ä¸€æ„æ€§ç¢ºèª
- [ ] **å¤‰æ•°ç¯„å›²**: è«–ç†çš„ç¯„å›²å†…ã‹ï¼ˆROA: -1ï½1, Leverage: 0ï½10ï¼‰
- [ ] **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º**: æœ€ä½500 firm-yearsï¼ˆæ¨å¥¨: 1000+ï¼‰
- [ ] **Survival bias**: Unbalanced panelè¨±å®¹orå¯¾ç­–

### 5.2 å®Ÿè£…: QAè‡ªå‹•åŒ–

```python
def data_quality_check(df, id_var='firm_id', time_var='year'):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯è‡ªå‹•åŒ–"""
    
    print("="*50)
    print("DATA QUALITY ASSURANCE REPORT")
    print("="*50)
    
    # 1. åŸºæœ¬æƒ…å ±
    print(f"\n1. BASIC INFO")
    print(f"   Shape: {df.shape}")
    print(f"   Firms: {df[id_var].nunique()}")
    print(f"   Years: {df[time_var].min()}-{df[time_var].max()}")
    
    # 2. æ¬ æå€¤
    print(f"\n2. MISSING VALUES")
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_report = pd.DataFrame({
        'Missing': missing,
        'Percentage': missing_pct
    })
    print(missing_report[missing_report['Missing'] > 0])
    
    # 3. é‡è¤‡
    print(f"\n3. DUPLICATES")
    duplicates = df.duplicated(subset=[id_var, time_var]).sum()
    print(f"   Duplicate rows: {duplicates}")
    
    # 4. å¤–ã‚Œå€¤
    print(f"\n4. OUTLIERS")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        outliers = ((df[col] < q1 - 1.5*iqr) | (df[col] > q3 + 1.5*iqr)).sum()
        print(f"   {col}: {outliers} outliers ({outliers/len(df)*100:.1f}%)")
    
    # 5. Panel balance
    print(f"\n5. PANEL BALANCE")
    firms_per_year = df.groupby(time_var)[id_var].nunique()
    print(f"   Mean firms/year: {firms_per_year.mean():.0f}")
    print(f"   Min firms/year: {firms_per_year.min()}")
    print(f"   Max firms/year: {firms_per_year.max()}")
    
    print("="*50)
    print("QA CHECK COMPLETE")
    print("="*50)

# å®Ÿè¡Œ
data_quality_check(df_panel.reset_index())
```

---

## Phase 6: Variable Construction

### 6.1 æ¨™æº–å¤‰æ•°æ§‹ç¯‰

â†’ **è©³ç´°**: `_shared/common-definitions.md`

**Performance Variables**:
```python
# ROA
df['roa'] = df['net_income'] / df['total_assets']

# ROE
df['roe'] = df['net_income'] / df['equity']

# Tobin's Q (Market Value / Book Value)
df['tobin_q'] = (df['market_cap'] + df['total_debt']) / df['total_assets']
```

**Innovation Variables**:
```python
# R&D Intensity
df['rd_intensity'] = df['rd_expense'] / df['revenue']
df['rd_intensity'] = df['rd_intensity'].fillna(0)  # æœªå ±å‘Š = 0

# Patent Count (log+1)
df['patent_count_log'] = np.log(df['patent_count'] + 1)

# Citation-weighted Patents
df['patent_quality'] = df['citation_count'] / df['patent_count']
```

**Control Variables**:
```python
# Firm Size (log of total assets)
df['firm_size'] = np.log(df['total_assets'])

# Leverage
df['leverage'] = df['total_debt'] / df['total_assets']

# Firm Age (log)
df['firm_age_log'] = np.log(df['firm_age'] + 1)

# Cash Ratio
df['cash_ratio'] = df['cash'] / df['total_assets']
```

### 6.2 ãƒ©ã‚°å¤‰æ•°ã®ä½œæˆ

```python
# 1å¹´ãƒ©ã‚°ï¼ˆç‹¬ç«‹å¤‰æ•°ï¼‰
df = df.sort_values(['firm_id', 'year'])
df['rd_intensity_lag1'] = df.groupby('firm_id')['rd_intensity'].shift(1)
df['firm_size_lag1'] = df.groupby('firm_id')['firm_size'].shift(1)

# ç†ç”±: å†…ç”Ÿæ€§è»½æ¸›ï¼ˆåŒæ™‚æ€§ãƒã‚¤ã‚¢ã‚¹å›é¿ï¼‰
# DV: ROA_t ã‚’ IV: RD_intensity_{t-1} ã§èª¬æ˜
```

### 6.3 èª¿æ•´å¤‰æ•°ãƒ»äº¤äº’ä½œç”¨é …

```python
# èª¿æ•´å¤‰æ•°ã®æ¨™æº–åŒ–ï¼ˆmean=0, sd=1ï¼‰
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df['rd_intensity_std'] = scaler.fit_transform(df[['rd_intensity']])
df['industry_maturity_std'] = scaler.fit_transform(df[['industry_maturity']])

# äº¤äº’ä½œç”¨é …
df['rd_x_maturity'] = df['rd_intensity_std'] * df['industry_maturity_std']
```

---

## Phase 7: Statistical Analysis

### 7.1 è¨˜è¿°çµ±è¨ˆ

```python
# è¨˜è¿°çµ±è¨ˆ
desc_stats = df[['roa', 'rd_intensity', 'firm_size', 'leverage']].describe()
print(desc_stats.T)

# ç›¸é–¢è¡Œåˆ—
corr_matrix = df[['roa', 'rd_intensity', 'firm_size', 'leverage']].corr()
print("\nCorrelation Matrix:")
print(corr_matrix)

# VIFï¼ˆå¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯ï¼‰
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df[['rd_intensity', 'firm_size', 'leverage']].dropna()
vif = pd.DataFrame({
    'Variable': X.columns,
    'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
})
print("\nVIF (Multicollinearity Check):")
print(vif)
# VIF > 10: å¤šé‡å…±ç·šæ€§ã‚ã‚Š
```

### 7.2 ãƒ‘ãƒãƒ«å›å¸°

â†’ **è©³ç´°**: `3-statistical-methods` skill

**Fixed Effectså›å¸°**:
```python
from linearmodels.panel import PanelOLS

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
df_panel = df.set_index(['firm_id', 'year'])

# FEå›å¸°
model_fe = PanelOLS.from_formula(
    'roa ~ rd_intensity_lag1 + firm_size_lag1 + leverage + EntityEffects + TimeEffects',
    data=df_panel
)

result_fe = model_fe.fit(cov_type='clustered', cluster_entity=True)
print(result_fe.summary)
```

**Random Effectså›å¸°**:
```python
from linearmodels.panel import RandomEffects

model_re = RandomEffects.from_formula(
    'roa ~ rd_intensity_lag1 + firm_size_lag1 + leverage',
    data=df_panel
)

result_re = model_re.fit()
print(result_re.summary)
```

**Hausman Test (FE vs RE)**:
```python
# Hausman testå®Ÿè£…
# H0: REé©åˆ‡, H1: FEé©åˆ‡
# å®Ÿè£…ã¯ 3-statistical-methods skillå‚ç…§
```

### 7.3 èª¿æ•´åŠ¹æœæ¤œè¨¼

```python
# äº¤äº’ä½œç”¨é …ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«
model_interaction = PanelOLS.from_formula(
    'roa ~ rd_intensity_std + industry_maturity_std + rd_x_maturity + '
    'firm_size_lag1 + leverage + EntityEffects + TimeEffects',
    data=df_panel
)

result_int = model_interaction.fit(cov_type='clustered', cluster_entity=True)
print(result_int.summary)

# äº¤äº’ä½œç”¨ã®è§£é‡ˆ: rd_x_maturityä¿‚æ•°
# æ­£: ç”£æ¥­æˆç†Ÿåº¦ãŒé«˜ã„ã»ã©ã€R&Dã®åŠ¹æœãŒå¼·ã¾ã‚‹
# è² : ç”£æ¥­æˆç†Ÿåº¦ãŒé«˜ã„ã»ã©ã€R&Dã®åŠ¹æœãŒå¼±ã¾ã‚‹
```

---

## Phase 8: Documentation & Reproducibility

### 8.1 å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ

â†’ **è©³ç´°**: `8-automation` skill

**å¿…é ˆè¦ç´ **:
```
replication_package/
â”œâ”€â”€ README.md              # å®Ÿè¡Œæ‰‹é †
â”œâ”€â”€ requirements.txt       # Pythonä¾å­˜é–¢ä¿‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # ç”Ÿãƒ‡ãƒ¼ã‚¿
â”‚   â””â”€â”€ processed/        # å‰å‡¦ç†æ¸ˆã¿
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ 01_collect.py
â”‚   â”œâ”€â”€ 02_clean.py
â”‚   â”œâ”€â”€ 03_analysis.py
â”‚   â””â”€â”€ run_all.sh        # å…¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ tables/           # å›å¸°çµæœè¡¨
â”‚   â””â”€â”€ figures/          # å›³
â””â”€â”€ docs/
    â”œâ”€â”€ data_dictionary.md
    â””â”€â”€ codebook.pdf
```

### 8.2 Data Dictionaryä½œæˆ

```markdown
# Data Dictionary

## Variables

| Variable | Description | Source | Calculation |
|----------|-------------|--------|-------------|
| roa | Return on Assets | Compustat | net_income / total_assets |
| rd_intensity | R&D Intensity | Compustat | rd_expense / revenue |
| firm_size | Firm Size (log) | Compustat | log(total_assets) |
| leverage | Financial Leverage | Compustat | total_debt / total_assets |

## Sample Selection

- **Data Source**: Compustat North America (WRDS)
- **Period**: 2010-2020
- **Industry**: Manufacturing (SIC 2000-3999)
- **Sample Size**: 12,450 firm-years (1,245 unique firms)
- **Missing Data**: Dropped if ROA or R&D missing
- **Winsorization**: 1% and 99% for all continuous variables
```

---

## Quick Start Guide

### 30åˆ†ã§ã‚¹ã‚¿ãƒ¼ãƒˆ

**Step 1**: RQã‚’1æ–‡ã§æ›¸ãï¼ˆ5åˆ†ï¼‰
```
ä¾‹: ã€Œç’°å¢ƒä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ã§ã€æ¢ç´¢çš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãŒçŸ­æœŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«
ä¸ãˆã‚‹è² ã®å½±éŸ¿ã¯ã€ã©ã®ã‚ˆã†ãªçµ„ç¹”çš„æ¡ä»¶ã§ç·©å’Œã•ã‚Œã‚‹ã‹ï¼Ÿã€
```

**Step 2**: ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é¸æŠï¼ˆ10åˆ†ï¼‰
â†’ `_shared/THEORY_FRAMEWORKS.md` å‚ç…§
```
ä¾‹: Dynamic Capabilities + Organizational Learning
```

**Step 3**: ä»®èª¬ã‚’3ã¤å°å‡ºï¼ˆ10åˆ†ï¼‰
```
H1: æ¢ç´¢çš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ â†’ çŸ­æœŸROAï¼ˆè² ï¼‰
H2: Organizational Slack â†’ H1ã‚’å¼±ã‚ã‚‹ï¼ˆèª¿æ•´åŠ¹æœï¼‰
H3: Absorptive Capacity â†’ H1ã‚’å¼±ã‚ã‚‹ï¼ˆèª¿æ•´åŠ¹æœï¼‰
```

**Step 4**: å¿…è¦ãªå¤‰æ•°ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ï¼ˆ5åˆ†ï¼‰
â†’ `_shared/common-definitions.md` å‚ç…§

### 1é€±é–“ã§Phase 1-3å®Œäº†

**Day 1-2**: Phase 1ï¼ˆResearch Designï¼‰
- RQç²¾ç·»åŒ–
- æ–‡çŒ®ãƒ¬ãƒ“ãƒ¥ãƒ¼
- ä»®èª¬å°å‡º

**Day 3-4**: Phase 2ï¼ˆData Source Discoveryï¼‰
â†’ `2-data-sources` skillä½¿ç”¨
- åŒ—ç±³: Compustaté¸æŠ
- æ—¥æœ¬: EDINETé¸æŠ

**Day 5-7**: Phase 3ï¼ˆData Collection Strategyï¼‰
â†’ `2-data-sources` + `8-automation` skills
- APIå®Ÿè£…
- ãƒãƒƒãƒåé›†

### 1ãƒ¶æœˆã§Phase 1-8å®Œèµ°

**Week 1**: Phase 1-3ï¼ˆè¨­è¨ˆã€œåé›†æˆ¦ç•¥ï¼‰
**Week 2**: Phase 4-5ï¼ˆPanelæ§‹ç¯‰ã€œQAï¼‰
**Week 3**: Phase 6-7ï¼ˆå¤‰æ•°æ§‹ç¯‰ã€œåˆ†æï¼‰
**Week 4**: Phase 8ï¼ˆDocumentationï¼‰ + è«–æ–‡åŸ·ç­†é–‹å§‹

---

## Common Pitfalls

### Pitfall 1: æ›–æ˜§ãªRQ

âŒ **æ‚ªã„ä¾‹**: ã€Œã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚ã€

**å•é¡Œ**:
- æ—¢å­˜ç ”ç©¶ã§æ—¢ã«ç¢ºç«‹
- å¤‰æ•°ãŒæ›–æ˜§ï¼ˆã€Œã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã€ã¨ã¯ï¼Ÿï¼‰
- æ¡ä»¶ãƒ»ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãªã—

âœ… **è‰¯ã„ä¾‹**: ã€Œç’°å¢ƒä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„çŠ¶æ³ã§ã€æ¢ç´¢çš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç‰¹è¨±ã®æ–°è¦æŠ€è¡“ã‚¯ãƒ©ã‚¹æ¯”ç‡ï¼‰ãŒçŸ­æœŸROAã«ä¸ãˆã‚‹è² ã®å½±éŸ¿ã¯ã€çµ„ç¹”ã‚¹ãƒ©ãƒƒã‚¯ï¼ˆç¾é‡‘æ¯”ç‡ï¼‰ã«ã‚ˆã£ã¦ã©ã®ç¨‹åº¦ç·©å’Œã•ã‚Œã‚‹ã‹ï¼Ÿã€

### Pitfall 2: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³

âŒ **æ‚ªã„ä¾‹**: 100ç¤¾Ã—3å¹´ = 300 firm-years

**å•é¡Œ**:
- ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åŸºæº–ï¼ˆ2000+ï¼‰ã«é ãåŠã°ãªã„
- çµ±è¨ˆçš„æ¤œå‡ºåŠ›ä¸è¶³
- Robustness checkså®Ÿæ–½ä¸å¯

âœ… **è§£æ±ºç­–**:
- æœŸé–“å»¶é•·: 3å¹´ â†’ 10å¹´
- ç”£æ¥­æ‹¡å¤§: è£½é€ æ¥­ã®ã¿ â†’ å…¨ç”£æ¥­
- åœ°åŸŸæ‹¡å¤§: ç±³å›½ã®ã¿ â†’ åŒ—ç±³å…¨ä½“

### Pitfall 3: å†…ç”Ÿæ€§ã¸ã®ç„¡å¯¾ç­–

âŒ **æ‚ªã„ä¾‹**: OLSå›å¸°ã®ã¿

**å•é¡Œ**:
- é€†å› æœ: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è‰¯å¥½ â†’ R&DæŠ•è³‡å¢—åŠ 
- åŒæ™‚æ€§ãƒã‚¤ã‚¢ã‚¹
- äº¤çµ¡å¤‰æ•°

âœ… **è§£æ±ºç­–**:
â†’ `3-statistical-methods` skillå‚ç…§
- ãƒ©ã‚°å¤‰æ•°ä½¿ç”¨: IV_{t-1} â†’ DV_t
- Instrumental Variables (IV)
- PSM, Heckman Selection

### Pitfall 4: ç†è«–ã¨åˆ†æã®ä¹–é›¢

âŒ **æ‚ªã„ä¾‹**: ç†è«–ã§ã€Œå‹•çš„ãƒ—ãƒ­ã‚»ã‚¹ã€ã‚’ä¸»å¼µã™ã‚‹ãŒã€åˆ†æã¯ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³

**å•é¡Œ**:
- å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒæ¤œè¨¼ã•ã‚Œã¦ã„ãªã„
- æ™‚é–“çš„é †åºãŒä¸æ˜

âœ… **è§£æ±ºç­–**:
- Panelåˆ†æã§æ™‚é–“çš„é †åºã‚’æ˜ç¤º
- ãƒ©ã‚°æ§‹é€ ã§å› æœã®æ–¹å‘æ€§ã‚’ç¤ºã™
- åª’ä»‹åˆ†æã§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¤œè¨¼

### Pitfall 5: Robustness Checksä¸è¶³

âŒ **æ‚ªã„ä¾‹**: ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«1ã¤ã®ã¿

**å•é¡Œ**:
- çµæœã®é ‘å¥æ€§ãŒä¸æ˜
- ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã‹ã‚‰è¦æ±‚ã•ã‚Œã‚‹

âœ… **è§£æ±ºç­–**: æœ€ä½5ã¤ã®Robustness Checks
1. ä»£æ›¿DVï¼ˆROA â†’ ROE, Tobin's Qï¼‰
2. ä»£æ›¿IVæ¸¬å®šï¼ˆR&D Intensity â†’ Patent Countï¼‰
3. ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå¤§ä¼æ¥­ã®ã¿ã€å°ä¼æ¥­ã®ã¿ï¼‰
4. ä»£æ›¿æ¨å®šæ³•ï¼ˆFE â†’ GMMï¼‰
5. Winsorizationæ°´æº–å¤‰æ›´ï¼ˆ1%,99% â†’ 5%,95%ï¼‰

---

## FAQ

### Q1: åˆã‚ã¦ã®å®šé‡ç ”ç©¶ã€‚ã©ã“ã‹ã‚‰å§‹ã‚ã‚‹ã¹ãã‹ï¼Ÿ

**A**: ä»¥ä¸‹ã®3ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¨å¥¨ã—ã¾ã™ï¼š

**Step 1**: æ—¢å­˜ç ”ç©¶ã®ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆ2-3é€±é–“ï¼‰
- ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆSMJ, AMJ, OSï¼‰ã‹ã‚‰1æœ¬é¸ã¶
- ãƒ‡ãƒ¼ã‚¿åé›†ã€œåˆ†æã‚’å®Œå…¨å†ç¾
- æ‰‹æ³•ã‚’å­¦ã¶æœ€é€Ÿã®æ–¹æ³•

**Step 2**: å°è¦æ¨¡ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆ1-2é€±é–“ï¼‰
- 100ç¤¾Ã—5å¹´ã§è©¦è¡Œ
- Phase 1-8ã‚’ä¸€é€šã‚ŠçµŒé¨“
- å•é¡Œç‚¹ã‚’æ´—ã„å‡ºã™

**Step 3**: æœ¬æ ¼ç ”ç©¶é–‹å§‹ï¼ˆ3-6ãƒ¶æœˆï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ‹¡å¤§: 1000+ç¤¾Ã—10å¹´
- Phase 1ã‹ã‚‰æœ¬ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—é€šã‚Šã«å®Ÿè¡Œ

---

### Q2: ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆSMJ, AMJ, OSï¼‰ã«è¼‰ã‚‹ã«ã¯ï¼Ÿ

**A**: ä»¥ä¸‹ã®åŸºæº–ã‚’æº€ãŸã™å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

**ç†è«–çš„è²¢çŒ®**:
- æ—¢å­˜ç†è«–ã®æ‹¡å¼µãƒ»ä¿®æ­£
- æ–°ã—ã„å› æœãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç™ºè¦‹
- å¢ƒç•Œæ¡ä»¶ï¼ˆBoundary Conditionsï¼‰ã®ç‰¹å®š

**æ–¹æ³•è«–çš„å³å¯†æ€§**:
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: **2000+ firm-years**
- å†…ç”Ÿæ€§å¯¾ç­–: **IV, PSM, Heckmanç­‰**
- Robustness checks: **æœ€ä½5ã¤**
- Fixed Effects + Clustered SE

**å®Ÿè£…ã‚¬ã‚¤ãƒ‰**:
â†’ `_shared/quality-checklist.md#top-journal-standards`

---

### Q3: ãƒ‡ãƒ¼ã‚¿åé›†ã«ä½•æ—¥ã‹ã‹ã‚‹ã‹ï¼Ÿ

**A**: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨è¦æ¨¡ã«ã‚ˆã‚Šç•°ãªã‚Šã¾ã™ï¼š

| ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | è¦æ¨¡ | æ‰€è¦æ™‚é–“ |
|-------------|------|---------|
| **Compustat (WRDS)** | 1000ç¤¾Ã—10å¹´ | **1-2æ—¥** |
| **EDINET (æ—¥æœ¬)** | 1000ç¤¾Ã—10å¹´ | **3-5æ—¥** |
| **SEC EDGAR (æ‰‹å‹•)** | 100ç¤¾Ã—10å¹´ | **2-3é€±é–“** |
| **Patent (USPTO)** | 1000ç¤¾Ã—10å¹´ | **1-2é€±é–“** |

**æ™‚é–“çŸ­ç¸®Tips**:
â†’ `8-automation` skillï¼ˆè‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰
â†’ `2-data-sources` skillï¼ˆAPIå®Ÿè£…ä¾‹ï¼‰

---

### Q4: Pythonã®çŸ¥è­˜ãŒãªã„ã€‚å­¦ã¶ã¹ãã‹ï¼Ÿ

**A**: **å¿…é ˆã§ã™**ã€‚ä»¥ä¸‹ã‚’ç¿’å¾—ã—ã¦ãã ã•ã„ï¼š

**æœ€ä½é™å¿…è¦ãªã‚¹ã‚­ãƒ«**ï¼ˆç¿’å¾—æ™‚é–“: 2-3é€±é–“ï¼‰:
- **pandas**: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ
- **numpy**: æ•°å€¤è¨ˆç®—
- **statsmodels/linearmodels**: å›å¸°åˆ†æ
- **matplotlib/seaborn**: å¯è¦–åŒ–

**å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹**:
- [Python for Data Analysis](https://wesmckinney.com/book/) (Wes McKinney)
- [pandaså…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://pandas.pydata.org/docs/getting_started/index.html)
- `8-automation` skillï¼ˆå®Ÿè£…ä¾‹ãŒè±Šå¯Œï¼‰

**ä»£æ›¿æ¡ˆ**:
- **Stata**: å­¦è¡“ç ”ç©¶ã§åºƒãä½¿ç”¨ã€ãƒ‘ãƒãƒ«åˆ†æã«å¼·ã„
- **R**: çµ±è¨ˆåˆ†æãƒ»å¯è¦–åŒ–ã«å„ªã‚Œã‚‹
â†’ ãŸã ã—**PythonãŒæœ€ã‚‚æ±ç”¨æ€§ãŒé«˜ã„**ï¼ˆãƒ‡ãƒ¼ã‚¿åé›†ã€œåˆ†æã€œå¯è¦–åŒ–ï¼‰

---

### Q5: ã©ã®ã‚¹ã‚­ãƒ«ã‚’ã„ã¤ä½¿ã†ã¹ãã‹ï¼Ÿ

**A**: Phaseåˆ¥ã«ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š

| Phase | ä¸»è¦ã‚¹ã‚­ãƒ« | ç›®çš„ |
|-------|-----------|------|
| **Phase 1-3** | æœ¬ã‚¹ã‚­ãƒ«ï¼ˆcore-workflowï¼‰ | ç ”ç©¶è¨­è¨ˆãƒ»è¨ˆç”» |
| **Phase 2** | `2-data-sources` | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¢ç´¢ |
| **Phase 3** | `2-data-sources` + `8-automation` | ãƒ‡ãƒ¼ã‚¿åé›† |
| **Phase 4** | æœ¬ã‚¹ã‚­ãƒ« + `8-automation` | Panelæ§‹ç¯‰ |
| **Phase 5** | `_shared/quality-checklist.md` | QA |
| **Phase 6** | `_shared/common-definitions.md` + å°‚é–€ã‚¹ã‚­ãƒ« | å¤‰æ•°æ§‹ç¯‰ |
| **Phase 7** | `3-statistical-methods` | çµ±è¨ˆåˆ†æ |
| **Phase 8** | `8-automation` | å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ |

**å°‚é–€ã‚¹ã‚­ãƒ«ï¼ˆPhase 6ã§å¿…è¦ã«å¿œã˜ã¦ï¼‰**:
- ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•° â†’ `4-text-analysis`
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰æ•° â†’ `5-network-analysis`
- ESGå¤‰æ•° â†’ `7-esg-sustainability`
- å› æœæ¨è«– â†’ `6-causal-ml`

**è©³ç´°ãªé¸æŠã‚¬ã‚¤ãƒ‰**:
â†’ `SKILL-INDEX.md`ï¼ˆDecision Treeå½¢å¼ï¼‰
â†’ `_shared/cross-references.md`ï¼ˆPhaseåˆ¥å‚ç…§ãƒãƒƒãƒ—ï¼‰

---

### Q6: å†…ç”Ÿæ€§å•é¡Œã¨ã¯ï¼Ÿã©ã†å¯¾å‡¦ã™ã‚‹ã‹ï¼Ÿ

**A**: å†…ç”Ÿæ€§ã®3ã‚¿ã‚¤ãƒ—ã¨å¯¾ç­–ï¼š

**1. Omitted Variable Biasï¼ˆæ¬ è½å¤‰æ•°ãƒã‚¤ã‚¢ã‚¹ï¼‰**
```
å•é¡Œ: é‡è¦ãªå¤‰æ•°ã‚’çµ±åˆ¶ã—ã¦ã„ãªã„
å¯¾ç­–: ç†è«–ã«åŸºã¥ãçµ±åˆ¶å¤‰æ•°ã®è¿½åŠ  + Fixed Effects
```

**2. Reverse Causalityï¼ˆé€†å› æœï¼‰**
```
å•é¡Œ: Y â†’ X ã®é€†æ–¹å‘ã®å› æœé–¢ä¿‚
å¯¾ç­–: ãƒ©ã‚°å¤‰æ•°ï¼ˆX_{t-1} â†’ Y_tï¼‰
     Instrumental Variables (IV)
```

**3. Simultaneityï¼ˆåŒæ™‚æ€§ï¼‰**
```
å•é¡Œ: Xã¨YãŒåŒæ™‚ã«æ±ºå®šã•ã‚Œã‚‹
å¯¾ç­–: 2SLS (Two-Stage Least Squares)
     GMM (Generalized Method of Moments)
```

**è©³ç´°å®Ÿè£…**:
â†’ `3-statistical-methods` skill

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install pandas numpy scipy statsmodels linearmodels

# ãƒ‡ãƒ¼ã‚¿åé›†
pip install wrds requests beautifulsoup4

# å¯è¦–åŒ–
pip install matplotlib seaborn

# WRDSæ¥ç¶šï¼ˆè¦ã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰
pip install wrds

# æ¨å¥¨: å…¨ä¾å­˜é–¢ä¿‚
pip install -r requirements.txt
```

---

## å‚è€ƒæ–‡çŒ®

### å¿…èª­æ–‡çŒ®

**æ–¹æ³•è«–**:
- Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press.
- Angrist, J. D., & Pischke, J.-S. (2009). *Mostly Harmless Econometrics*. Princeton University Press.

**æˆ¦ç•¥ç ”ç©¶**:
- Barney, J. (1991). "Firm resources and sustained competitive advantage." *Journal of Management*, 17(1), 99-120.
- Teece, D. J., Pisano, G., & Shuen, A. (1997). "Dynamic capabilities and strategic management." *Strategic Management Journal*, 18(7), 509-533.

**çµ„ç¹”ç ”ç©¶**:
- March, J. G. (1991). "Exploration and exploitation in organizational learning." *Organization Science*, 2(1), 71-87.

### ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«

- **Strategic Management Journal (SMJ)**
- **Academy of Management Journal (AMJ)**
- **Organization Science (OS)**
- **Journal of Management (JOM)**

---

## Quick Reference: Phase-Skill Mapping

| Phase | Duration | Key Skills | Output |
|-------|----------|-----------|--------|
| **1. Research Design** | 1-2é€±é–“ | core-workflow, THEORY_FRAMEWORKS | RQ, ä»®èª¬ |
| **2. Data Source Discovery** | 3-5æ—¥ | data-sources | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ |
| **3. Data Collection** | 1-3é€±é–“ | data-sources, automation | ç”Ÿãƒ‡ãƒ¼ã‚¿ |
| **4. Panel Construction** | 3-5æ—¥ | core-workflow | ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ |
| **5. Quality Assurance** | 2-3æ—¥ | quality-checklist | ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ |
| **6. Variable Construction** | 3-5æ—¥ | common-definitions, å°‚é–€ã‚¹ã‚­ãƒ« | åˆ†æå¤‰æ•° |
| **7. Statistical Analysis** | 1-2é€±é–“ | statistical-methods | å›å¸°çµæœ |
| **8. Documentation** | 3-5æ—¥ | automation | å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ |

**Total**: ç´„2-3ãƒ¶æœˆï¼ˆãƒ•ãƒ«ã‚¿ã‚¤ãƒ æƒ³å®šï¼‰

---

**Version**: 4.0  
**Last Updated**: 2025-11-01  
**Maintainer**: Strategic Research Suite Team  
**License**: MIT

---

**Next Steps**:
1. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¢ç´¢ â†’ `2-data-sources` skill
2. çµ±è¨ˆåˆ†ææ‰‹æ³• â†’ `3-statistical-methods` skill
3. å®Œå…¨è‡ªå‹•åŒ– â†’ `8-automation` skill
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
