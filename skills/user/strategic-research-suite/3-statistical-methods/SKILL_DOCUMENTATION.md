---
name: strategic-research-statistical-methods
description: Advanced statistical methods for strategic management research including panel regression (Fixed Effects, Random Effects, Pooled OLS), endogeneity solutions (IV, PSM, Heckman, DiD), moderation and mediation analysis, multilevel modeling, survival analysis, and comprehensive robustness checks with implementation examples.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 7 (Statistical Analysis)
  - data-sources: Data integration for analysis
  - text-analysis: Text-derived variables in regression
  - network-analysis: Network variables in regression
  - causal-ml: Advanced causal inference methods
---

# Statistical Methods Advanced v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥çµŒå–¶ãƒ»çµ„ç¹”è«–ç ”ç©¶ã§ä½¿ç”¨ã™ã‚‹**é«˜åº¦ãªçµ±è¨ˆæ‰‹æ³•**ã‚’ã€å®Ÿè£…å¯èƒ½ãªPythonã‚³ãƒ¼ãƒ‰ä»˜ãã§æä¾›ã—ã¾ã™ã€‚

### ã‚«ãƒãƒ¬ãƒƒã‚¸

```
çµ±è¨ˆæ‰‹æ³•:
â”œâ”€ ãƒ‘ãƒãƒ«å›å¸°: FE, RE, Pooled OLS
â”œâ”€ å†…ç”Ÿæ€§å¯¾ç­–: IV, PSM, Heckman, DiD
â”œâ”€ èª¿æ•´åŠ¹æœãƒ»åª’ä»‹åŠ¹æœåˆ†æ
â”œâ”€ å¤šéšå±¤ãƒ¢ãƒ‡ãƒ« (MLM)
â”œâ”€ ç”Ÿå­˜åˆ†æ (Cox Hazard)
â”œâ”€ Robustness Checks
â””â”€ è¨ºæ–­ãƒ†ã‚¹ãƒˆ (VIF, ç•°åˆ†æ•£æ€§ç­‰)
```

---

### ã„ã¤ä½¿ã†ã‹

âœ… **Phase 7: Statistical Analysis**
- ä»®èª¬æ¤œè¨¼ã®ãŸã‚ã®çµ±è¨ˆåˆ†æ
- å†…ç”Ÿæ€§å•é¡Œã¸ã®å¯¾å‡¦
- Robustness checksã®å®Ÿæ–½

âœ… **è«–æ–‡åŸ·ç­†æ™‚**
- ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åŸºæº–ã®åˆ†æ
- æŸ»èª­è€…ã‚³ãƒ¡ãƒ³ãƒˆã¸ã®å¯¾å¿œ

âœ… **ç ”ç©¶è¨­è¨ˆæ®µéš**
- é©åˆ‡ãªåˆ†ææ‰‹æ³•ã®é¸æŠ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—

---

### å‰ææ¡ä»¶

**å¿…é ˆçŸ¥è­˜**:
- å›å¸°åˆ†æã®åŸºç¤
- ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¦‚å¿µ
- çµ±è¨ˆçš„ä»®èª¬æ¤œå®š

**æ¨å¥¨çŸ¥è­˜**:
- è¨ˆé‡çµŒæ¸ˆå­¦ã®åŸºç¤
- å› æœæ¨è«–ã®æ¦‚å¿µ
- Python/RçµŒé¨“

**æŠ€è¡“ç’°å¢ƒ**:
- Python 3.8ä»¥ä¸Š
- statsmodels, linearmodels

---

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

| ç”¨é€” | é€£æºã‚¹ã‚­ãƒ« | ç›®çš„ |
|------|-----------|------|
| åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ | `1-core-workflow` Phase 7 | åˆ†æã®ä½ç½®ã¥ã‘ |
| ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•° | `4-text-analysis` | ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆç­‰ã‚’èª¬æ˜å¤‰æ•°ã« |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¤‰æ•° | `5-network-analysis` | Centralityç­‰ã‚’èª¬æ˜å¤‰æ•°ã« |
| é«˜åº¦å› æœæ¨è«– | `6-causal-ml` | ML+å› æœæ¨è«– |

---

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ‘ãƒãƒ«å›å¸°](#1-ãƒ‘ãƒãƒ«å›å¸°)
2. [å†…ç”Ÿæ€§å¯¾ç­–](#2-å†…ç”Ÿæ€§å¯¾ç­–)
3. [èª¿æ•´åŠ¹æœåˆ†æ](#3-èª¿æ•´åŠ¹æœåˆ†æ)
4. [åª’ä»‹åŠ¹æœåˆ†æ](#4-åª’ä»‹åŠ¹æœåˆ†æ)
5. [å¤šéšå±¤ãƒ¢ãƒ‡ãƒ«](#5-å¤šéšå±¤ãƒ¢ãƒ‡ãƒ«mlm)
6. [ç”Ÿå­˜åˆ†æ](#6-ç”Ÿå­˜åˆ†æ)
7. [Robustness Checks](#7-robustness-checks)
8. [è¨ºæ–­ãƒ†ã‚¹ãƒˆ](#8-è¨ºæ–­ãƒ†ã‚¹ãƒˆ)

---

## 1. ãƒ‘ãƒãƒ«å›å¸°

### 1.1 åŸºæœ¬æ¦‚å¿µ

**ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿**: è¤‡æ•°ã®ä¼æ¥­ï¼ˆiï¼‰ã‚’è¤‡æ•°ã®æ™‚ç‚¹ï¼ˆtï¼‰ã§è¦³æ¸¬

```
ãƒ‡ãƒ¼ã‚¿æ§‹é€ :
- N: ä¼æ¥­æ•°
- T: æœŸé–“
- N Ã— T: ç·è¦³æ¸¬æ•°
```

**3ã¤ã®ä¸»è¦æ‰‹æ³•**:
1. **Pooled OLS**: ãƒ‘ãƒãƒ«æ§‹é€ ã‚’ç„¡è¦–
2. **Fixed Effects (FE)**: ä¼æ¥­å›ºæœ‰åŠ¹æœã‚’çµ±åˆ¶
3. **Random Effects (RE)**: ä¼æ¥­å›ºæœ‰åŠ¹æœã‚’ãƒ©ãƒ³ãƒ€ãƒ ã¨ä»®å®š

---

### 1.2 Pooled OLS

**ãƒ¢ãƒ‡ãƒ«**:
```
Y_it = Î²â‚€ + Î²â‚X_it + Î²â‚‚Controls_it + Îµ_it
```

**ç‰¹å¾´**:
- æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«
- ä¼æ¥­å›ºæœ‰åŠ¹æœã‚’ç„¡è¦–ï¼ˆãƒã‚¤ã‚¢ã‚¹ã®å¯èƒ½æ€§ï¼‰

**å®Ÿè£…**:

```python
import pandas as pd
import statsmodels.formula.api as smf

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ—¢ã«ãƒ‘ãƒãƒ«æ§‹é€ ï¼‰
# df: (firm_id, year)ã®MultiIndex

# Pooled OLS
model_pooled = smf.ols('''
roa ~ rd_intensity + firm_size + leverage + firm_age + 
      capital_intensity + C(industry) + C(year)
''', data=df.reset_index()).fit(
    cov_type='cluster',
    cov_kwds={'groups': df.reset_index()['firm_id']}
)

print(model_pooled.summary())

# çµæœã®ä¿å­˜
results_pooled = {
    'coef_rd': model_pooled.params['rd_intensity'],
    'se_rd': model_pooled.bse['rd_intensity'],
    'pval_rd': model_pooled.pvalues['rd_intensity'],
    'r2': model_pooled.rsquared,
    'n_obs': model_pooled.nobs
}

print(f"R&Dä¿‚æ•°: {results_pooled['coef_rd']:.4f} (p={results_pooled['pval_rd']:.3f})")
```

---

### 1.3 Fixed Effects (FE)

**ãƒ¢ãƒ‡ãƒ«**:
```
Y_it = Î²â‚X_it + Î±_i + Î»_t + Îµ_it

Î±_i: ä¼æ¥­å›ºå®šåŠ¹æœï¼ˆæ™‚é–“ä¸å¤‰ãªä¼æ¥­ç‰¹æ€§ï¼‰
Î»_t: æ™‚é–“å›ºå®šåŠ¹æœï¼ˆå¹´æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
```

**ç‰¹å¾´**:
- âœ… ä¼æ¥­å›ºæœ‰ã®æ™‚é–“ä¸å¤‰ãªç•°è³ªæ€§ã‚’çµ±åˆ¶
- âœ… å†…ç”Ÿæ€§ã®ä¸€éƒ¨ã‚’å¯¾å‡¦
- âŒ æ™‚é–“ä¸å¤‰ãªå¤‰æ•°ï¼ˆæ¥­ç•Œç­‰ï¼‰ã®åŠ¹æœã‚’æ¨å®šä¸å¯

**å®Ÿè£…**:

```python
from linearmodels.panel import PanelOLS
import pandas as pd

# ãƒ‘ãƒãƒ«æ§‹é€ ç¢ºèª
df_panel = df.copy()
df_panel = df_panel.sort_index()  # (firm_id, year)ã§ã‚½ãƒ¼ãƒˆ

# Fixed Effects Model
model_fe = PanelOLS.from_formula('''
roa ~ rd_intensity + firm_size + leverage + firm_age + 
      capital_intensity + EntityEffects + TimeEffects
''', data=df_panel).fit(
    cov_type='clustered',
    cluster_entity=True  # ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
)

print(model_fe)

# ä¸»è¦çµæœ
print(f"\nR&DåŠ¹æœ:")
print(f"ä¿‚æ•°: {model_fe.params['rd_intensity']:.4f}")
print(f"æ¨™æº–èª¤å·®: {model_fe.std_errors['rd_intensity']:.4f}")
print(f"tçµ±è¨ˆé‡: {model_fe.tstats['rd_intensity']:.2f}")
print(f"på€¤: {model_fe.pvalues['rd_intensity']:.4f}")

# ãƒ¢ãƒ‡ãƒ«é©åˆåº¦
print(f"\nRÂ²: {model_fe.rsquared:.4f}")
print(f"RÂ² (Within): {model_fe.rsquared_within:.4f}")
print(f"RÂ² (Between): {model_fe.rsquared_between:.4f}")
print(f"RÂ² (Overall): {model_fe.rsquared_overall:.4f}")

# è¦³æ¸¬æ•°
print(f"\nè¦³æ¸¬æ•°: {model_fe.nobs}")
print(f"ä¼æ¥­æ•°: {model_fe.entity_info.total}")
print(f"æœŸé–“: {df_panel.index.get_level_values('year').nunique()}å¹´")
```

---

### 1.4 Random Effects (RE)

**ãƒ¢ãƒ‡ãƒ«**:
```
Y_it = Î²â‚€ + Î²â‚X_it + (u_i + Îµ_it)

u_i ~ N(0, ÏƒÂ²_u): ãƒ©ãƒ³ãƒ€ãƒ ãªä¼æ¥­åŠ¹æœ
```

**ç‰¹å¾´**:
- âœ… æ™‚é–“ä¸å¤‰ãªå¤‰æ•°ã®åŠ¹æœã‚’æ¨å®šå¯èƒ½
- âŒ ä¼æ¥­åŠ¹æœã¨XãŒç„¡ç›¸é–¢ã¨ã„ã†å¼·ã„ä»®å®š

**å®Ÿè£…**:

```python
from linearmodels.panel import RandomEffects

# Random Effects Model
model_re = RandomEffects.from_formula('''
roa ~ rd_intensity + firm_size + leverage + firm_age + 
      capital_intensity + TimeEffects
''', data=df_panel).fit(
    cov_type='clustered',
    cluster_entity=True
)

print(model_re)

# FEã¨ã®æ¯”è¼ƒ
print(f"\nFE vs. REæ¯”è¼ƒ:")
print(f"FE R&Dä¿‚æ•°: {model_fe.params['rd_intensity']:.4f}")
print(f"RE R&Dä¿‚æ•°: {model_re.params['rd_intensity']:.4f}")
```

---

### 1.5 Hausman Test (FE vs. RE)

**ç›®çš„**: FEã¨REã®ã©ã¡ã‚‰ã‚’ä½¿ã†ã¹ãã‹æ¤œå®š

**å¸°ç„¡ä»®èª¬**: REé©åˆ‡ï¼ˆä¼æ¥­åŠ¹æœã¨XãŒç„¡ç›¸é–¢ï¼‰  
**å¯¾ç«‹ä»®èª¬**: FEé©åˆ‡ï¼ˆç›¸é–¢ã‚ã‚Šï¼‰

```python
from linearmodels.panel import compare

# Hausman Test
comparison = compare({'FE': model_fe, 'RE': model_re})
print(comparison)

# æ‰‹å‹•è¨ˆç®—
hausman_stat = (model_fe.params - model_re.params).T @ \
               np.linalg.inv(model_fe.cov - model_re.cov) @ \
               (model_fe.params - model_re.params)

from scipy.stats import chi2
p_value = 1 - chi2.cdf(hausman_stat, df=len(model_fe.params))

print(f"\nHausmançµ±è¨ˆé‡: {hausman_stat:.2f}")
print(f"på€¤: {p_value:.4f}")

if p_value < 0.05:
    print("â†’ FEæ¨å¥¨ï¼ˆä¼æ¥­åŠ¹æœã¨Xã«ç›¸é–¢ã‚ã‚Šï¼‰")
else:
    print("â†’ REæ¨å¥¨ï¼ˆç„¡ç›¸é–¢ã®ä»®å®šOKï¼‰")
```

**å®Ÿå‹™**: ã»ã¼å¸¸ã«FEã‚’ä½¿ç”¨ï¼ˆãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æ¨™æº–ï¼‰

---

## 2. å†…ç”Ÿæ€§å¯¾ç­–

### 2.1 å†…ç”Ÿæ€§ã®ç¨®é¡

**1. Omitted Variable Biasï¼ˆæ¬ è½å¤‰æ•°ãƒã‚¤ã‚¢ã‚¹ï¼‰**
```
ä¾‹: çµŒå–¶è€…èƒ½åŠ›ï¼ˆè¦³æ¸¬ä¸å¯ï¼‰ãŒR&DæŠ•è³‡ã¨æ¥­ç¸¾ã®ä¸¡æ–¹ã«å½±éŸ¿
å¯¾ç­–: Fixed Effectsï¼ˆæ™‚é–“ä¸å¤‰ãªã‚‰ï¼‰
```

**2. Simultaneityï¼ˆåŒæ™‚æ€§ï¼‰**
```
ä¾‹: æ¥­ç¸¾è‰¯å¥½ â†’ R&DæŠ•è³‡å¢— AND R&DæŠ•è³‡ â†’ æ¥­ç¸¾å‘ä¸Š
å¯¾ç­–: Instrumental Variables (IV)
```

**3. Measurement Errorï¼ˆæ¸¬å®šèª¤å·®ï¼‰**
```
ä¾‹: R&Dæ”¯å‡ºã®å ±å‘Šèª¤å·®
å¯¾ç­–: IV, Multiple Indicators
```

---

### 2.2 Instrumental Variables (IV)

**åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢**: 
- å†…ç”Ÿå¤‰æ•°Xã¨ç›¸é–¢ã™ã‚‹ãŒã€èª¤å·®é …Îµã¨ç„¡ç›¸é–¢ãªå¤‰æ•°Zï¼ˆæ“ä½œå¤‰æ•°ï¼‰ã‚’ä½¿ç”¨

**æ¡ä»¶**:
1. **Relevance**: Cov(Z, X) â‰  0ï¼ˆå¼·ã„ç›¸é–¢ï¼‰
2. **Exogeneity**: Cov(Z, Îµ) = 0ï¼ˆèª¤å·®ã¨ç„¡ç›¸é–¢ï¼‰

**2SLS (Two-Stage Least Squares)**:

**Stage 1**: å†…ç”Ÿå¤‰æ•°ã‚’IVã§äºˆæ¸¬
```
X = Î³â‚€ + Î³â‚Z + Î³â‚‚Controls + u
```

**Stage 2**: äºˆæ¸¬å€¤XÌ‚ã‚’ä½¿ã£ã¦å›å¸°
```
Y = Î²â‚€ + Î²â‚XÌ‚ + Î²â‚‚Controls + Îµ
```

---

### 2.3 IVå®Ÿè£…ä¾‹

**ç ”ç©¶ä¾‹**: R&DæŠ•è³‡ã®åŠ¹æœï¼ˆR&Dã¯å†…ç”Ÿï¼‰

**IVå€™è£œ**: æ¥­ç•Œå¹³å‡R&Då¼·åº¦ï¼ˆä»–ç¤¾ã®R&Dã¯è‡ªç¤¾æ¥­ç¸¾ã«ç›´æ¥å½±éŸ¿ã—ãªã„ï¼‰

```python
from linearmodels.iv import IV2SLS
import pandas as pd

# æ¥­ç•Œå¹³å‡R&Dè¨ˆç®—ï¼ˆè‡ªç¤¾é™¤ãï¼‰
df['industry_avg_rd'] = df.groupby(['industry', 'year'])['rd_intensity'].transform(
    lambda x: (x.sum() - x) / (x.count() - 1)
)

# ãƒ‘ãƒãƒ«æ§‹é€ 
df_panel = df.set_index(['firm_id', 'year'])

# 2SLS with Fixed Effects
iv_model = IV2SLS.from_formula('''
roa ~ [rd_intensity ~ industry_avg_rd] + 
      firm_size + leverage + firm_age + capital_intensity +
      EntityEffects + TimeEffects
''', data=df_panel).fit(
    cov_type='clustered',
    cluster_entity=True
)

print(iv_model)

# IVå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
print(f"\nã€Stage 1ã€‘")
print(f"Fçµ±è¨ˆé‡: {iv_model.first_stage.diagnostics['f.stat'].iloc[0]:.2f}")
if iv_model.first_stage.diagnostics['f.stat'].iloc[0] > 10:
    print("âœ“ Weak IVå•é¡Œãªã—ï¼ˆF > 10ï¼‰")
else:
    print("âœ— Weak IVå•é¡Œã‚ã‚Šï¼ˆF < 10ï¼‰")

# Stage 1çµæœ
print(f"\nIV â†’ å†…ç”Ÿå¤‰æ•°ã®ä¿‚æ•°: {iv_model.first_stage.params.iloc[0]:.4f}")
print(f"på€¤: {iv_model.first_stage.pvalues.iloc[0]:.4f}")

# Stage 2çµæœ
print(f"\nã€Stage 2ã€‘")
print(f"R&DåŠ¹æœï¼ˆIVï¼‰: {iv_model.params['rd_intensity']:.4f}")
print(f"på€¤: {iv_model.pvalues['rd_intensity']:.4f}")
```

---

### 2.4 Propensity Score Matching (PSM)

**ç›®çš„**: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã‚’é¡ä¼¼ã—ãŸç‰¹æ€§ã§ãƒãƒƒãƒãƒ³ã‚°

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹**:
- M&Aç ”ç©¶ï¼ˆè²·åä¼æ¥­ vs. éè²·åä¼æ¥­ï¼‰
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æ¡ç”¨ï¼ˆæ¡ç”¨ä¼æ¥­ vs. éæ¡ç”¨ä¼æ¥­ï¼‰

**æ‰‹é †**:
1. Propensity Scoreæ¨å®šï¼ˆå‡¦ç½®ç¢ºç‡ï¼‰
2. ãƒãƒƒãƒãƒ³ã‚°ï¼ˆ1:1, 1:N, Kernelç­‰ï¼‰
3. ãƒãƒƒãƒå¾Œã®ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
4. å‡¦ç½®åŠ¹æœæ¨å®š

**å®Ÿè£…**:

```python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors
import numpy as np

# Step 1: Propensity Scoreæ¨å®š
X_covariates = df[['firm_size', 'leverage', 'firm_age', 'industry_dummy']]
treatment = df['adopted_innovation']  # 1=æ¡ç”¨, 0=éæ¡ç”¨

# Logistic Regression
lr = LogisticRegression()
lr.fit(X_covariates, treatment)

# Propensity Score
df['propensity_score'] = lr.predict_proba(X_covariates)[:, 1]

# Step 2: 1:1 Nearest Neighbor Matching
treated = df[df['adopted_innovation'] == 1]
control = df[df['adopted_innovation'] == 0]

# KNNãƒãƒƒãƒãƒ³ã‚°
nn = NearestNeighbors(n_neighbors=1, metric='euclidean')
nn.fit(control[['propensity_score']])

distances, indices = nn.kneighbors(treated[['propensity_score']])

# ãƒãƒƒãƒã•ã‚ŒãŸå¯¾ç…§ç¾¤
matched_control_indices = control.index[indices.flatten()]
matched_control = control.loc[matched_control_indices]

# ãƒãƒƒãƒå¾Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
df_matched = pd.concat([treated, matched_control])

print(f"å‡¦ç½®ç¾¤: {len(treated)}")
print(f"ãƒãƒƒãƒã•ã‚ŒãŸå¯¾ç…§ç¾¤: {len(matched_control)}")

# Step 3: ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
from scipy.stats import ttest_ind

print("\nã€ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ã€‘")
for var in ['firm_size', 'leverage', 'firm_age']:
    t_stat, p_val = ttest_ind(
        df_matched[df_matched['adopted_innovation']==1][var],
        df_matched[df_matched['adopted_innovation']==0][var]
    )
    print(f"{var}: t={t_stat:.2f}, p={p_val:.3f}")
    if p_val > 0.05:
        print(f"  âœ“ ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½")
    else:
        print(f"  âœ— ä¸å‡è¡¡ã‚ã‚Š")

# Step 4: å‡¦ç½®åŠ¹æœæ¨å®šï¼ˆATT: Average Treatment effect on the Treatedï¼‰
outcome_treated = df_matched[df_matched['adopted_innovation']==1]['roa'].mean()
outcome_control = df_matched[df_matched['adopted_innovation']==0]['roa'].mean()

att = outcome_treated - outcome_control

print(f"\nã€å‡¦ç½®åŠ¹æœã€‘")
print(f"å‡¦ç½®ç¾¤å¹³å‡ROA: {outcome_treated:.4f}")
print(f"å¯¾ç…§ç¾¤å¹³å‡ROA: {outcome_control:.4f}")
print(f"ATT: {att:.4f}")

# tæ¤œå®š
t_stat, p_val = ttest_ind(
    df_matched[df_matched['adopted_innovation']==1]['roa'],
    df_matched[df_matched['adopted_innovation']==0]['roa']
)
print(f"tçµ±è¨ˆé‡: {t_stat:.2f}, på€¤: {p_val:.3f}")
```

---

### 2.5 Heckman Selection Model

**ç›®çš„**: ã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒã‚¤ã‚¢ã‚¹ã®è£œæ­£

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹**:
- è¼¸å‡ºä¼æ¥­ã®ã¿ã‚’åˆ†æï¼ˆéè¼¸å‡ºä¼æ¥­ã®é¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰
- R&Då®Ÿæ–½ä¼æ¥­ã®ã¿ã‚’åˆ†æï¼ˆéå®Ÿæ–½ä¼æ¥­ã®é¸æŠãƒã‚¤ã‚¢ã‚¹ï¼‰

**2æ®µéš**:
1. **Selection Equation**: ã‚µãƒ³ãƒ—ãƒ«é¸æŠã‚’äºˆæ¸¬ï¼ˆProbitï¼‰
2. **Outcome Equation**: é¸æŠå¾Œã®çµæœã‚’åˆ†æï¼ˆOLS + IMRï¼‰

**å®Ÿè£…**:

```python
from statsmodels.regression.linear_model import OLS
from statsmodels.discrete.discrete_model import Probit
from scipy.stats import norm

# Step 1: Selection Equation (Probit)
# å¾“å±å¤‰æ•°: rd_dummy (R&Då®Ÿæ–½=1, éå®Ÿæ–½=0)

X_selection = df[['firm_size', 'leverage', 'industry_competition']]
X_selection = sm.add_constant(X_selection)

probit_model = Probit(df['rd_dummy'], X_selection).fit()

# Inverse Mills Ratio (IMR)è¨ˆç®—
df['z'] = probit_model.predict(X_selection)
df['imr'] = norm.pdf(df['z']) / norm.cdf(df['z'])

# Step 2: Outcome Equation (OLS + IMR)
df_selected = df[df['rd_dummy'] == 1]  # R&Då®Ÿæ–½ä¼æ¥­ã®ã¿

X_outcome = df_selected[['rd_intensity', 'firm_size', 'leverage', 'imr']]
X_outcome = sm.add_constant(X_outcome)

ols_model = OLS(df_selected['roa'], X_outcome).fit()

print(ols_model.summary())

# IMRã®æœ‰æ„æ€§ãƒã‚§ãƒƒã‚¯
print(f"\nIMRä¿‚æ•°: {ols_model.params['imr']:.4f}")
print(f"på€¤: {ols_model.pvalues['imr']:.4f}")

if ols_model.pvalues['imr'] < 0.05:
    print("âœ“ é¸æŠãƒã‚¤ã‚¢ã‚¹æœ‰æ„ï¼ˆHeckmanè£œæ­£å¿…è¦ï¼‰")
else:
    print("  é¸æŠãƒã‚¤ã‚¢ã‚¹éæœ‰æ„")
```

---

### 2.6 Difference-in-Differences (DiD)

**ç›®çš„**: ã‚¤ãƒ™ãƒ³ãƒˆãƒ»æ”¿ç­–å¤‰æ›´ã®å› æœåŠ¹æœæ¨å®š

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹**:
- æ–°è¦åˆ¶å°å…¥ã®åŠ¹æœ
- M&A announcementåŠ¹æœ
- çµ„ç¹”å¤‰é©ã®åŠ¹æœ

**åŸºæœ¬ãƒ¢ãƒ‡ãƒ«**:
```
Y_it = Î²â‚€ + Î²â‚Treated_i + Î²â‚‚Post_t + Î²â‚ƒ(Treated Ã— Post)_it + Îµ_it

Î²â‚ƒ: DiDæ¨å®šé‡ï¼ˆå‡¦ç½®åŠ¹æœï¼‰
```

**å®Ÿè£…**:

```python
import pandas as pd
import statsmodels.formula.api as smf

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
# treated: å‡¦ç½®ç¾¤=1, å¯¾ç…§ç¾¤=0
# post: ã‚¤ãƒ™ãƒ³ãƒˆå¾Œ=1, å‰=0

# DiDå›å¸°
did_model = smf.ols('''
roa ~ treated + post + treated:post + 
      firm_size + leverage + C(industry) + C(year)
''', data=df).fit(cov_type='cluster', cov_kwds={'groups': df['firm_id']})

print(did_model.summary())

# DiDåŠ¹æœ
did_effect = did_model.params['treated:post']
print(f"\nDiDåŠ¹æœ: {did_effect:.4f}")
print(f"på€¤: {did_model.pvalues['treated:post']:.4f}")

# å¹³è¡Œãƒˆãƒ¬ãƒ³ãƒ‰ä»®å®šãƒã‚§ãƒƒã‚¯ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆå‰ï¼‰
df_pre = df[df['post'] == 0]

# å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”è¼ƒ
treated_trend = df_pre[df_pre['treated']==1].groupby('year')['roa'].mean()
control_trend = df_pre[df_pre['treated']==0].groupby('year')['roa'].mean()

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(treated_trend.index, treated_trend.values, label='Treated', marker='o')
plt.plot(control_trend.index, control_trend.values, label='Control', marker='s')
plt.axvline(x=event_year, color='r', linestyle='--', label='Event')
plt.xlabel('Year')
plt.ylabel('ROA')
plt.title('Parallel Trends Check')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3. èª¿æ•´åŠ¹æœåˆ†æ

### 3.1 åŸºæœ¬æ¦‚å¿µ

**èª¿æ•´åŠ¹æœï¼ˆModerationï¼‰**: Xã®åŠ¹æœãŒZã«ã‚ˆã£ã¦å¤‰åŒ–

```
Y = Î²â‚€ + Î²â‚X + Î²â‚‚Z + Î²â‚ƒ(X Ã— Z) + Îµ

Î²â‚ƒ: èª¿æ•´åŠ¹æœ
```

**è§£é‡ˆ**:
- Î²â‚ƒ > 0: ZãŒXã®åŠ¹æœã‚’å¼·åŒ–
- Î²â‚ƒ < 0: ZãŒXã®åŠ¹æœã‚’å¼±åŒ–

---

### 3.2 å®Ÿè£…

```python
import statsmodels.formula.api as smf
import numpy as np

# å¤‰æ•°ã®æ¨™æº–åŒ–ï¼ˆäº¤äº’ä½œç”¨é …ã®å¤šé‡å…±ç·šæ€§è»½æ¸›ï¼‰
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df['rd_intensity_std'] = scaler.fit_transform(df[['rd_intensity']])
df['env_uncertainty_std'] = scaler.fit_transform(df[['env_uncertainty']])

# äº¤äº’ä½œç”¨é …
df['rd_x_uncertainty'] = df['rd_intensity_std'] * df['env_uncertainty_std']

# èª¿æ•´åŠ¹æœãƒ¢ãƒ‡ãƒ«
mod_model = smf.ols('''
roa ~ rd_intensity_std + env_uncertainty_std + rd_x_uncertainty +
      firm_size + leverage + firm_age + C(industry) + C(year)
''', data=df).fit(cov_type='cluster', cov_kwds={'groups': df['firm_id']})

print(mod_model.summary())

# èª¿æ•´åŠ¹æœã®è§£é‡ˆ
beta_x = mod_model.params['rd_intensity_std']
beta_xz = mod_model.params['rd_x_uncertainty']

print(f"\nã€èª¿æ•´åŠ¹æœã€‘")
print(f"R&Dä¸»åŠ¹æœï¼ˆÎ²â‚ï¼‰: {beta_x:.4f}")
print(f"äº¤äº’ä½œç”¨ï¼ˆÎ²â‚ƒï¼‰: {beta_xz:.4f}, p={mod_model.pvalues['rd_x_uncertainty']:.3f}")

if mod_model.pvalues['rd_x_uncertainty'] < 0.05:
    if beta_xz > 0:
        print("âœ“ ç’°å¢ƒä¸ç¢ºå®Ÿæ€§ãŒR&DåŠ¹æœã‚’å¼·åŒ–")
    else:
        print("âœ“ ç’°å¢ƒä¸ç¢ºå®Ÿæ€§ãŒR&DåŠ¹æœã‚’å¼±åŒ–")
```

---

### 3.3 Simple Slopeåˆ†æ

```python
import matplotlib.pyplot as plt

# ä¸ç¢ºå®Ÿæ€§ã®é«˜ä½ï¼ˆÂ±1SDï¼‰
uncertainty_low = df['env_uncertainty_std'].mean() - df['env_uncertainty_std'].std()
uncertainty_high = df['env_uncertainty_std'].mean() + df['env_uncertainty_std'].std()

# R&Dã®ç¯„å›²
rd_range = np.linspace(df['rd_intensity_std'].min(), 
                       df['rd_intensity_std'].max(), 100)

# Simple Slopes
slope_low = beta_x + beta_xz * uncertainty_low
slope_high = beta_x + beta_xz * uncertainty_high

roa_low = mod_model.params['Intercept'] + slope_low * rd_range
roa_high = mod_model.params['Intercept'] + slope_high * rd_range

# ãƒ—ãƒ­ãƒƒãƒˆ
plt.figure(figsize=(10, 6))
plt.plot(rd_range, roa_low, label=f'Low Uncertainty (-1SD)', linestyle='--')
plt.plot(rd_range, roa_high, label=f'High Uncertainty (+1SD)', linestyle='-')
plt.xlabel('R&D Intensity (Standardized)')
plt.ylabel('ROA')
plt.title('Moderation Effect: Environmental Uncertainty on R&Dâ†’ROA')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Simple Slopeæ¤œå®š
from scipy.stats import t as t_dist

se_x = mod_model.bse['rd_intensity_std']
se_xz = mod_model.bse['rd_x_uncertainty']
cov_x_xz = mod_model.cov_params().loc['rd_intensity_std', 'rd_x_uncertainty']

# Low Uncertaintyæ™‚ã®SE
se_low = np.sqrt(se_x**2 + (uncertainty_low**2) * se_xz**2 + 
                 2 * uncertainty_low * cov_x_xz)
t_low = slope_low / se_low
p_low = 2 * (1 - t_dist.cdf(abs(t_low), df=mod_model.df_resid))

# High Uncertaintyæ™‚ã®SE
se_high = np.sqrt(se_x**2 + (uncertainty_high**2) * se_xz**2 + 
                  2 * uncertainty_high * cov_x_xz)
t_high = slope_high / se_high
p_high = 2 * (1 - t_dist.cdf(abs(t_high), df=mod_model.df_resid))

print(f"\nã€Simple Slopeæ¤œå®šã€‘")
print(f"Low Uncertainty: Slope={slope_low:.4f}, t={t_low:.2f}, p={p_low:.3f}")
print(f"High Uncertainty: Slope={slope_high:.4f}, t={t_high:.2f}, p={p_high:.3f}")
```

---

## 4. åª’ä»‹åŠ¹æœåˆ†æ

### 4.1 åŸºæœ¬æ¦‚å¿µ

**åª’ä»‹åŠ¹æœï¼ˆMediationï¼‰**: XãŒMã‚’é€šã˜ã¦Yã«å½±éŸ¿

```
X â†’ M â†’ Y

ç›´æ¥åŠ¹æœ: X â†’ Y (c')
é–“æ¥åŠ¹æœ: X â†’ M â†’ Y (a Ã— b)
ç·åŠ¹æœ: c = c' + ab
```

**Baron & Kenny 4ã‚¹ãƒ†ãƒƒãƒ—**:
1. X â†’ Yï¼ˆç·åŠ¹æœ cï¼‰
2. X â†’ Mï¼ˆãƒ‘ã‚¹ aï¼‰
3. M â†’ Yï¼ˆãƒ‘ã‚¹ bï¼‰
4. X + M â†’ Yï¼ˆç›´æ¥åŠ¹æœ c'ï¼‰

---

### 4.2 å®Ÿè£…

```python
import statsmodels.api as sm
import pandas as pd

# Step 1: X â†’ Yï¼ˆç·åŠ¹æœï¼‰
X = sm.add_constant(df[['rd_intensity', 'firm_size', 'leverage']])
y = df['roa']

model_total = sm.OLS(y, X).fit()
c = model_total.params['rd_intensity']
print(f"Step 1ï¼ˆç·åŠ¹æœ cï¼‰: {c:.4f}, p={model_total.pvalues['rd_intensity']:.3f}")

# Step 2: X â†’ Mï¼ˆãƒ‘ã‚¹ aï¼‰
M = df['organizational_learning']  # åª’ä»‹å¤‰æ•°
model_a = sm.OLS(M, X).fit()
a = model_a.params['rd_intensity']
print(f"Step 2ï¼ˆãƒ‘ã‚¹ aï¼‰: {a:.4f}, p={model_a.pvalues['rd_intensity']:.3f}")

# Step 3: M â†’ Yï¼ˆãƒ‘ã‚¹ bï¼‰
X_with_M = sm.add_constant(df[['organizational_learning', 'firm_size', 'leverage']])
model_b = sm.OLS(y, X_with_M).fit()
b = model_b.params['organizational_learning']
print(f"Step 3ï¼ˆãƒ‘ã‚¹ bï¼‰: {b:.4f}, p={model_b.pvalues['organizational_learning']:.3f}")

# Step 4: X + M â†’ Yï¼ˆç›´æ¥åŠ¹æœ c'ï¼‰
X_full = sm.add_constant(df[['rd_intensity', 'organizational_learning', 
                              'firm_size', 'leverage']])
model_direct = sm.OLS(y, X_full).fit()
c_prime = model_direct.params['rd_intensity']
print(f"Step 4ï¼ˆç›´æ¥åŠ¹æœ c'ï¼‰: {c_prime:.4f}, p={model_direct.pvalues['rd_intensity']:.3f}")

# é–“æ¥åŠ¹æœ
indirect_effect = a * b
print(f"\né–“æ¥åŠ¹æœï¼ˆa Ã— bï¼‰: {indirect_effect:.4f}")
print(f"åª’ä»‹å‰²åˆ: {(indirect_effect / c) * 100:.1f}%")

# åª’ä»‹ã‚¿ã‚¤ãƒ—åˆ¤å®š
if model_direct.pvalues['rd_intensity'] > 0.05 and indirect_effect != 0:
    print("â†’ å®Œå…¨åª’ä»‹ï¼ˆFull Mediationï¼‰")
elif model_direct.pvalues['rd_intensity'] < 0.05 and indirect_effect != 0:
    print("â†’ éƒ¨åˆ†åª’ä»‹ï¼ˆPartial Mediationï¼‰")
```

---

### 4.3 Sobel Testï¼ˆé–“æ¥åŠ¹æœã®æœ‰æ„æ€§æ¤œå®šï¼‰

```python
import numpy as np
from scipy.stats import norm

# Sobel Test
se_a = model_a.bse['rd_intensity']
se_b = model_b.bse['organizational_learning']

# Sobelçµ±è¨ˆé‡
sobel_stat = indirect_effect / np.sqrt(b**2 * se_a**2 + a**2 * se_b**2)
p_sobel = 2 * (1 - norm.cdf(abs(sobel_stat)))

print(f"\nã€Sobel Testã€‘")
print(f"çµ±è¨ˆé‡: {sobel_stat:.2f}")
print(f"på€¤: {p_sobel:.4f}")

if p_sobel < 0.05:
    print("âœ“ é–“æ¥åŠ¹æœæœ‰æ„")
```

---

### 4.4 Bootstrapä¿¡é ¼åŒºé–“ï¼ˆæ¨å¥¨ï¼‰

```python
from scipy.stats import bootstrap

def indirect_effect_func(data):
    """Bootstrapç”¨ã®é–¢æ•°"""
    # X â†’ M
    X_boot = sm.add_constant(data[:, [0, 2, 3]])  # rd, size, lev
    M_boot = data[:, 1]  # org_learning
    model_a_boot = sm.OLS(M_boot, X_boot).fit()
    a_boot = model_a_boot.params[1]  # rdä¿‚æ•°
    
    # M â†’ Y
    X_M_boot = sm.add_constant(data[:, [1, 2, 3]])  # org_learning, size, lev
    y_boot = data[:, 4]  # roa
    model_b_boot = sm.OLS(y_boot, X_M_boot).fit()
    b_boot = model_b_boot.params[1]  # org_learningä¿‚æ•°
    
    return a_boot * b_boot

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
boot_data = df[['rd_intensity', 'organizational_learning', 
                'firm_size', 'leverage', 'roa']].dropna().values

# Bootstrap (n=5000)
np.random.seed(42)
n_bootstrap = 5000
indirect_effects = []

for _ in range(n_bootstrap):
    sample_indices = np.random.choice(len(boot_data), size=len(boot_data), replace=True)
    sample = boot_data[sample_indices]
    indirect_effects.append(indirect_effect_func(sample))

# 95%ä¿¡é ¼åŒºé–“
ci_lower = np.percentile(indirect_effects, 2.5)
ci_upper = np.percentile(indirect_effects, 97.5)

print(f"\nã€Bootstrapä¿¡é ¼åŒºé–“ã€‘")
print(f"é–“æ¥åŠ¹æœ: {indirect_effect:.4f}")
print(f"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")

if ci_lower > 0 or ci_upper < 0:
    print("âœ“ é–“æ¥åŠ¹æœæœ‰æ„ï¼ˆCIãŒ0ã‚’å«ã¾ãªã„ï¼‰")
```

---

## 5. å¤šéšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆMLMï¼‰

### 5.1 åŸºæœ¬æ¦‚å¿µ

**éšå±¤æ§‹é€ **: å€‹ä½“ï¼ˆLevel 1ï¼‰ãŒã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆLevel 2ï¼‰ã«nested

```
ä¾‹:
- å¾“æ¥­å“¡ï¼ˆL1ï¼‰ nested in ä¼æ¥­ï¼ˆL2ï¼‰
- ä¼æ¥­ï¼ˆL1ï¼‰ nested in æ¥­ç•Œï¼ˆL2ï¼‰
- ä¼æ¥­-å¹´ï¼ˆL1ï¼‰ nested in ä¼æ¥­ï¼ˆL2ï¼‰
```

**åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ï¼ˆRandom Interceptï¼‰**:
```
Level 1: Y_ij = Î²â‚€j + Î²â‚X_ij + Îµ_ij
Level 2: Î²â‚€j = Î³â‚€â‚€ + uâ‚€j

çµ±åˆå½¢:
Y_ij = Î³â‚€â‚€ + Î²â‚X_ij + uâ‚€j + Îµ_ij
```

---

### 5.2 å®Ÿè£…

```python
import statsmodels.formula.api as smf

# Random Intercept Model
# ä¼æ¥­ï¼ˆLevel 2ï¼‰nested in æ¥­ç•Œï¼ˆLevel 1ï¼‰

mlm_model = smf.mixedlm(
    formula='roa ~ rd_intensity + firm_size + leverage',
    data=df,
    groups=df['industry']  # Level 2ã‚°ãƒ«ãƒ¼ãƒ—
).fit()

print(mlm_model.summary())

# Random Effects
print(f"\nã€Random Effectsã€‘")
print(f"æ¥­ç•Œåˆ†æ•£ï¼ˆÏ„â‚€â‚€ï¼‰: {mlm_model.cov_re.iloc[0,0]:.4f}")
print(f"æ®‹å·®åˆ†æ•£ï¼ˆÏƒÂ²ï¼‰: {mlm_model.scale:.4f}")

# ICC (Intraclass Correlation)
icc = mlm_model.cov_re.iloc[0,0] / (mlm_model.cov_re.iloc[0,0] + mlm_model.scale)
print(f"ICC: {icc:.3f}")
print(f"  â†’ æ¥­ç•ŒãŒèª¬æ˜ã™ã‚‹åˆ†æ•£: {icc*100:.1f}%")
```

---

## 6. ç”Ÿå­˜åˆ†æ

### 6.1 Cox Hazard Model

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹**:
- ä¼æ¥­å­˜ç¶šåˆ†æ
- M&Aå¾Œã®çµ±åˆæœŸé–“
- ææºç¶™ç¶šæœŸé–“

**ãƒ¢ãƒ‡ãƒ«**:
```
h(t|X) = hâ‚€(t) Ã— exp(Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ...)

h(t|X): æ™‚ç‚¹tã§ã®ãƒã‚¶ãƒ¼ãƒ‰ç‡
hâ‚€(t): ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒã‚¶ãƒ¼ãƒ‰
```

---

### 6.2 å®Ÿè£…

```python
from lifelines import CoxPHFitter
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
# duration: å­˜ç¶šæœŸé–“ï¼ˆå¹´æ•°ï¼‰
# event: 1=å»ƒæ¥­, 0=æ‰“ã¡åˆ‡ã‚Šï¼ˆè¦³æ¸¬çµ‚äº†æ™‚ç‚¹ã§å­˜ç¶šï¼‰

df_survival = df[['duration', 'event', 'rd_intensity', 
                   'firm_size', 'leverage', 'firm_age']].dropna()

# Cox PH Model
cph = CoxPHFitter()
cph.fit(df_survival, duration_col='duration', event_col='event')

print(cph.summary)

# Hazard Ratioè§£é‡ˆ
print(f"\nã€Hazard Ratioã€‘")
for var in ['rd_intensity', 'firm_size']:
    hr = np.exp(cph.params_[var])
    print(f"{var}: HR={hr:.3f}")
    if hr > 1:
        print(f"  â†’ 1å˜ä½å¢—åŠ ã§å»ƒæ¥­ãƒªã‚¹ã‚¯{(hr-1)*100:.1f}%å¢—")
    else:
        print(f"  â†’ 1å˜ä½å¢—åŠ ã§å»ƒæ¥­ãƒªã‚¹ã‚¯{(1-hr)*100:.1f}%æ¸›")

# Proportional Hazardsä»®å®šãƒã‚§ãƒƒã‚¯
from lifelines.statistics import proportional_hazard_test

ph_test = proportional_hazard_test(cph, df_survival, time_transform='rank')
print(f"\nPHä»®å®šæ¤œå®š: p={ph_test.summary['p'].min():.3f}")
if ph_test.summary['p'].min() > 0.05:
    print("âœ“ PHä»®å®šOK")
```

---

## 7. Robustness Checks

### 7.1 å¿…é ˆãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½3ã¤ï¼‰

**1. ä»£æ›¿å¾“å±å¤‰æ•°**
```python
# ROA â†’ Tobin's Q
model_rob1 = PanelOLS.from_formula('''
tobin_q ~ rd_intensity + controls + EntityEffects + TimeEffects
''', data=df_panel).fit(cov_type='clustered', cluster_entity=True)
```

**2. ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«åˆ†æ**
```python
# ãƒã‚¤ãƒ†ã‚¯æ¥­ç•Œã®ã¿
df_hightech = df[df['industry'].isin([357, 367, 384])]
model_rob2 = PanelOLS.from_formula('''
roa ~ rd_intensity + controls + EntityEffects + TimeEffects
''', data=df_hightech.set_index(['firm_id', 'year'])).fit(cov_type='clustered', cluster_entity=True)
```

**3. ç•°ãªã‚‹ãƒ©ã‚°æ§‹é€ **
```python
# t+1 â†’ t+2
df['roa_lead2'] = df.groupby('firm_id')['roa'].shift(-2)
model_rob3 = PanelOLS.from_formula('''
roa_lead2 ~ rd_intensity + controls + EntityEffects + TimeEffects
''', data=df_panel).fit(cov_type='clustered', cluster_entity=True)
```

---

### 7.2 Robustnessçµæœã®å ±å‘Š

```python
# çµæœã‚’è¡¨å½¢å¼ã§ã¾ã¨ã‚
robustness_results = pd.DataFrame({
    'Model': ['Main', 'Alt DV (Tobin Q)', 'Hightech Only', 'Lag t+2'],
    'Coefficient': [
        model_main.params['rd_intensity'],
        model_rob1.params['rd_intensity'],
        model_rob2.params['rd_intensity'],
        model_rob3.params['rd_intensity']
    ],
    'Std Error': [
        model_main.std_errors['rd_intensity'],
        model_rob1.std_errors['rd_intensity'],
        model_rob2.std_errors['rd_intensity'],
        model_rob3.std_errors['rd_intensity']
    ],
    'P-value': [
        model_main.pvalues['rd_intensity'],
        model_rob1.pvalues['rd_intensity'],
        model_rob2.pvalues['rd_intensity'],
        model_rob3.pvalues['rd_intensity']
    ],
    'N': [
        model_main.nobs,
        model_rob1.nobs,
        model_rob2.nobs,
        model_rob3.nobs
    ]
})

print("\nã€Robustness Checksã€‘")
print(robustness_results.to_string(index=False))

# ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§æœ‰æ„ã‹ç¢ºèª
all_significant = (robustness_results['P-value'] < 0.05).all()
if all_significant:
    print("\nâœ“ ã™ã¹ã¦ã®Robustness checksã§çµæœé ‘å¥")
```

---

## 8. è¨ºæ–­ãƒ†ã‚¹ãƒˆ

### 8.1 å¤šé‡å…±ç·šæ€§ï¼ˆVIFï¼‰

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

# VIFè¨ˆç®—
X = df[['rd_intensity', 'firm_size', 'leverage', 'firm_age', 'capital_intensity']]
X = X.dropna()

vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]

print("ã€å¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯ã€‘")
print(vif_data)

# åˆ¤å®š
max_vif = vif_data["VIF"].max()
if max_vif > 10:
    print(f"\nâœ— è­¦å‘Š: æœ€å¤§VIF={max_vif:.1f} > 10ï¼ˆå¤šé‡å…±ç·šæ€§ã‚ã‚Šï¼‰")
elif max_vif > 5:
    print(f"\nâš  æ³¨æ„: æœ€å¤§VIF={max_vif:.1f} > 5ï¼ˆã‚„ã‚„é«˜ã„ï¼‰")
else:
    print(f"\nâœ“ å¤šé‡å…±ç·šæ€§å•é¡Œãªã—ï¼ˆæœ€å¤§VIF={max_vif:.1f}ï¼‰")
```

---

### 8.2 ç•°åˆ†æ•£æ€§

```python
from statsmodels.stats.diagnostic import het_white

# OLSãƒ¢ãƒ‡ãƒ«ã§å®Ÿæ–½
model = smf.ols('roa ~ rd_intensity + firm_size + leverage', data=df).fit()

# White test
lm_stat, lm_pval, f_stat, f_pval = het_white(model.resid, model.model.exog)

print(f"ã€ç•°åˆ†æ•£æ€§ãƒã‚§ãƒƒã‚¯ã€‘")
print(f"White test: LM statistic={lm_stat:.2f}, p={lm_pval:.4f}")

if lm_pval < 0.05:
    print("âœ— ç•°åˆ†æ•£æ€§ã‚ã‚Š â†’ Robustæ¨™æº–èª¤å·®ä½¿ç”¨æ¨å¥¨")
else:
    print("âœ“ ç­‰åˆ†æ•£æ€§OK")
```

---

### 8.3 è‡ªå·±ç›¸é–¢

```python
from statsmodels.stats.stattools import durbin_watson

# Durbin-Watson test
dw_stat = durbin_watson(model.resid)

print(f"ã€è‡ªå·±ç›¸é–¢ãƒã‚§ãƒƒã‚¯ã€‘")
print(f"Durbin-Watsonçµ±è¨ˆé‡: {dw_stat:.2f}")

if 1.5 < dw_stat < 2.5:
    print("âœ“ è‡ªå·±ç›¸é–¢å•é¡Œãªã—")
else:
    print("âš  è‡ªå·±ç›¸é–¢ã®å¯èƒ½æ€§ â†’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨™æº–èª¤å·®æ¨å¥¨")
```

---

## ğŸ“Š Quick Reference

### æ‰‹æ³•é¸æŠãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```
ãƒ‡ãƒ¼ã‚¿æ§‹é€ :
â”œâ”€ ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ï¼Ÿ
â”‚  Yes â†’ Fixed Effectsï¼ˆæ¨å¥¨ï¼‰
â”‚  No â†’ OLS
â”‚
â”œâ”€ å†…ç”Ÿæ€§ã®æ‡¸å¿µï¼Ÿ
â”‚  Yes â†’ IV, PSM, Heckman, DiD
â”‚  No â†’ FE/OLS
â”‚
â”œâ”€ èª¿æ•´åŠ¹æœï¼Ÿ
â”‚  Yes â†’ äº¤äº’ä½œç”¨é … + Simple Slope
â”‚
â”œâ”€ åª’ä»‹åŠ¹æœï¼Ÿ
â”‚  Yes â†’ Baron & Kenny + Bootstrap CI
â”‚
â””â”€ éšå±¤æ§‹é€ ï¼Ÿ
   Yes â†’ Multilevel Model
```

---

### ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åŸºæº–

| ã‚¸ãƒ£ãƒ¼ãƒŠãƒ« | å¿…é ˆæ‰‹æ³• | Robustness | å†…ç”Ÿæ€§å¯¾ç­– |
|-----------|---------|-----------|-----------|
| SMJ | FE, Cluster SE | 3ã¤ä»¥ä¸Š | å¿…é ˆ |
| AMJ | FE, Poweråˆ†æ | 3ã¤ä»¥ä¸Š | å¿…é ˆ |
| OS | FE, æ–°è¦æ‰‹æ³• | 3ã¤ä»¥ä¸Š | å¿…é ˆ |

---

### Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```bash
# å¿…é ˆ
pip install pandas numpy scipy statsmodels linearmodels

# ç”Ÿå­˜åˆ†æ
pip install lifelines

# æ©Ÿæ¢°å­¦ç¿’ï¼ˆPSMç”¨ï¼‰
pip install scikit-learn

# å¯è¦–åŒ–
pip install matplotlib seaborn
```

---

## å‚è€ƒæ–‡çŒ®

### æ–¹æ³•è«–

- Wooldridge, J. M. (2010). *Econometric Analysis of Cross Section and Panel Data*. MIT Press.
- Angrist, J. D., & Pischke, J. S. (2009). *Mostly Harmless Econometrics*. Princeton University Press.
- Hayes, A. F. (2017). *Introduction to Mediation, Moderation, and Conditional Process Analysis*. Guilford Press.

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
â†’ [`1-core-workflow` skill](../1-core-workflow/SKILL.md) Phase 7

### é«˜åº¦å› æœæ¨è«–
â†’ [`6-causal-ml` skill](../6-causal-ml/SKILL.md)

### ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ•°ã®çµ±åˆ
â†’ [`4-text-analysis` skill](../4-text-analysis/SKILL.md)

---

**ã“ã®ã‚¹ã‚­ãƒ«ã§ã€ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æ°´æº–ã®çµ±è¨ˆåˆ†æã‚’å®Ÿè£…ã§ãã¾ã™ã€‚**  
**å†…ç”Ÿæ€§å¯¾ç­–ã‹ã‚‰Robustness checksã¾ã§ã€å®Œå…¨ãªåˆ†æãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã—ã‚‡ã†ã€‚**

---

**æœ€çµ‚æ›´æ–°**: 2025-11-01  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 4.0.0  
**æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹äºˆå®š**: 2025-12-01
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
