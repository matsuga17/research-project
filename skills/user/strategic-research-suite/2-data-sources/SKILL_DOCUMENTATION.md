---
name: strategic-research-data-sources
description: Comprehensive data source catalog for strategic management research covering North America (Compustat, CRSP, WRDS), Europe (Orbis, Worldscope), Asia-11 countries (Japan-EDINET, Korea-DART, China-CNINFO, ASEAN), and global free sources (World Bank, IMF, OECD, SEC EDGAR) with API implementation examples.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 2 (Data Source Discovery)
  - statistical-methods: Data analysis integration
  - text-analysis: Text data sources (SEC EDGAR, earnings calls)
  - esg-sustainability: ESG data sources
  - automation: Automated data collection
---

# Data Sources Catalog v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥çµŒå–¶ãƒ»çµ„ç¹”è«–ç ”ç©¶ã§ä½¿ç”¨ã™ã‚‹**ä¸–ç•Œä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**ã‚’ç¶²ç¾…çš„ã«ã‚«ã‚¿ãƒ­ã‚°åŒ–ã—ã€ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ãƒ»APIå®Ÿè£…ä¾‹ã‚’æä¾›ã—ã¾ã™ã€‚

### ã‚«ãƒãƒ¬ãƒƒã‚¸

```
åœ°åŸŸåˆ¥ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
â”œâ”€ åŒ—ç±³ï¼ˆç±³å›½ãƒ»ã‚«ãƒŠãƒ€ï¼‰: 5å¤§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”œâ”€ æ¬§å·: 3å¤§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”œâ”€ ã‚¢ã‚¸ã‚¢11ã‚«å›½:
â”‚  â”œâ”€ æ—¥æœ¬ï¼ˆ4ã‚½ãƒ¼ã‚¹ï¼‰
â”‚  â”œâ”€ éŸ“å›½ï¼ˆ2ã‚½ãƒ¼ã‚¹ï¼‰
â”‚  â”œâ”€ ä¸­å›½ï¼ˆ3ã‚½ãƒ¼ã‚¹ï¼‰
â”‚  â”œâ”€ ASEAN 6ã‚«å›½
â”‚  â””â”€ ã‚¤ãƒ³ãƒ‰ï¼ˆ2ã‚½ãƒ¼ã‚¹ï¼‰
â””â”€ ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™: 10+ã‚½ãƒ¼ã‚¹
```

**åˆè¨ˆ**: 40ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

---

### ã„ã¤ä½¿ã†ã‹

âœ… **Phase 2: Data Source Discovery**
- RQã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æ¢ã™
- å„ã‚½ãƒ¼ã‚¹ã®ç‰¹å¾´ãƒ»è²»ç”¨ã‚’æ¯”è¼ƒ
- ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•ã‚’ç¢ºèª

âœ… **Phase 3: Data Collection**
- APIå®Ÿè£…ä¾‹ã‚’å‚ç…§
- ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ

âœ… **ç ”ç©¶è¨ˆç”»æ®µéš**
- ã‚µãƒ³ãƒ—ãƒ«ã®å®Ÿç¾å¯èƒ½æ€§ç¢ºèª
- ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ€§ã®äº‹å‰èª¿æŸ»

---

### å‰ææ¡ä»¶

**å¿…é ˆã‚¹ã‚­ãƒ«**:
- PythonåŸºç¤ï¼ˆrequests, pandasï¼‰
- APIåŸºç¤ï¼ˆREST API, èªè¨¼ï¼‰

**æ¨å¥¨ã‚¹ã‚­ãƒ«**:
- SQLåŸºç¤ï¼ˆWRDSç­‰ã§å¿…è¦ï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµŒé¨“

**æŠ€è¡“ç’°å¢ƒ**:
- Python 3.8ä»¥ä¸Š
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š
- APIèªè¨¼æƒ…å ±ï¼ˆã‚½ãƒ¼ã‚¹ã«ã‚ˆã‚‹ï¼‰

---

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

| ç”¨é€” | é€£æºã‚¹ã‚­ãƒ« | ç›®çš„ |
|------|-----------|------|
| ãƒ‡ãƒ¼ã‚¿åé›†è¨ˆç”» | `1-core-workflow` Phase 2-3 | ã‚½ãƒ¼ã‚¹é¸æŠãƒ»åé›†æˆ¦ç•¥ |
| ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ | `4-text-analysis` | SEC EDGAR, æ±ºç®—èª¬æ˜ä¼š |
| ESGãƒ‡ãƒ¼ã‚¿ | `7-esg-sustainability` | ESGå°‚é–€ã‚½ãƒ¼ã‚¹ |
| è‡ªå‹•åé›† | `8-automation` | å¤§è¦æ¨¡è‡ªå‹•åé›† |

---

## ğŸ“‹ ç›®æ¬¡

### åœ°åŸŸåˆ¥ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
1. [åŒ—ç±³ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#1-åŒ—ç±³ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
2. [æ¬§å·ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#2-æ¬§å·ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
3. [æ—¥æœ¬ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#3-æ—¥æœ¬ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
4. [éŸ“å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#4-éŸ“å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
5. [ä¸­å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹](#5-ä¸­å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹)
6. [ASEANè«¸å›½](#6-aseanè«¸å›½)
7. [ã‚¤ãƒ³ãƒ‰](#7-ã‚¤ãƒ³ãƒ‰)
8. [ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹](#8-ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹)

### å®Ÿç”¨ã‚¬ã‚¤ãƒ‰
9. [ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠãƒãƒˆãƒªãƒƒã‚¯ã‚¹](#9-ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠãƒãƒˆãƒªãƒƒã‚¯ã‚¹)
10. [APIå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³](#10-apiå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³)
11. [ãƒ‡ãƒ¼ã‚¿å“è³ªæ¯”è¼ƒ](#11-ãƒ‡ãƒ¼ã‚¿å“è³ªæ¯”è¼ƒ)
12. [ã‚³ã‚¹ãƒˆæ¯”è¼ƒ](#12-ã‚³ã‚¹ãƒˆæ¯”è¼ƒ)

---

## 1. åŒ—ç±³ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 1.1 Compustat (Standard & Poor's)

**æä¾›å…ƒ**: S&P Global Market Intelligence  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ç±³å›½ãƒ»ã‚«ãƒŠãƒ€ä¸Šå ´ä¼æ¥­ï¼ˆ1950å¹´ä»£ã€œç¾åœ¨ï¼‰  
**ä¼æ¥­æ•°**: 25,000ç¤¾ä»¥ä¸Šï¼ˆç±³å›½ï¼‰ã€3,000ç¤¾ä»¥ä¸Šï¼ˆã‚«ãƒŠãƒ€ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**è²¡å‹™ãƒ‡ãƒ¼ã‚¿**:
- æç›Šè¨ˆç®—æ›¸ï¼ˆIncome Statementï¼‰
- è²¸å€Ÿå¯¾ç…§è¡¨ï¼ˆBalance Sheetï¼‰
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸ï¼ˆCash Flowï¼‰
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ï¼ˆSegment Dataï¼‰

**å¤‰æ•°ä¾‹**:
```python
key_variables = {
    'at': 'ç·è³‡ç”£ (Total Assets)',
    'sale': 'å£²ä¸Šé«˜ (Sales)',
    'ni': 'ç´”åˆ©ç›Š (Net Income)',
    'xrd': 'R&Dæ”¯å‡º (R&D Expense)',
    'capx': 'è¨­å‚™æŠ•è³‡ (Capital Expenditure)',
    'dltt': 'é•·æœŸè² å‚µ (Long-term Debt)',
    'sich': 'æ¥­ç•Œã‚³ãƒ¼ãƒ‰ (SIC Code)',
    'fyear': 'ä¼šè¨ˆå¹´åº¦ (Fiscal Year)'
}
```

#### ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

**WRDSçµŒç”±ï¼ˆæ¨å¥¨ï¼‰**:
```python
import wrds

# WRDSæ¥ç¶š
db = wrds.Connection(wrds_username='your_username')

# Compustat North America
query = """
SELECT 
    gvkey AS firm_id,
    fyear AS year,
    conm AS company_name,
    sich AS industry_code,
    at AS total_assets,
    sale AS sales,
    ni AS net_income,
    xrd AS rd_expense,
    capx AS capex,
    dltt AS long_term_debt,
    dlc AS short_term_debt,
    che AS cash
FROM 
    comp.funda
WHERE 
    fyear BETWEEN 2010 AND 2023
    AND indfmt = 'INDL'    -- Industrial format
    AND datafmt = 'STD'    -- Standardized data
    AND popsrc = 'D'       -- Domestic companies
    AND consol = 'C'       -- Consolidated
    AND sich BETWEEN 2000 AND 3999  -- Manufacturing
ORDER BY 
    gvkey, fyear
"""

df_compustat = db.raw_sql(query)

print(f"å–å¾—ä¼æ¥­æ•°: {df_compustat['firm_id'].nunique():,}ç¤¾")
print(f"è¦³æ¸¬æ•°: {len(df_compustat):,}")

# ä¿å­˜
df_compustat.to_csv('data/compustat_manufacturing.csv', index=False)

db.close()
```

#### ãƒ‡ãƒ¼ã‚¿å“è³ª

**é•·æ‰€**:
- âœ… é•·æœŸæ™‚ç³»åˆ—ï¼ˆ70å¹´ä»¥ä¸Šï¼‰
- âœ… æ¨™æº–åŒ–ã•ã‚ŒãŸå¤‰æ•°
- âœ… é«˜ã„ä¿¡é ¼æ€§
- âœ… å­¦è¡“ç ”ç©¶ã§æ¨™æº–

**çŸ­æ‰€**:
- âŒ æœ‰æ–™ï¼ˆå¤§å­¦å¥‘ç´„ãŒå¿…è¦ï¼‰
- âŒ å°è¦æ¨¡ä¼æ¥­ã®ã‚«ãƒãƒ¬ãƒƒã‚¸é™å®šçš„
- âŒ éä¸Šå ´ä¼æ¥­ãªã—

**ä½¿ç”¨ä¾‹ï¼ˆè«–æ–‡ï¼‰**:
- SMJ: 80%ä»¥ä¸ŠãŒCompustatä½¿ç”¨
- AMJ: 70%ä»¥ä¸Š
- OS: 60%ä»¥ä¸Š

---

### 1.2 CRSP (Center for Research in Security Prices)

**æä¾›å…ƒ**: University of Chicago Booth School of Business  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ç±³å›½æ ªå¼å¸‚å ´ï¼ˆNYSE, NASDAQ, AMEXï¼‰ï¼ˆ1926å¹´ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**æ ªä¾¡ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿**:
- æ—¥æ¬¡æ ªä¾¡ï¼ˆDaily Stock Pricesï¼‰
- æœˆæ¬¡ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆMonthly Returnsï¼‰
- æ ªå¼åˆ†å‰²èª¿æ•´å¾Œä¾¡æ ¼ï¼ˆSplit-adjusted Pricesï¼‰
- é…å½“åˆ©å›ã‚Šï¼ˆDividend Yieldï¼‰
- å¸‚å ´æ™‚ä¾¡ç·é¡ï¼ˆMarket Capitalizationï¼‰

#### å®Ÿè£…ä¾‹

```python
import wrds

db = wrds.Connection(wrds_username='your_username')

# æœˆæ¬¡æ ªä¾¡ãƒªã‚¿ãƒ¼ãƒ³å–å¾—
query = """
SELECT 
    a.permno AS stock_id,
    a.date,
    b.ticker,
    b.comnam AS company_name,
    a.ret AS monthly_return,
    a.prc AS price,
    a.shrout AS shares_outstanding,
    a.vol AS volume
FROM 
    crsp.msf AS a
LEFT JOIN 
    crsp.msenames AS b
ON 
    a.permno = b.permno 
    AND b.namedt <= a.date 
    AND a.date <= b.nameendt
WHERE 
    a.date BETWEEN '2010-01-01' AND '2023-12-31'
    AND b.exchcd IN (1, 2, 3)  -- NYSE, AMEX, NASDAQ
ORDER BY 
    a.permno, a.date
"""

df_crsp = db.raw_sql(query)

# å¸‚å ´æ™‚ä¾¡ç·é¡è¨ˆç®—
df_crsp['market_cap'] = df_crsp['price'].abs() * df_crsp['shares_outstanding']

print(f"å–å¾—éŠ˜æŸ„æ•°: {df_crsp['stock_id'].nunique():,}")

db.close()
```

#### Compustatã¨ã®ãƒªãƒ³ã‚¯

**CRSP-Compustat Merged Database (CCM)**:
```python
# CCM Link Table
query = """
SELECT 
    lpermno AS permno,
    gvkey,
    linkdt AS link_start_date,
    linkenddt AS link_end_date
FROM 
    crsp.ccmxpf_linktable
WHERE 
    linktype IN ('LU', 'LC')
    AND linkprim IN ('P', 'C')
"""

df_link = db.raw_sql(query)

# Merge Compustat & CRSP
df_merged = df_compustat.merge(
    df_link, 
    on='gvkey', 
    how='left'
).merge(
    df_crsp,
    on='permno',
    how='left'
)
```

---

### 1.3 ExecuComp (å½¹å“¡å ±é…¬ãƒ‡ãƒ¼ã‚¿)

**æä¾›å…ƒ**: S&P Global  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: S&P 1500ä¼æ¥­ã®ãƒˆãƒƒãƒ—5å½¹å“¡ï¼ˆ1992å¹´ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**å ±é…¬ãƒ‡ãƒ¼ã‚¿**:
- åŸºæœ¬çµ¦ï¼ˆSalaryï¼‰
- ãƒœãƒ¼ãƒŠã‚¹ï¼ˆBonusï¼‰
- ã‚¹ãƒˆãƒƒã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆStock Optionsï¼‰
- åˆ¶é™ä»˜æ ªå¼ï¼ˆRestricted Stockï¼‰
- ç·å ±é…¬ï¼ˆTotal Compensationï¼‰

#### å®Ÿè£…ä¾‹

```python
import wrds

db = wrds.Connection(wrds_username='your_username')

# CEOå ±é…¬ãƒ‡ãƒ¼ã‚¿å–å¾—
query = """
SELECT 
    gvkey,
    year,
    exec_fullname AS ceo_name,
    salary,
    bonus,
    option_awards_blk_value AS stock_options,
    stock_awards_fv AS restricted_stock,
    tdc1 AS total_compensation,
    ceoann AS is_ceo
FROM 
    comp.execcomp
WHERE 
    year BETWEEN 2010 AND 2023
    AND ceoann = 'CEO'  -- CEOã®ã¿
ORDER BY 
    gvkey, year
"""

df_execcomp = db.raw_sql(query)

# CEOå ±é…¬ã®è¨˜è¿°çµ±è¨ˆ
print("CEOå ±é…¬ã®è¨˜è¿°çµ±è¨ˆï¼ˆUSDï¼‰:")
print(df_execcomp[['salary', 'total_compensation']].describe())

db.close()
```

#### ç ”ç©¶ç”¨é€”

**Agency Theoryç ”ç©¶**:
- CEOå ±é…¬ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚
- Pay-performance sensitivity
- Equity-based compensationåŠ¹æœ

---

### 1.4 Thomson Reuters SDC (M&Aãƒ»IPOãƒ‡ãƒ¼ã‚¿)

**æä¾›å…ƒ**: Refinitiv (Thomson Reuters)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ã‚°ãƒ­ãƒ¼ãƒãƒ«M&Aã€IPOã€Joint Ventureï¼ˆ1970å¹´ä»£ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**M&Aãƒ‡ãƒ¼ã‚¿**:
- å–å¼•é‡‘é¡ï¼ˆDeal Valueï¼‰
- è²·åè€…ãƒ»å¯¾è±¡ä¼æ¥­ï¼ˆAcquirer & Targetï¼‰
- å–å¼•å½¢æ…‹ï¼ˆDeal Typeï¼‰
- æ”¯æ‰•æ–¹æ³•ï¼ˆPayment Methodï¼‰
- ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ï¼ˆAdvisorsï¼‰

#### ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆWRDSçµŒç”±ï¼‰

```python
import wrds

db = wrds.Connection(wrds_username='your_username')

# M&Aå–å¼•ãƒ‡ãƒ¼ã‚¿
query = """
SELECT 
    dealnum AS deal_id,
    da AS announcement_date,
    dt AS completion_date,
    an AS acquirer_name,
    tn AS target_name,
    ams AS acquirer_macro_industry,
    tms AS target_macro_industry,
    dv AS deal_value,
    pctacq AS percent_acquired,
    datype AS deal_attitude,
    paymeth AS payment_method
FROM 
    sdc.ma
WHERE 
    da BETWEEN '2010-01-01' AND '2023-12-31'
    AND ams = 'United States'  -- ç±³å›½è²·åè€…
    AND dv IS NOT NULL  -- å–å¼•é‡‘é¡ã‚ã‚Š
ORDER BY 
    da
"""

df_ma = db.raw_sql(query)

print(f"M&Aå–å¼•æ•°: {len(df_ma):,}ä»¶")
print(f"ç·å–å¼•é¡: ${df_ma['deal_value'].sum()/1000:.1f}B")

db.close()
```

#### ç ”ç©¶ç”¨é€”

**M&Aæˆ¦ç•¥ç ”ç©¶**:
- è²·åãƒ—ãƒ¬ãƒŸã‚¢ãƒ æ±ºå®šè¦å› 
- M&A announcementåŠ¹æœ
- Post-merger performance

---

### 1.5 BoardEx (å–ç· å½¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿)

**æä¾›å…ƒ**: BoardEx (ISS)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¼æ¥­ã®å½¹å“¡ãƒ»å–ç· å½¹ï¼ˆ2000å¹´ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**å–ç· å½¹ãƒ‡ãƒ¼ã‚¿**:
- å–ç· å½¹å€‹äººIDï¼ˆDirector IDï¼‰
- æ°åãƒ»çµŒæ­´ï¼ˆName & Backgroundï¼‰
- ç¾è·ãƒ»éå»è·ï¼ˆCurrent & Past Positionsï¼‰
- å­¦æ­´ï¼ˆEducationï¼‰
- ä»–ç¤¾å–ç· å½¹å…¼ä»»ï¼ˆBoard Interlocksï¼‰

#### ã‚¢ã‚¯ã‚»ã‚¹

**WRDSçµŒç”±**:
```python
import wrds

db = wrds.Connection(wrds_username='your_username')

# å–ç· å½¹ãƒ‡ãƒ¼ã‚¿å–å¾—
query = """
SELECT 
    companyid AS company_id,
    companyname AS company_name,
    directorid AS director_id,
    directorname AS director_name,
    datestartrole AS start_date,
    dateendrole AS end_date,
    rolename AS position,
    seniority
FROM 
    boardex.na_wrds_company_profile
WHERE 
    datestartrole BETWEEN '2010-01-01' AND '2023-12-31'
ORDER BY 
    companyid, datestartrole
"""

df_board = db.raw_sql(query)

print(f"ä¼æ¥­æ•°: {df_board['company_id'].nunique():,}")
print(f"å–ç· å½¹æ•°: {df_board['director_id'].nunique():,}")

db.close()
```

#### Board Interlock Networkæ§‹ç¯‰

```python
import pandas as pd
import networkx as nx

# åŒã˜å–ç· å½¹ãŒè¤‡æ•°ä¼æ¥­ã«æ‰€å± = Board Interlock
director_companies = df_board.groupby('director_id')['company_id'].apply(list)

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
G = nx.Graph()

for director, companies in director_companies.items():
    if len(companies) > 1:
        # åŒã˜å–ç· å½¹ã‚’å…±æœ‰ã™ã‚‹ä¼æ¥­é–“ã«ã‚¨ãƒƒã‚¸
        for i in range(len(companies)):
            for j in range(i+1, len(companies)):
                G.add_edge(companies[i], companies[j])

print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒãƒ¼ãƒ‰æ•°ï¼ˆä¼æ¥­ï¼‰: {G.number_of_nodes()}")
print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒƒã‚¸æ•°ï¼ˆInterlockï¼‰: {G.number_of_edges()}")

# Centralityè¨ˆç®—
degree_centrality = nx.degree_centrality(G)
betweenness = nx.betweenness_centrality(G)
```

è©³ç´°: [`5-network-analysis` skill](../5-network-analysis/SKILL.md)

---

## 2. æ¬§å·ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 2.1 Orbis (Bureau van Dijk)

**æä¾›å…ƒ**: Moody's Analytics (Bureau van Dijk)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸–ç•Œ400ç™¾ä¸‡ç¤¾ä»¥ä¸Šï¼ˆæ¬§å·ãŒæœ€ã‚‚å……å®Ÿï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**è²¡å‹™ãƒ‡ãƒ¼ã‚¿**:
- æç›Šè¨ˆç®—æ›¸
- è²¸å€Ÿå¯¾ç…§è¡¨
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼
- è²¡å‹™æ¯”ç‡

**éè²¡å‹™ãƒ‡ãƒ¼ã‚¿**:
- ä¼æ¥­æ¦‚è¦ãƒ»ä½æ‰€
- æ¥­ç•Œåˆ†é¡ï¼ˆNACE, SICï¼‰
- æ‰€æœ‰æ¨©æ§‹é€ 
- å­ä¼šç¤¾æƒ…å ±

#### ç‰¹å¾´

**é•·æ‰€**:
- âœ… æ¬§å·ä¼æ¥­ã®æœ€ã‚‚åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿
- âœ… éä¸Šå ´ä¼æ¥­ã‚‚å«ã‚€
- âœ… æ‰€æœ‰æ¨©ãƒ‡ãƒ¼ã‚¿ãŒå……å®Ÿ

**çŸ­æ‰€**:
- âŒ é«˜é¡ï¼ˆä¼æ¥­ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¿…è¦ï¼‰
- âŒ ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–ãŒCompustatã‚ˆã‚ŠåŠ£ã‚‹
- âŒ æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ãŒé™å®šçš„

#### ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

**Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**ï¼ˆAPIãªã—ï¼‰:
1. Orbisãƒãƒ¼ã‚¿ãƒ«ã«ãƒ­ã‚°ã‚¤ãƒ³
2. æ¤œç´¢æ¡ä»¶è¨­å®šï¼ˆå›½ã€æ¥­ç•Œã€æœŸé–“ç­‰ï¼‰
3. CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

**æ¨å¥¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**:
```python
# 1. Orbisã‹ã‚‰æ‰‹å‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆCSVï¼‰
# 2. Pythonã§èª­ã¿è¾¼ã¿ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

import pandas as pd

df_orbis = pd.read_csv('orbis_export.csv', encoding='latin-1')

# å¤‰æ•°åæ¨™æº–åŒ–
df_orbis = df_orbis.rename(columns={
    'Operating revenue\nEUR': 'sales',
    'Total assets\nEUR': 'total_assets',
    'P/L before tax\nEUR': 'pretax_income'
})

# é€šè²¨æ›ç®—ï¼ˆEURã‹ã‚‰USDï¼‰
eur_usd_rate = 1.10  # é©åˆ‡ãªãƒ¬ãƒ¼ãƒˆä½¿ç”¨
df_orbis['sales_usd'] = df_orbis['sales'] * eur_usd_rate
```

---

### 2.2 Amadeus (Bureau van Dijk)

**æä¾›å…ƒ**: Moody's Analytics  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: æ¬§å·ä¼æ¥­ç‰¹åŒ–ç‰ˆOrbisï¼ˆ21ç™¾ä¸‡ç¤¾ï¼‰

**Orbisã¨ã®é•ã„**:
- æ¬§å·ã«ç‰¹åŒ–
- ã‚ˆã‚Šè©³ç´°ãªæ¬§å·ä¼æ¥­ãƒ‡ãƒ¼ã‚¿
- Orbisã‚ˆã‚Šå®‰ä¾¡ï¼ˆæ¬§å·ç ”ç©¶ã®ã¿ã®å ´åˆï¼‰

---

### 2.3 Datastream (Refinitiv)

**æä¾›å…ƒ**: Refinitiv (LSEG)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ã‚°ãƒ­ãƒ¼ãƒãƒ«æ ªä¾¡ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿ï¼ˆ175ã‚«å›½ã€2000å¹´ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**æ ªä¾¡ãƒ‡ãƒ¼ã‚¿**:
- æ—¥æ¬¡æ ªä¾¡
- å–å¼•é‡
- å¸‚å ´æ™‚ä¾¡ç·é¡

**è²¡å‹™ãƒ‡ãƒ¼ã‚¿**:
- P/L, B/S, C/F
- è²¡å‹™æ¯”ç‡

#### Pythonã‚¢ã‚¯ã‚»ã‚¹ï¼ˆRefinitiv Eikon APIï¼‰

```python
import eikon as ek

# API Keyè¨­å®š
ek.set_app_key('YOUR_APP_KEY')

# æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
df_price, err = ek.get_data(
    instruments=['AAPL.O', 'MSFT.O', 'GOOGL.O'],
    fields=['TR.PriceClose', 'TR.Volume'],
    parameters={'SDate': '2020-01-01', 'EDate': '2023-12-31'}
)

print(df_price)

# è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—
df_financials, err = ek.get_data(
    instruments=['AAPL.O'],
    fields=['TR.Revenue', 'TR.NetIncome', 'TR.TotalAssets'],
    parameters={'Period': 'FY0', 'SDate': '2010', 'EDate': '2023'}
)
```

---

## 3. æ—¥æœ¬ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 3.1 EDINET (é‡‘èåº æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸)

**æä¾›å…ƒ**: é‡‘èåº  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: æ—¥æœ¬ä¸Šå ´ä¼æ¥­ï¼ˆ2008å¹´ã€œç¾åœ¨ã€ä¸€éƒ¨2004å¹´ã€œï¼‰  
**ä¼æ¥­æ•°**: ç´„4,000ç¤¾ï¼ˆä¸Šå ´ä¼æ¥­ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸**:
- ä¼æ¥­æ¦‚æ³
- äº‹æ¥­ã®çŠ¶æ³
- è¨­å‚™ã®çŠ¶æ³
- æå‡ºä¼šç¤¾ã®çŠ¶æ³
- çµŒç†ã®çŠ¶æ³ï¼ˆè²¡å‹™è«¸è¡¨ï¼‰
- æ ªå¼ã®çŠ¶æ³

#### APIå®Ÿè£…ï¼ˆå®Œå…¨ç‰ˆï¼‰

```python
import requests
import pandas as pd
from bs4 import BeautifulSoup
import time

class EDINETCollector:
    """EDINET APIã‹ã‚‰æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    
    def __init__(self):
        self.base_url = "https://disclosure.edinet-fsa.go.jp/api/v1"
        
    def get_document_list(self, date):
        """æŒ‡å®šæ—¥ã®æå‡ºæ›¸é¡ä¸€è¦§ã‚’å–å¾—
        
        Args:
            date: 'YYYY-MM-DD'å½¢å¼
        """
        url = f"{self.base_url}/documents.json"
        params = {'date': date, 'type': 2}  # type=2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿
        
        response = requests.get(url, params=params)
        
        if response.status_code == 200:
            return response.json()
        else:
            return None
    
    def get_financials(self, start_date, end_date, doc_type='120'):
        """æœŸé–“å†…ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚’åé›†
        
        Args:
            start_date: 'YYYY-MM-DD'
            end_date: 'YYYY-MM-DD'
            doc_type: '120'=æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
        """
        date_range = pd.date_range(start_date, end_date, freq='D')
        
        all_docs = []
        
        for date in date_range:
            date_str = date.strftime('%Y-%m-%d')
            print(f"å–å¾—ä¸­: {date_str}")
            
            data = self.get_document_list(date_str)
            
            if data and 'results' in data:
                for doc in data['results']:
                    if doc['docTypeCode'] == doc_type:
                        all_docs.append({
                            'edinetCode': doc['edinetCode'],
                            'secCode': doc.get('secCode'),
                            'filerName': doc['filerName'],
                            'docID': doc['docID'],
                            'submitDateTime': doc['submitDateTime'],
                            'periodStart': doc.get('periodStart'),
                            'periodEnd': doc.get('periodEnd')
                        })
            
            time.sleep(0.5)  # APIè² è·è»½æ¸›
        
        return pd.DataFrame(all_docs)

# ä½¿ç”¨ä¾‹
collector = EDINETCollector()

# 2023å¹´ã®æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ä¸€è¦§å–å¾—
df_edinet = collector.get_financials('2023-01-01', '2023-12-31')

print(f"å–å¾—æ›¸é¡æ•°: {len(df_edinet)}")
print(f"ä¼æ¥­æ•°: {df_edinet['edinetCode'].nunique()}")

# ä¸Šå ´ä¼æ¥­ã®ã¿ï¼ˆè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚ã‚Šï¼‰
df_listed = df_edinet[df_edinet['secCode'].notna()]
print(f"ä¸Šå ´ä¼æ¥­æ•°: {df_listed['secCode'].nunique()}")
```

#### XBRLè²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡º

```python
import requests
import zipfile
import io
from lxml import etree

def download_xbrl(doc_id):
    """XBRLãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    url = f"https://disclosure.edinet-fsa.go.jp/api/v1/documents/{doc_id}"
    params = {'type': 1}  # type=1: æå‡ºæœ¬æ–‡æ›¸åŠã³ç›£æŸ»å ±å‘Šæ›¸
    
    response = requests.get(url, params=params)
    
    if response.status_code == 200:
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹
        zip_file = zipfile.ZipFile(io.BytesIO(response.content))
        return zip_file
    return None

def extract_financials_from_xbrl(zip_file):
    """XBRLã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
    # XBRLãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    xbrl_files = [f for f in zip_file.namelist() if f.endswith('.xbrl')]
    
    if xbrl_files:
        with zip_file.open(xbrl_files[0]) as f:
            tree = etree.parse(f)
            root = tree.getroot()
            
            # åå‰ç©ºé–“
            ns = {'xbrli': 'http://www.xbrl.org/2003/instance',
                  'jpcrp': 'http://disclosure.edinet-fsa.go.jp/taxonomy/jpcrp/2023-11-01'}
            
            # ç·è³‡ç”£ã®æŠ½å‡ºä¾‹
            total_assets = root.find('.//jpcrp:Assets', ns)
            if total_assets is not None:
                return {'total_assets': total_assets.text}
    
    return {}

# ä½¿ç”¨ä¾‹
doc_id = df_edinet.iloc[0]['docID']
zip_file = download_xbrl(doc_id)
if zip_file:
    financials = extract_financials_from_xbrl(zip_file)
    print(financials)
```

**æ³¨æ„**: XBRLè§£æã¯è¤‡é›‘ã€‚å®Ÿç”¨ã«ã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆarelleç­‰ï¼‰æ¨å¥¨ã€‚

---

### 3.2 JPX (æ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ— - æ ªä¾¡ãƒ‡ãƒ¼ã‚¿)

**æä¾›å…ƒ**: æ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ—  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: æ±è¨¼ä¸Šå ´ä¼æ¥­ï¼ˆ1949å¹´ã€œç¾åœ¨ï¼‰

#### ãƒ‡ãƒ¼ã‚¿å–å¾—æ–¹æ³•

**Option 1: å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
- æ—¥æ¬¡æ ªä¾¡: https://www.jpx.co.jp/markets/statistics-equities/misc/01.html

**Option 2: Yahoo Finance Japan APIï¼ˆéå…¬å¼ï¼‰**
```python
import pandas as pd
import yfinance as yf

# ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šï¼ˆ7203.Tï¼‰ã®æ ªä¾¡å–å¾—
ticker = yf.Ticker("7203.T")

# æ—¥æ¬¡æ ªä¾¡
df_price = ticker.history(start="2020-01-01", end="2023-12-31")

print(df_price.head())
print(f"å–å¾—æœŸé–“: {df_price.index[0]} ~ {df_price.index[-1]}")

# è²¡å‹™ãƒ‡ãƒ¼ã‚¿
financials = ticker.financials
balance_sheet = ticker.balance_sheet
cashflow = ticker.cashflow

print("ç·è³‡ç”£:", balance_sheet.loc['Total Assets'].iloc[0])
```

**Option 3: Nikkei NEEDSï¼ˆæœ‰æ–™ï¼‰**
- æœ€ã‚‚åŒ…æ‹¬çš„ãªæ—¥æœ¬æ ªä¾¡ãƒ‡ãƒ¼ã‚¿
- å¤§å­¦ãƒ»ç ”ç©¶æ©Ÿé–¢å‘ã‘ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

---

### 3.3 e-Stat (æ”¿åºœçµ±è¨ˆãƒãƒ¼ã‚¿ãƒ«)

**æä¾›å…ƒ**: ç·å‹™çœçµ±è¨ˆå±€  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: æ—¥æœ¬ã®å®˜å…¬åºçµ±è¨ˆï¼ˆç„¡æ–™ï¼‰

#### ä¸»è¦çµ±è¨ˆ

- çµŒæ¸ˆã‚»ãƒ³ã‚µã‚¹
- å·¥æ¥­çµ±è¨ˆ
- å•†æ¥­çµ±è¨ˆ
- GDPçµ±è¨ˆ
- äººå£çµ±è¨ˆ

#### APIå®Ÿè£…

```python
import requests
import pandas as pd

class EStatAPI:
    """e-Stat APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, app_id):
        """
        Args:
            app_id: e-Stat APIåˆ©ç”¨ç™»éŒ²ã§å–å¾—ï¼ˆç„¡æ–™ï¼‰
        """
        self.base_url = "https://api.e-stat.go.jp/rest/3.0/app/json"
        self.app_id = app_id
    
    def get_stats_list(self, search_word):
        """çµ±è¨ˆè¡¨æ¤œç´¢"""
        url = f"{self.base_url}/getStatsList"
        params = {
            'appId': self.app_id,
            'searchWord': search_word,
            'limit': 100
        }
        
        response = requests.get(url, params=params)
        if response.status_code == 200:
            return response.json()
        return None
    
    def get_stats_data(self, stats_data_id):
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿å–å¾—"""
        url = f"{self.base_url}/getStatsData"
        params = {
            'appId': self.app_id,
            'statsDataId': stats_data_id,
            'metaGetFlg': 'Y'
        }
        
        response = requests.get(url, params=params)
        if response.status_code == 200:
            data = response.json()
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
            return self._parse_stats_data(data)
        return None
    
    def _parse_stats_data(self, data):
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›"""
        # ç°¡ç•¥åŒ–ç‰ˆï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šè¤‡é›‘ï¼‰
        values = data['GET_STATS_DATA']['STATISTICAL_DATA']['DATA_INF']['VALUE']
        
        df = pd.DataFrame(values)
        return df

# ä½¿ç”¨ä¾‹
api = EStatAPI(app_id='YOUR_APP_ID')  # e-Statã‚µã‚¤ãƒˆã§ç™»éŒ²

# GDPçµ±è¨ˆæ¤œç´¢
results = api.get_stats_list('GDP')
print(f"æ¤œç´¢çµæœ: {len(results)}ä»¶")

# çµ±è¨ˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆä¾‹: çµ±è¨ˆè¡¨IDï¼‰
df_gdp = api.get_stats_data(stats_data_id='0003410379')
```

---

### 3.4 NEEDS (æ—¥æœ¬çµŒæ¸ˆæ–°èç¤¾)

**æä¾›å…ƒ**: æ—¥æœ¬çµŒæ¸ˆæ–°èç¤¾  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: æ—¥æœ¬ä¼æ¥­ï¼ˆ1970å¹´ä»£ã€œç¾åœ¨ï¼‰

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**è²¡å‹™ãƒ‡ãƒ¼ã‚¿**:
- æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ãƒ™ãƒ¼ã‚¹
- é€£çµãƒ»å˜ä½“è²¡å‹™è«¸è¡¨
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±

**æ ªä¾¡ãƒ‡ãƒ¼ã‚¿**:
- æ—¥æ¬¡ãƒ»é€±æ¬¡ãƒ»æœˆæ¬¡
- èª¿æ•´å¾Œä¾¡æ ¼
- æ ªå¼åˆ†å‰²è€ƒæ…®

**ä¼æ¥­æƒ…å ±**:
- ä¼æ¥­æ¦‚è¦
- å½¹å“¡æƒ…å ±
- æ ªä¸»æ§‹æˆ

#### ã‚¢ã‚¯ã‚»ã‚¹

**å¤§å­¦å¥‘ç´„ãŒå¿…è¦**ï¼ˆWRDSçš„ãªä½ç½®ã¥ã‘ï¼‰

```python
# NEEDS-Financial QUESTã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¾‹ï¼ˆæ¦‚å¿µï¼‰
# å®Ÿéš›ã®APIã¯å¥‘ç´„å…ˆã«ã‚ˆã‚‹

import pandas as pd

# NEEDSãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆé€šå¸¸ã¯CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰
df_needs = pd.read_csv('needs_export.csv', encoding='shift-jis')

# å¤‰æ•°åæ¨™æº–åŒ–
df_needs = df_needs.rename(columns={
    'è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰': 'sec_code',
    'ä¼šç¤¾å': 'company_name',
    'æ±ºç®—æœŸ': 'fiscal_year',
    'å£²ä¸Šé«˜': 'sales',
    'ç·è³‡ç”£': 'total_assets',
    'ç´”åˆ©ç›Š': 'net_income'
})
```

---

## 4. éŸ“å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 4.1 DART (éŸ“å›½é‡‘èç›£ç£é™¢ é›»å­å…¬ç¤ºã‚·ã‚¹ãƒ†ãƒ )

**æä¾›å…ƒ**: éŸ“å›½é‡‘èç›£ç£é™¢ (FSS)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: éŸ“å›½ä¸Šå ´ä¼æ¥­ï¼ˆ2000å¹´ã€œç¾åœ¨ï¼‰  
**ä¼æ¥­æ•°**: ç´„2,500ç¤¾

#### ä¸»è¦ãƒ‡ãƒ¼ã‚¿

**äº‹æ¥­å ±å‘Šæ›¸**:
- è²¡å‹™è«¸è¡¨
- ç›£æŸ»å ±å‘Šæ›¸
- é‡è¦äº‹é …å ±å‘Š

#### APIå®Ÿè£…ï¼ˆOpen DART APIï¼‰

```python
import requests
import pandas as pd

class DARTCollector:
    """éŸ“å›½ DART API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, api_key):
        """
        Args:
            api_key: DART API Keyï¼ˆç„¡æ–™ç™»éŒ²ï¼‰
                    https://opendart.fss.or.kr/
        """
        self.base_url = "https://opendart.fss.or.kr/api"
        self.api_key = api_key
    
    def get_corp_list(self):
        """ä¼æ¥­ãƒªã‚¹ãƒˆå–å¾—"""
        url = f"{self.base_url}/corpCode.xml"
        params = {'crtfc_key': self.api_key}
        
        response = requests.get(url, params=params)
        
        if response.status_code == 200:
            # XMLãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡ç•¥åŒ–ï¼‰
            return response.content
        return None
    
    def get_financial_statements(self, corp_code, bsns_year, reprt_code='11011'):
        """è²¡å‹™è«¸è¡¨å–å¾—
        
        Args:
            corp_code: ä¼æ¥­ã‚³ãƒ¼ãƒ‰
            bsns_year: ì‚¬ì—…ì—°ë„ (YYYY)
            reprt_code: '11011'=ì‚¬ì—…ë³´ê³ ì„œ(å¹´æ¬¡)
        """
        url = f"{self.base_url}/fnlttSinglAcntAll.json"
        params = {
            'crtfc_key': self.api_key,
            'corp_code': corp_code,
            'bsns_year': bsns_year,
            'reprt_code': reprt_code,
            'fs_div': 'CFS'  # é€£çµè²¡å‹™è«¸è¡¨
        }
        
        response = requests.get(url, params=params)
        
        if response.status_code == 200:
            data = response.json()
            if data['status'] == '000':
                return pd.DataFrame(data['list'])
        return None

# ä½¿ç”¨ä¾‹
dart = DARTCollector(api_key='YOUR_API_KEY')

# ã‚µãƒ ã‚¹ãƒ³é›»å­ï¼ˆä¾‹: corp_code='00126380'ï¼‰ã®è²¡å‹™è«¸è¡¨
df_samsung = dart.get_financial_statements(
    corp_code='00126380',
    bsns_year='2023'
)

if df_samsung is not None:
    print("è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ:")
    print(df_samsung[['account_nm', 'thstrm_amount']].head(10))
```

---

### 4.2 KRX (éŸ“å›½å–å¼•æ‰€ - æ ªä¾¡ãƒ‡ãƒ¼ã‚¿)

**æä¾›å…ƒ**: éŸ“å›½å–å¼•æ‰€  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: KOSPI, KOSDAQä¸Šå ´ä¼æ¥­

#### ãƒ‡ãƒ¼ã‚¿å–å¾—

**Option 1: KRXå…¬å¼ã‚µã‚¤ãƒˆ**
- http://data.krx.co.kr/

**Option 2: pykrx ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆéå…¬å¼ï¼‰**
```python
from pykrx import stock
import pandas as pd

# ã‚µãƒ ã‚¹ãƒ³é›»å­ï¼ˆ005930ï¼‰ã®æ ªä¾¡
df_price = stock.get_market_ohlcv_by_date(
    fromdate="20200101", 
    todate="20231231", 
    ticker="005930"
)

print(df_price.head())

# å…¨KOSPIéŠ˜æŸ„ãƒªã‚¹ãƒˆ
tickers = stock.get_market_ticker_list("20231231", market="KOSPI")
print(f"KOSPIä¸Šå ´ä¼æ¥­æ•°: {len(tickers)}")

# æ™‚ä¾¡ç·é¡
market_cap = stock.get_market_cap_by_ticker("20231231", market="KOSPI")
print(market_cap.head())
```

---

## 5. ä¸­å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### 5.1 CNINFO (å·¨æ½®è³‡è¨Šç¶²)

**æä¾›å…ƒ**: æ·±åœ³è¨¼åˆ¸å–å¼•æ‰€  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸Šæµ·ãƒ»æ·±åœ³ä¸Šå ´ä¼æ¥­ï¼ˆ1990å¹´ä»£ã€œç¾åœ¨ï¼‰

#### ãƒ‡ãƒ¼ã‚¿å–å¾—

**å…¬å¼ã‚µã‚¤ãƒˆ**: http://www.cninfo.com.cn/

**Pythonå®Ÿè£…ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰**:
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

def get_cninfo_announcement_list(stock_code, start_date, end_date):
    """CNINFOã‹ã‚‰å…¬å‘Šãƒªã‚¹ãƒˆå–å¾—
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç  (ä¾‹: '000001'=å¹³å®‰éŠ€è¡Œ)
        start_date: 'YYYY-MM-DD'
        end_date: 'YYYY-MM-DD'
    """
    url = "http://www.cninfo.com.cn/new/disclosure/stock"
    params = {
        'stockCode': stock_code,
        'plate': '',  # æ¿å—
        'category': '',  # ç±»åˆ«
        'pageNum': 1,
        'pageSize': 30
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0',
        'Accept': 'application/json'
    }
    
    response = requests.get(url, params=params, headers=headers)
    
    if response.status_code == 200:
        data = response.json()
        return pd.DataFrame(data['announcements'])
    return None

# ä½¿ç”¨ä¾‹
df_announcements = get_cninfo_announcement_list(
    stock_code='000001',
    start_date='2023-01-01',
    end_date='2023-12-31'
)

if df_announcements is not None:
    print(f"å…¬å‘Šä»¶æ•°: {len(df_announcements)}")
```

**æ³¨æ„**: CNINFOã®å…¬å¼APIã¯é™å®šçš„ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»è¦ç´„ã«æ³¨æ„ã€‚

---

### 5.2 Tushare (Python Financial Data Interface)

**æä¾›å…ƒ**: Tushare.pro (æ°‘é–“)  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸­å›½Aæ ªãƒ»é¦™æ¸¯æ ªï¼ˆ1990å¹´ã€œç¾åœ¨ï¼‰

#### APIå®Ÿè£…

```python
import tushare as ts

# API Tokenè¨­å®šï¼ˆç™»éŒ²å¿…è¦: https://tushare.pro/ï¼‰
ts.set_token('YOUR_TOKEN')
pro = ts.pro_api()

# ä¸Šå ´ä¼æ¥­ãƒªã‚¹ãƒˆ
df_stock_basic = pro.stock_basic(
    exchange='',
    list_status='L',
    fields='ts_code,symbol,name,area,industry,list_date'
)

print(f"ä¸Šå ´ä¼æ¥­æ•°: {len(df_stock_basic)}")

# å¹³å®‰éŠ€è¡Œï¼ˆ000001.SZï¼‰ã®æ—¥æ¬¡æ ªä¾¡
df_daily = pro.daily(
    ts_code='000001.SZ',
    start_date='20200101',
    end_date='20231231'
)

print(df_daily.head())

# è²¡å‹™æŒ‡æ¨™
df_fina_indicator = pro.fina_indicator(
    ts_code='000001.SZ',
    start_date='20200101',
    end_date='20231231'
)

print("è²¡å‹™æŒ‡æ¨™:")
print(df_fina_indicator[['end_date', 'roe', 'roa', 'debt_to_assets']].head())
```

#### é•·æ‰€ãƒ»çŸ­æ‰€

**é•·æ‰€**:
- âœ… Pythonãƒã‚¤ãƒ†ã‚£ãƒ–
- âœ… APIãŒå……å®Ÿ
- âœ… ç„¡æ–™ãƒ—ãƒ©ãƒ³ï¿½ã‚Š

**çŸ­æ‰€**:
- âŒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆæœ‰æ–™ãƒ—ãƒ©ãƒ³ã§ç·©å’Œï¼‰
- âŒ è‹±èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé™å®šçš„

---

### 5.3 AKShare (Another Knowledge Share)

**æä¾›å…ƒ**: ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸­å›½ãƒ»ã‚°ãƒ­ãƒ¼ãƒãƒ«è²¡å‹™ãƒ‡ãƒ¼ã‚¿

```python
import akshare as ak

# Aæ ªãƒªã‚¹ãƒˆ
stock_info_a_code_name_df = ak.stock_info_a_code_name()
print(f"Aæ ªæ•°: {len(stock_info_a_code_name_df)}")

# å¹³å®‰éŠ€è¡Œã®æ ªä¾¡
stock_zh_a_hist_df = ak.stock_zh_a_hist(
    symbol="000001",
    period="daily",
    start_date="20200101",
    end_date="20231231",
    adjust=""
)

print(stock_zh_a_hist_df.head())

# è²¡å‹™è«¸è¡¨
stock_financial_report_sina_df = ak.stock_financial_report_sina(
    stock="000001",
    symbol="èµ„äº§è´Ÿå€ºè¡¨"
)

print("è³‡ç”£è² å‚µè¡¨:")
print(stock_financial_report_sina_df)
```

---

## 6. ASEANè«¸å›½

### 6.1 ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«

**SGX (Singapore Exchange)**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.sgx.com/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„700ç¤¾
- ãƒ‡ãƒ¼ã‚¿å–å¾—: Yahoo Finance, Bloomberg

```python
import yfinance as yf

# DBSéŠ€è¡Œï¼ˆã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«æœ€å¤§æ‰‹ï¼‰
dbs = yf.Ticker("D05.SI")
df_dbs = dbs.history(start="2020-01-01", end="2023-12-31")
```

---

### 6.2 ã‚¿ã‚¤

**SET (Stock Exchange of Thailand)**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.set.or.th/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„800ç¤¾
- API: SET Market Data APIï¼ˆæœ‰æ–™ï¼‰

---

### 6.3 ãƒãƒ¬ãƒ¼ã‚·ã‚¢

**Bursa Malaysia**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.bursamalaysia.com/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„900ç¤¾

---

### 6.4 ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢

**IDX (Indonesia Stock Exchange)**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.idx.co.id/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„800ç¤¾

---

### 6.5 ãƒ™ãƒˆãƒŠãƒ 

**HOSE (Ho Chi Minh Stock Exchange)**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.hsx.vn/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„400ç¤¾

---

### 6.6 ãƒ•ã‚£ãƒªãƒ”ãƒ³

**PSE (Philippine Stock Exchange)**
- å…¬å¼ã‚µã‚¤ãƒˆ: https://www.pse.com.ph/
- ä¸Šå ´ä¼æ¥­æ•°: ç´„270ç¤¾

---

## 7. ã‚¤ãƒ³ãƒ‰

### 7.1 BSE (Bombay Stock Exchange)

**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ç´„5,000ç¤¾

```python
import yfinance as yf

# Reliance Industriesï¼ˆã‚¤ãƒ³ãƒ‰æœ€å¤§æ‰‹ï¼‰
reliance = yf.Ticker("RELIANCE.BO")  # .BO = BSE
df_reliance = reliance.history(start="2020-01-01")
```

---

### 7.2 NSE (National Stock Exchange)

**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ç´„2,000ç¤¾

```python
# Reliance Industriesï¼ˆNSEï¼‰
reliance_nse = yf.Ticker("RELIANCE.NS")  # .NS = NSE
```

---

## 8. ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹

### 8.1 World Bank Open Data

**æä¾›å…ƒ**: ä¸–ç•ŒéŠ€è¡Œ  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸–ç•Œå„å›½ã®ãƒã‚¯ãƒ­çµŒæ¸ˆãƒ‡ãƒ¼ã‚¿

```python
import wbdata
import pandas as pd

# GDP per capitaå–å¾—
gdp_indicator = {'NY.GDP.PCAP.CD': 'gdp_per_capita'}

df_gdp = wbdata.get_dataframe(
    gdp_indicator,
    country=['USA', 'CHN', 'JPN', 'DEU'],
    convert_date=True
)

print(df_gdp)

# å…¨æŒ‡æ¨™ãƒªã‚¹ãƒˆ
indicators = wbdata.get_indicator(source=2)
print(f"åˆ©ç”¨å¯èƒ½æŒ‡æ¨™æ•°: {len(indicators)}")
```

---

### 8.2 IMF Data

**æä¾›å…ƒ**: å›½éš›é€šè²¨åŸºé‡‘  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ä¸–ç•ŒçµŒæ¸ˆè¦‹é€šã—ã€é‡‘èãƒ‡ãƒ¼ã‚¿

```python
import requests
import pandas as pd

# IMF API
url = "http://dataservices.imf.org/REST/SDMX_JSON.svc/CompactData/IFS/A.US.NGDP_R_K_IX"

response = requests.get(url)

if response.status_code == 200:
    data = response.json()
    # JSONãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡é›‘ï¼‰
    print("IMFãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
```

---

### 8.3 OECD.Stat

**æä¾›å…ƒ**: çµŒæ¸ˆå”åŠ›é–‹ç™ºæ©Ÿæ§‹  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: OECDåŠ ç›Ÿå›½çµ±è¨ˆ

```python
import pandas as pd

# OECD API
url = "https://stats.oecd.org/SDMX-JSON/data/QNA/JPN+USA+DEU.B1_GE.GPSA.Q/all"

response = requests.get(url, params={'startTime': '2010-Q1', 'endTime': '2023-Q4'})

if response.status_code == 200:
    data = response.json()
    # ãƒ‘ãƒ¼ã‚¹...
```

---

### 8.4 SEC EDGAR (ç±³å›½ä¼æ¥­é–‹ç¤º)

**æä¾›å…ƒ**: ç±³å›½è¨¼åˆ¸å–å¼•å§”å“¡ä¼š  
**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ç±³å›½ä¸Šå ´ä¼æ¥­ã®å…¨é–‹ç¤ºæ›¸é¡

è©³ç´°: [`4-text-analysis` skill](../4-text-analysis/SKILL.md) - SEC EDGARã‚»ã‚¯ã‚·ãƒ§ãƒ³

---

### 8.5 Yahoo Finance

**ã‚«ãƒãƒ¬ãƒƒã‚¸**: ã‚°ãƒ­ãƒ¼ãƒãƒ«æ ªä¾¡ãƒ‡ãƒ¼ã‚¿

```python
import yfinance as yf

# è¤‡æ•°éŠ˜æŸ„ä¸€æ‹¬å–å¾—
tickers = ['AAPL', 'MSFT', 'GOOGL', '7203.T', '005930.KS']
df = yf.download(tickers, start='2020-01-01', end='2023-12-31')

print(df['Close'].head())
```

---

## 9. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠãƒãƒˆãƒªãƒƒã‚¯ã‚¹

### 9.1 åœ°åŸŸÃ—å¤‰æ•°ã‚¿ã‚¤ãƒ—

| åœ°åŸŸ/å›½ | è²¡å‹™ãƒ‡ãƒ¼ã‚¿ | æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ | ã‚¬ãƒãƒŠãƒ³ã‚¹ | M&A | ESG | ç„¡æ–™? |
|---------|-----------|-----------|-----------|-----|-----|-------|
| **åŒ—ç±³** | Compustat | CRSP | ExecuComp, BoardEx | SDC | MSCI, CDP | âŒ |
| **æ¬§å·** | Orbis, Datastream | Datastream | BoardEx | Zephyr | Refinitiv | âŒ |
| **æ—¥æœ¬** | EDINET, NEEDS | JPX, Yahoo | EDINET | RECOF | - | âœ…/âŒ |
| **éŸ“å›½** | DART | KRX | DART | - | - | âœ… |
| **ä¸­å›½** | Tushare, AKShare | Tushare | CNINFO | - | - | âœ… |
| **ASEAN** | å„å›½å–å¼•æ‰€ | Yahoo Finance | - | - | - | éƒ¨åˆ†çš„ |
| **ã‚°ãƒ­ãƒ¼ãƒãƒ«** | World Bank | Yahoo Finance | - | - | CDP | âœ… |

---

### 9.2 ç ”ç©¶ãƒ†ãƒ¼ãƒåˆ¥æ¨å¥¨ã‚½ãƒ¼ã‚¹

**ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ï¼ˆR&Dâ†’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼‰**:
```
ç±³å›½: Compustat (R&D) + CRSP (æ ªä¾¡)
æ—¥æœ¬: EDINET (R&D) + JPX (æ ªä¾¡)
æ¬§å·: Orbis (R&D) + Datastream (æ ªä¾¡)
```

**ã‚¬ãƒãƒŠãƒ³ã‚¹ç ”ç©¶ï¼ˆBoardæ§‹æˆâ†’æˆ¦ç•¥ï¼‰**:
```
ç±³å›½: ExecuComp + BoardEx + Compustat
æ—¥æœ¬: EDINET (å½¹å“¡æƒ…å ±) + NEEDS
æ¬§å·: BoardEx + Orbis
```

**M&Aç ”ç©¶**:
```
ã‚°ãƒ­ãƒ¼ãƒãƒ«: SDC Platinum (Thomson Reuters)
æ—¥æœ¬: RECOF M&Aãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
```

**ESGç ”ç©¶**:
```
ã‚°ãƒ­ãƒ¼ãƒãƒ«: MSCI ESG, Refinitiv ESG, CDP
ç„¡æ–™: CDP Climate Change, EPA TRI

è©³ç´°: 7-esg-sustainability skill
```

---

## 10. APIå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### 10.1 ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

```python
import time
import requests
from functools import wraps

def rate_limited(max_per_second=1):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    min_interval = 1.0 / max_per_second
    
    def decorator(func):
        last_called = [0.0]
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            
            result = func(*args, **kwargs)
            last_called[0] = time.time()
            return result
        
        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@rate_limited(max_per_second=2)  # 1ç§’ã«2ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§
def fetch_data(url):
    return requests.get(url)
```

---

### 10.2 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def create_robust_session():
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³"""
    session = requests.Session()
    
    retry = Retry(
        total=5,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )
    
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    return session

# ä½¿ç”¨ä¾‹
session = create_robust_session()
response = session.get('https://api.example.com/data')
```

---

### 10.3 èªè¨¼ãƒ‘ã‚¿ãƒ¼ãƒ³

**Basic Auth**:
```python
import requests

response = requests.get(
    'https://api.example.com/data',
    auth=('username', 'password')
)
```

**API Keyï¼ˆHeaderï¼‰**:
```python
headers = {'Authorization': f'Bearer {api_key}'}
response = requests.get(url, headers=headers)
```

**OAuth 2.0**:
```python
from requests_oauthlib import OAuth2Session

client_id = 'YOUR_CLIENT_ID'
client_secret = 'YOUR_CLIENT_SECRET'

oauth = OAuth2Session(client_id)
token = oauth.fetch_token(
    'https://api.example.com/oauth/token',
    client_secret=client_secret
)

response = oauth.get('https://api.example.com/data')
```

---

## 11. ãƒ‡ãƒ¼ã‚¿å“è³ªæ¯”è¼ƒ

### 11.1 ä¿¡é ¼æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°

| ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | ä¿¡é ¼æ€§ | ã‚«ãƒãƒ¬ãƒƒã‚¸ | æ›´æ–°é »åº¦ | å­¦è¡“ä½¿ç”¨å®Ÿç¸¾ |
|------------|--------|----------|---------|-------------|
| Compustat | â­â­â­â­â­ | â­â­â­â­ | å››åŠæœŸ | â­â­â­â­â­ |
| CRSP | â­â­â­â­â­ | â­â­â­â­ | æ—¥æ¬¡ | â­â­â­â­â­ |
| Orbis | â­â­â­â­ | â­â­â­â­â­ | å¹´æ¬¡ | â­â­â­â­ |
| EDINET | â­â­â­â­â­ | â­â­â­ | å³æ™‚ | â­â­â­â­ |
| DART | â­â­â­â­ | â­â­â­ | å³æ™‚ | â­â­â­ |
| Tushare | â­â­â­ | â­â­â­â­ | æ—¥æ¬¡ | â­â­â­ |
| Yahoo Finance | â­â­â­ | â­â­â­â­â­ | æ—¥æ¬¡ | â­â­ |

---

## 12. ã‚³ã‚¹ãƒˆæ¯”è¼ƒ

### 12.1 è²»ç”¨ãƒ¬ãƒ™ãƒ«

**ç„¡æ–™**:
- âœ… EDINET (æ—¥æœ¬)
- âœ… DART (éŸ“å›½)
- âœ… Tushare (ä¸­å›½ã€åŸºæœ¬ãƒ—ãƒ©ãƒ³)
- âœ… World Bank Open Data
- âœ… IMF Data
- âœ… OECD.Stat
- âœ… SEC EDGAR (ç±³å›½)
- âœ… Yahoo Finance

**å¤§å­¦å¥‘ç´„ï¼ˆå­¦ç”Ÿã¯å®Ÿè³ªç„¡æ–™ï¼‰**:
- ğŸŸ¡ WRDS (Compustat, CRSPå«ã‚€): $2,000-5,000/å¹´
- ğŸŸ¡ NEEDS (æ—¥æœ¬): å¤§å­¦ã«ã‚ˆã‚‹
- ğŸŸ¡ BoardEx: WRDSçµŒç”±

**é«˜é¡**:
- ğŸ”´ Bloomberg Terminal: $24,000/å¹´
- ğŸ”´ Refinitiv Eikon: $20,000/å¹´
- ğŸ”´ Orbis: â‚¬æ•°åƒã€œæ•°ä¸‡/å¹´
- ğŸ”´ SDC Platinum: $æ•°åƒ/å¹´

---

## ğŸ“Š Quick Reference

### åˆå¿ƒè€…å‘ã‘æ¨å¥¨é–‹å§‹ç‚¹

**ç±³å›½ä¼æ¥­ç ”ç©¶**:
1. WRDSå¥‘ç´„ç¢ºèªï¼ˆå¤§å­¦ï¼‰
2. Compustat + CRSP
3. ä»£æ›¿: Yahoo Financeï¼ˆç„¡æ–™ï¼‰

**æ—¥æœ¬ä¼æ¥­ç ”ç©¶**:
1. EDINET APIï¼ˆç„¡æ–™ï¼‰
2. Yahoo Finance Japan
3. äºˆç®—ã‚ã‚Š: NEEDS

**ä¸­å›½ä¼æ¥­ç ”ç©¶**:
1. Tushareï¼ˆç„¡æ–™ãƒ—ãƒ©ãƒ³ï¼‰
2. AKShareï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ï¼‰

**ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¯”è¼ƒç ”ç©¶**:
1. World Bank Open Data
2. Yahoo Finance
3. Orbisï¼ˆäºˆç®—ã‚ã‚Šï¼‰

---

### Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```bash
# å¿…é ˆ
pip install pandas numpy requests

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥
pip install wrds  # WRDSç”¨
pip install yfinance  # Yahoo Finance
pip install wbdata  # World Bank
pip install tushare  # ä¸­å›½
pip install akshare  # ä¸­å›½
pip install pykrx  # éŸ“å›½

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
pip install beautifulsoup4 lxml  # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
pip install openpyxl xlrd  # Excelèª­ã¿è¾¼ã¿
```

---

## å‚è€ƒæ–‡çŒ®

- Fama, E. F., & French, K. R. (2015). A five-factor asset pricing model. *Journal of Financial Economics*, 116(1), 1-22.
  - Compustat & CRSPä½¿ç”¨ä¾‹

- Fan, J. P., Wong, T. J., & Zhang, T. (2007). Politically connected CEOs, corporate governance, and Post-IPO performance of China's newly partially privatized firms. *Journal of Financial Economics*, 84(2), 330-357.
  - ä¸­å›½ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ä¾‹

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### ãƒ‡ãƒ¼ã‚¿åé›†å¾Œ
â†’ [`1-core-workflow` skill](../1-core-workflow/SKILL.md) Phase 4 (Dataset Construction)

### ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æ
â†’ [`4-text-analysis` skill](../4-text-analysis/SKILL.md)

### è‡ªå‹•åé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â†’ [`8-automation` skill](../8-automation/SKILL.md)

---

**ã“ã®ã‚¹ã‚­ãƒ«ã§ã€ä¸–ç•Œä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã§ãã¾ã™ã€‚**  
**RQã«æœ€é©ãªã‚½ãƒ¼ã‚¹ã‚’é¸æŠã—ã€åŠ¹ç‡çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚**

---

**æœ€çµ‚æ›´æ–°**: 2025-11-01  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 4.0.0  
**æ¬¡å›ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹äºˆå®š**: 2025-12-01
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
