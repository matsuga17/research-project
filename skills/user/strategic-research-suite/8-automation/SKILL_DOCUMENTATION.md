---
name: strategic-research-automation
description: End-to-end research automation pipeline for strategic management studies including data collection, preprocessing, analysis, and documentation with reproducibility package generation.
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 3 (Data Collection), Phase 8 (Documentation)
  - data-sources: Automated data collection
  - statistical-methods: Automated analysis
---

# Research Automation Pipeline v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

**Phase 1-8å®Œå…¨è‡ªå‹•åŒ–**ã®ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

- âœ… ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’ä¸€æ‹¬å®Ÿè¡Œã—ãŸã„
- âœ… å†ç¾å¯èƒ½æ€§ã‚’æœ€å¤§åŒ–ã—ãŸã„
- âœ… è¤‡æ•°ã®ç ”ç©¶ã‚’ä¸¦è¡Œå®Ÿè¡Œã—ãŸã„
- âœ… ãƒ‡ãƒ¼ã‚¿åé›†ã€œåˆ†æã‚’è‡ªå‹•åŒ–ã—ãŸã„

### å‰ææ¡ä»¶

- Pythonä¸­ç´šï¼ˆã‚¯ãƒ©ã‚¹è¨­è¨ˆã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®åŸºç¤
- GitåŸºç¤ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ï¼‰

### ä»–ã‚¹ã‚­ãƒ«ã¨ã®é€£æº

- ã™ã¹ã¦ã®ã‚¹ã‚­ãƒ«ã‚’çµ±åˆ

---

## ğŸ“‹ ç›®æ¬¡

1. [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ](#1-ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ )
2. [è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#2-è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
3. [å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸](#3-å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸)
4. [Quick Reference](#4-quick-reference)

---

## 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

### 1.1 æ¨™æº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
research_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ›´å³ç¦ï¼‰
â”‚   â”œâ”€â”€ processed/        # å‰å‡¦ç†æ¸ˆã¿
â”‚   â””â”€â”€ final/            # åˆ†æç”¨æœ€çµ‚ç‰ˆ
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ 01_collect.py     # ãƒ‡ãƒ¼ã‚¿åé›†
â”‚   â”œâ”€â”€ 02_clean.py       # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
â”‚   â”œâ”€â”€ 03_merge.py       # ãƒãƒ¼ã‚¸
â”‚   â”œâ”€â”€ 04_variables.py   # å¤‰æ•°æ§‹ç¯‰
â”‚   â”œâ”€â”€ 05_analysis.py    # çµ±è¨ˆåˆ†æ
â”‚   â””â”€â”€ 06_visualize.py   # å¯è¦–åŒ–
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ tables/           # å›å¸°çµæœè¡¨
â”‚   â”œâ”€â”€ figures/          # å›³
â”‚   â””â”€â”€ logs/             # å®Ÿè¡Œãƒ­ã‚°
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â”œâ”€â”€ codebook.md
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ config.yaml           # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ requirements.txt      # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”œâ”€â”€ run_all.sh            # å…¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
â””â”€â”€ README.md
```

### 1.2 è‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
import os

def create_project_structure(project_name):
    """ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’è‡ªå‹•ç”Ÿæˆ"""
    
    dirs = [
        f'{project_name}/data/raw',
        f'{project_name}/data/processed',
        f'{project_name}/data/final',
        f'{project_name}/code',
        f'{project_name}/output/tables',
        f'{project_name}/output/figures',
        f'{project_name}/output/logs',
        f'{project_name}/docs'
    ]
    
    for d in dirs:
        os.makedirs(d, exist_ok=True)
    
    # READMEç”Ÿæˆ
    readme = f"""# {project_name}

## Research Question
[è¨˜å…¥ã—ã¦ãã ã•ã„]

## Data Sources
- Source 1: [è©³ç´°]
- Source 2: [è©³ç´°]

## Execution
```bash
bash run_all.sh
```

## Output
- Tables: `output/tables/`
- Figures: `output/figures/`
"""
    
    with open(f'{project_name}/README.md', 'w') as f:
        f.write(readme)
    
    print(f"Project structure created: {project_name}/")

# ä½¿ç”¨ä¾‹
create_project_structure('rd_performance_study')
```

---

## 2. è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### 2.1 StrategicResearchPipeline ã‚¯ãƒ©ã‚¹

```python
import pandas as pd
import logging
from datetime import datetime

class StrategicResearchPipeline:
    """Phase 1-8å®Œå…¨è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config):
        """
        Args:
            config: dict with keys:
                - research_question: str
                - data_sources: list of dict
                - sample_criteria: dict
                - output_dir: str
        """
        self.config = config
        self.setup_logging()
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_file = f"output/logs/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def phase1_design(self):
        """Phase 1: Research Design"""
        self.logger.info("=== Phase 1: Research Design ===")
        self.logger.info(f"RQ: {self.config['research_question']}")
        
        # ä»®èª¬ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        for i, hyp in enumerate(self.config.get('hypotheses', []), 1):
            self.logger.info(f"H{i}: {hyp}")
    
    def phase2_collect_data(self):
        """Phase 2-3: Data Collection"""
        self.logger.info("=== Phase 2-3: Data Collection ===")
        
        dfs = []
        for source in self.config['data_sources']:
            self.logger.info(f"Collecting from: {source['name']}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®åé›†ãƒ­ã‚¸ãƒƒã‚¯
            if source['type'] == 'compustat':
                df = self._collect_compustat(source['params'])
            elif source['type'] == 'edinet':
                df = self._collect_edinet(source['params'])
            else:
                raise ValueError(f"Unknown source type: {source['type']}")
            
            dfs.append(df)
            self.logger.info(f"  Collected: {len(df)} records")
        
        return dfs
    
    def phase4_build_panel(self, dfs):
        """Phase 4: Panel Dataset Construction"""
        self.logger.info("=== Phase 4: Panel Construction ===")
        
        # ãƒãƒ¼ã‚¸
        df_panel = dfs[0]
        for df in dfs[1:]:
            df_panel = df_panel.merge(df, on=['firm_id', 'year'], how='inner')
        
        # MultiIndexè¨­å®š
        df_panel = df_panel.set_index(['firm_id', 'year']).sort_index()
        
        self.logger.info(f"Panel size: {len(df_panel)} firm-years")
        self.logger.info(f"Firms: {df_panel.index.get_level_values('firm_id').nunique()}")
        
        return df_panel
    
    def phase5_quality_check(self, df):
        """Phase 5: Quality Assurance"""
        self.logger.info("=== Phase 5: QA ===")
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing = df.isnull().sum()
        self.logger.info(f"Missing values:\n{missing[missing > 0]}")
        
        # å¤–ã‚Œå€¤ãƒã‚§ãƒƒã‚¯
        from scipy.stats import zscore
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
        
        for col in numeric_cols:
            outliers = (abs(zscore(df[col].dropna())) > 3).sum()
            if outliers > 0:
                self.logger.warning(f"{col}: {outliers} outliers detected")
        
        return df
    
    def phase6_construct_variables(self, df):
        """Phase 6: Variable Construction"""
        self.logger.info("=== Phase 6: Variables ===")
        
        # æ¨™æº–å¤‰æ•°æ§‹ç¯‰
        df['roa'] = df['net_income'] / df['total_assets']
        df['rd_intensity'] = df['rd_expense'] / df['revenue']
        df['firm_size'] = np.log(df['total_assets'])
        df['leverage'] = df['total_debt'] / df['total_assets']
        
        # ãƒ©ã‚°å¤‰æ•°
        df = df.reset_index()
        df = df.sort_values(['firm_id', 'year'])
        df['rd_intensity_lag1'] = df.groupby('firm_id')['rd_intensity'].shift(1)
        df = df.set_index(['firm_id', 'year'])
        
        self.logger.info("Variables constructed")
        
        return df
    
    def phase7_analyze(self, df):
        """Phase 7: Statistical Analysis"""
        self.logger.info("=== Phase 7: Analysis ===")
        
        from linearmodels.panel import PanelOLS
        
        # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
        model = PanelOLS.from_formula(
            'roa ~ rd_intensity_lag1 + firm_size + leverage + EntityEffects + TimeEffects',
            data=df
        )
        
        result = model.fit(cov_type='clustered', cluster_entity=True)
        
        self.logger.info("Main model estimated")
        self.logger.info(f"R-squared: {result.rsquared:.4f}")
        
        # çµæœä¿å­˜
        with open('output/tables/main_results.txt', 'w') as f:
            f.write(result.summary.as_text())
        
        return result
    
    def phase8_document(self, df, result):
        """Phase 8: Documentation"""
        self.logger.info("=== Phase 8: Documentation ===")
        
        # Data Dictionary
        data_dict = {
            'variable': df.columns.tolist(),
            'description': ['...'] * len(df.columns),
            'mean': df.mean().values,
            'std': df.std().values
        }
        
        pd.DataFrame(data_dict).to_csv('docs/data_dictionary.csv', index=False)
        
        # å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        self._create_replication_package(df, result)
        
        self.logger.info("Documentation completed")
    
    def run_full_pipeline(self):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            self.phase1_design()
            dfs = self.phase2_collect_data()
            df_panel = self.phase4_build_panel(dfs)
            df_panel = self.phase5_quality_check(df_panel)
            df_panel = self.phase6_construct_variables(df_panel)
            result = self.phase7_analyze(df_panel)
            self.phase8_document(df_panel, result)
            
            self.logger.info("=== PIPELINE COMPLETED ===")
            
            return {
                'data': df_panel,
                'result': result,
                'status': 'success'
            }
        
        except Exception as e:
            self.logger.error(f"Pipeline failed: {e}")
            return {'status': 'failed', 'error': str(e)}

# ä½¿ç”¨ä¾‹
config = {
    'research_question': "R&DæŠ•è³‡ã¯ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ­£ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ï¼Ÿ",
    'hypotheses': [
        "H1: R&DæŠ•è³‡å¼·åº¦ã¯ROAã«æ­£ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹"
    ],
    'data_sources': [
        {'name': 'Compustat', 'type': 'compustat', 'params': {...}},
    ],
    'sample_criteria': {
        'industry': 'manufacturing',
        'years': (2010, 2020)
    },
    'output_dir': './output/'
}

pipeline = StrategicResearchPipeline(config)
results = pipeline.run_full_pipeline()
```

---

## 3. å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

### 3.1 å¿…é ˆè¦ç´ 

```python
def create_replication_package(output_dir='replication_package'):
    """å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç”Ÿæˆ"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. README
    readme = """# Replication Package

## Requirements
```bash
pip install -r requirements.txt
```

## Data
- Place raw data in `data/raw/`
- Data sources: [è©³ç´°]

## Execution
```bash
bash run_all.sh
```

## Expected Output
- Tables: `output/tables/`
- Figures: `output/figures/`
- Estimated time: 2 hours

## Contact
[Your Email]
"""
    
    with open(f'{output_dir}/README.md', 'w') as f:
        f.write(readme)
    
    # 2. requirements.txt
    requirements = """pandas==2.0.0
numpy==1.24.0
statsmodels==0.14.0
linearmodels==5.3
scikit-learn==1.3.0
matplotlib==3.7.0
"""
    
    with open(f'{output_dir}/requirements.txt', 'w') as f:
        f.write(requirements)
    
    # 3. run_all.sh
    run_script = """#!/bin/bash
echo "Starting replication..."

python code/01_collect.py
python code/02_clean.py
python code/03_merge.py
python code/04_variables.py
python code/05_analysis.py
python code/06_visualize.py

echo "Replication completed!"
"""
    
    with open(f'{output_dir}/run_all.sh', 'w') as f:
        f.write(run_script)
    
    os.chmod(f'{output_dir}/run_all.sh', 0o755)
    
    print(f"Replication package created: {output_dir}/")

create_replication_package()
```

### 3.2 DockeråŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /research

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["bash", "run_all.sh"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  research:
    build: .
    volumes:
      - ./data:/research/data
      - ./output:/research/output
    environment:
      - WRDS_USERNAME=${WRDS_USERNAME}
      - WRDS_PASSWORD=${WRDS_PASSWORD}
```

---

## 4. Quick Reference

### è‡ªå‹•åŒ–ãƒ¬ãƒ™ãƒ«

| ãƒ¬ãƒ™ãƒ« | å†…å®¹ | æ‰€è¦æ™‚é–“ |
|--------|------|---------|
| **Level 1** | ãƒ‡ãƒ¼ã‚¿åé›†ã®ã¿ | 1-2æ™‚é–“ |
| **Level 2** | åé›† + å‰å‡¦ç† | 3-4æ™‚é–“ |
| **Level 3** | åé›† + å‰å‡¦ç† + åˆ†æ | 1æ—¥ |
| **Level 4** | Phase 1-8å®Œå…¨è‡ªå‹•åŒ– | 1-2æ—¥ |

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
import time
from functools import wraps

def retry_on_failure(max_retries=3, delay=5):
    """å¤±æ•—æ™‚ã®ãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    print(f"Attempt {attempt + 1} failed: {e}. Retrying...")
                    time.sleep(delay)
        return wrapper
    return decorator

# ä½¿ç”¨ä¾‹
@retry_on_failure(max_retries=3, delay=10)
def collect_data_with_retry():
    # APIå‘¼ã³å‡ºã—ç­‰
    pass
```

### é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
from tqdm import tqdm

def process_firms(firm_ids):
    """é€²æ—ãƒãƒ¼ä»˜ãå‡¦ç†"""
    results = []
    
    for firm_id in tqdm(firm_ids, desc="Processing firms"):
        result = process_single_firm(firm_id)
        results.append(result)
    
    return results
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½¿ç”¨

```yaml
# config.yaml
research:
  question: "R&DæŠ•è³‡ã®åŠ¹æœã¯ä¼æ¥­è¦æ¨¡ã§ç•°ãªã‚‹ã‹ï¼Ÿ"
  
data:
  sources:
    - name: compustat
      years: [2010, 2020]
    - name: crsp
      frequency: monthly

sample:
  industry_codes: [2000, 3999]
  min_observations: 5

analysis:
  dependent_var: roa
  independent_vars: [rd_intensity, firm_size, leverage]
  fixed_effects: [entity, time]
```

### 2. ãƒ­ã‚°ã®æ´»ç”¨

```python
import logging

logging.info("Data collection started")
logging.warning("Missing data detected for firm 123")
logging.error("API connection failed")
```

### 3. ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

```bash
# GitåˆæœŸåŒ–
git init
git add .
git commit -m "Initial commit: Research project setup"

# .gitignore
data/raw/
output/
*.log
__pycache__/
```

---

## ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install pandas numpy statsmodels linearmodels scikit-learn matplotlib pyyaml tqdm
```

---

## å‚è€ƒæ–‡çŒ®

- Christensen, G., & Miguel, E. (2018). "Transparency, reproducibility, and the credibility of economics research." *Journal of Economic Literature*, 56(3), 920-980.

---

**Version**: 4.0  
**Last Updated**: 2025-11-01  
**Complete**: All 8 skills finished! ğŸ‰
**ç—‡çŠ¶**:
```
HTTPError 403: Forbidden
ConnectionError: Failed to establish connection
```

**åŸå› **:
- User-Agentæœªè¨­å®š
- API keyç„¡åŠ¹ã¾ãŸã¯expired
- IPåˆ¶é™ãƒ»Rate limit
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šå•é¡Œ

**è§£æ±ºç­–**:

#### 1. User-Agentã‚’è¨­å®š:
```python
headers = {
    'User-Agent': 'YourUniversity research@email.edu'
}
response = requests.get(url, headers=headers)
```

#### 2. API keyã‚’ç¢ºèª:
```python
import os
api_key = os.getenv('API_KEY')
if not api_key:
    print("API key not set. Export it: export API_KEY='your_key'")
```

#### 3. Rate limitã«å¯¾å‡¦:
```python
import time
from functools import wraps

def rate_limited(max_calls=10, period=60):
    """Rate limiting decorator"""
    calls = []
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            calls[:] = [c for c in calls if c > now - period]
            
            if len(calls) >= max_calls:
                sleep_time = period - (now - calls[0])
                print(f"Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
            
            calls.append(time.time())
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limited(max_calls=10, period=60)
def call_api(url):
    return requests.get(url)
```

#### 4. ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯:
```python
def fetch_with_retry(url, max_retries=3, backoff=5):
    """Exponential backoff retry"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                wait_time = backoff * (2 ** attempt)
                print(f"Retry {attempt + 1}/{max_retries} after {wait_time}s")
                time.sleep(wait_time)
            else:
                raise
```

---

### ğŸŸ  Problem 2: Memory Error with Large Dataset

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
Killed (OOM)
```

**åŸå› **:
- ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€åº¦ã«å…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰
- `float64`ã®éåº¦ãªä½¿ç”¨
- ä¸è¦ãªã‚«ãƒ©ãƒ ã®ä¿æŒ
- ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ãŒå¤šã„

**è§£æ±ºç­–**:

#### 1. Chunk processingã‚’ä½¿ç”¨:
```python
# Instead of:
df = pd.read_csv('large_file.csv')

# Use:
chunk_size = 10000
chunks = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # Process each chunk
    processed = process_chunk(chunk)
    chunks.append(processed)

df = pd.concat(chunks, ignore_index=True)
```

#### 2. dtypeã‚’æœ€é©åŒ–:
```python
# Memory optimization
df['year'] = df['year'].astype('int16')       # int64 â†’ int16 (4x less)
df['firm_id'] = df['firm_id'].astype('category')  # string â†’ category
df['industry'] = df['industry'].astype('category')

# Check memory usage
print(df.memory_usage(deep=True))
```

#### 3. ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤:
```python
# Only load needed columns
df = pd.read_csv('file.csv', usecols=['col1', 'col2', 'col3'])

# Drop columns after use
df = df.drop(columns=['temp_col1', 'temp_col2'])
```

#### 4. In-placeæ“ä½œã‚’ä½¿ç”¨:
```python
# Bad: creates copy
df = df.fillna(0)

# Good: in-place
df.fillna(0, inplace=True)
```

#### 5. Daskã‚’ä½¿ç”¨ï¼ˆè¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰:
```python
import dask.dataframe as dd

# Lazy loading
ddf = dd.read_csv('huge_file.csv')

# Parallel processing
result = ddf.groupby('firm_id').mean().compute()
```

---

### ğŸŸ¡ Problem 3: Text Encoding Issues

**ç—‡çŠ¶**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
UnicodeEncodeError: 'ascii' codec can't encode character
```

**åŸå› **:
- ãƒ•ã‚¡ã‚¤ãƒ«ãŒUTF-8ä»¥å¤–ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ç‰¹æ®Šæ–‡å­—ãƒ»çµµæ–‡å­—ã®å‡¦ç†
- HTML entities

**è§£æ±ºç­–**:

#### 1. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡º:
```python
import chardet

# Detect encoding
with open('file.txt', 'rb') as f:
    result = chardet.detect(f.read(10000))
    encoding = result['encoding']
    print(f"Detected encoding: {encoding}")

# Read with detected encoding
df = pd.read_csv('file.csv', encoding=encoding)
```

#### 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†:
```python
# Ignore errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='ignore')

# Replace errors
df = pd.read_csv('file.csv', encoding='utf-8', errors='replace')

# Best: specify correct encoding
df = pd.read_csv('file.csv', encoding='shift_jis')  # For Japanese
```

#### 3. ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°:
```python
import unicodedata

def clean_text(text):
    """Remove special characters"""
    # Normalize unicode
    text = unicodedata.normalize('NFKD', text)
    # Remove non-ASCII
    text = text.encode('ascii', 'ignore').decode('ascii')
    return text

df['text'] = df['text'].apply(clean_text)
```

---

### ğŸŸ¢ Problem 4: Missing Data Handling

**ç—‡çŠ¶**:
- ãƒ¢ãƒ‡ãƒ«ãŒåæŸã—ãªã„
- çµ±è¨ˆæ¤œå®šã§å¥‡å¦™ãªçµæœ
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå¤§å¹…ã«æ¸›å°‘

**åŸå› **:
- æ¬ æå€¤ã®ä¸é©åˆ‡ãªå‡¦ç†
- Listwise deletionï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
- æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç„¡è¦–

**è§£æ±ºç­–**:

#### 1. æ¬ æå€¤ã‚’ç¢ºèª:
```python
# Missing value summary
missing_summary = pd.DataFrame({
    'column': df.columns,
    'missing_count': df.isnull().sum(),
    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)
})

print(missing_summary[missing_summary['missing_count'] > 0])

# Visualize missing pattern
import missingno as msno
msno.matrix(df)
plt.show()
```

#### 2. é©åˆ‡ãªè£œå®Œæ–¹æ³•ã‚’é¸æŠ:
```python
# Mean imputation (é€£ç¶šå¤‰æ•°)
df['revenue'].fillna(df['revenue'].mean(), inplace=True)

# Median imputation (å¤–ã‚Œå€¤ãŒã‚ã‚‹å ´åˆ)
df['revenue'].fillna(df['revenue'].median(), inplace=True)

# Forward fill (æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿)
df['price'] = df.groupby('firm_id')['price'].fillna(method='ffill')

# Industry mean (ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å¹³å‡)
df['leverage'] = df.groupby('industry')['leverage'].transform(
    lambda x: x.fillna(x.mean())
)
```

#### 3. æ¬ æãƒ•ãƒ©ã‚°ã‚’ä½œæˆ:
```python
# Create missing indicator
df['revenue_missing'] = df['revenue'].isnull().astype(int)

# Then impute
df['revenue'].fillna(0, inplace=True)
```

---

### ğŸ”µ Problem 5: Slow Processing / Performance

**ç—‡çŠ¶**:
- ã‚³ãƒ¼ãƒ‰ãŒæ•°æ™‚é–“ã‹ã‹ã‚‹
- CPUãŒ100%ã§å›ºã¾ã‚‹
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãŒå‹•ã‹ãªã„

**è§£æ±ºç­–**:

#### 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š:
```python
import time

# Simple timing
start = time.time()
result = slow_function()
print(f"Elapsed: {time.time() - start:.2f}s")

# Line profiler
%load_ext line_profiler
%lprun -f slow_function slow_function()
```

#### 2. Vectorization ã‚’ä½¿ç”¨:
```python
# Bad: Loop
for i in range(len(df)):
    df.loc[i, 'result'] = df.loc[i, 'a'] * df.loc[i, 'b']

# Good: Vectorized
df['result'] = df['a'] * df['b']
```

#### 3. ä¸¦åˆ—å‡¦ç†:
```python
from multiprocessing import Pool

def process_firm(firm_id):
    # Heavy computation
    return result

# Parallel processing
with Pool(processes=4) as pool:
    results = pool.map(process_firm, firm_ids)
```

#### 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼:
```python
from tqdm import tqdm

# Add progress bar
for item in tqdm(items, desc="Processing"):
    process(item)
```

---

### ğŸ“š General Debugging Tips

#### 1. ãƒ‡ãƒ¼ã‚¿ã®å“è³ªç¢ºèª:
```python
# Quick data check
def check_data_quality(df):
    print(f"Shape: {df.shape}")
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nMissing:\n{df.isnull().sum()}")
    print(f"\nDuplicates: {df.duplicated().sum()}")
    print(f"\nSummary:\n{df.describe()}")

check_data_quality(df)
```

#### 2. Small sampleã§ãƒ†ã‚¹ãƒˆ:
```python
# Test with small sample first
df_sample = df.head(100)
result = your_function(df_sample)

# If works, run on full data
result = your_function(df)
```

#### 3. ãƒ­ã‚°ã‚’å‡ºåŠ›:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

logger.info("Starting process...")
logger.warning("Missing data detected")
logger.error("API call failed")
```

---

### ğŸ†˜ When to Ask for Help

**Stack Overflowå‰ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:
1. âœ… ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Œå…¨ã«èª­ã‚“ã ã‹ï¼Ÿ
2. âœ… Googleæ¤œç´¢ã—ãŸã‹ï¼Ÿ
3. âœ… å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
4. âœ… Small exampleã§å†ç¾ã§ãã‚‹ã‹ï¼Ÿ
5. âœ… ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ

**è³ªå•ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**:
```
ã€ç’°å¢ƒã€‘
- OS: macOS 14.0
- Python: 3.11.5
- pandas: 2.0.3

ã€å•é¡Œã€‘
[ç°¡æ½”ãªèª¬æ˜]

ã€å†ç¾ã‚³ãƒ¼ãƒ‰ã€‘
[æœ€å°é™ã®å®Ÿè¡Œå¯èƒ½ã‚³ãƒ¼ãƒ‰]

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
[å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯]

ã€è©¦ã—ãŸã“ã¨ã€‘
1. [è©¦ã—ãŸå¯¾å‡¦æ³•1] â†’ [çµæœ]
2. [è©¦ã—ãŸå¯¾å‡¦æ³•2] â†’ [çµæœ]
```

---

**Version**: 4.0  
**Last Updated**: 2025-11-01
