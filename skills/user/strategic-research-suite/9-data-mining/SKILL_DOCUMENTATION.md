---
name: strategic-research-data-mining
description: Data mining methods for strategic management research including clustering (firm grouping), dimensionality reduction (PCA, t-SNE), classification (bankruptcy prediction), anomaly detection (fraud detection), and time series analysis (strategy change points).
version: 4.0
part_of: strategic-research-suite
related_skills:
  - core-workflow: Phase 6 (Variable Construction)
  - statistical-methods: Feature engineering and model validation
  - text-analysis: Text features for clustering
  - network-analysis: Network features for classification
---

# Data Mining Toolkit for Strategic Research v4.0

**Part of**: [Strategic Research Suite v4.0](../README.md)

---

## ğŸ¯ ã“ã®ã‚¹ã‚­ãƒ«ã«ã¤ã„ã¦

æˆ¦ç•¥çµŒå–¶ç ”ç©¶ã®ãŸã‚ã®**ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•**ã‚’æä¾›ã—ã¾ã™ã€‚ä¼æ¥­ã®é¡å‹åŒ–ã€ç ´ç¶»äºˆæ¸¬ã€ç•°å¸¸æ¤œçŸ¥ã€æˆ¦ç•¥è»¢æ›ç‚¹ã®ç‰¹å®šãªã©ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹ã¨äºˆæ¸¬ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

### ã„ã¤ä½¿ã†ã‹

âœ… **ä¼æ¥­ã‚°ãƒ«ãƒ¼ãƒ—ã®è­˜åˆ¥**
- æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆ†é¡
- é¡ä¼¼ä¼æ¥­ã®ç™ºè¦‹
- å¸‚å ´ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

âœ… **äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**
- ç ´ç¶»ãƒ»M&Aäºˆæ¸¬
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬
- æˆ¦ç•¥å¤‰æ›´ã®äºˆæ¸¬

âœ… **ç•°å¸¸æ¤œçŸ¥**
- ä¸æ­£ä¼šè¨ˆã®æ¤œå‡º
- ç•°å¸¸ä¼æ¥­ã®è­˜åˆ¥
- ãƒªã‚¹ã‚¯ä¼æ¥­ã®æ—©æœŸç™ºè¦‹

âœ… **ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹**
- æˆ¦ç•¥è»¢æ›ç‚¹ã®æ¤œå‡º
- æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è­˜åˆ¥
- é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–

### å‰ææ¡ä»¶

**å¿…é ˆçŸ¥è­˜**:
- PythonåŸºç¤ï¼ˆpandas, numpyï¼‰
- åŸºæœ¬çš„ãªçµ±è¨ˆçŸ¥è­˜
- æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µ

**æ¨å¥¨çŸ¥è­˜**:
- scikit-learnä½¿ç”¨çµŒé¨“
- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãƒ»ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- ãƒ¢ãƒ‡ãƒ«è©•ä¾¡æŒ‡æ¨™ã®ç†è§£

---

## ğŸ“‹ ç›®æ¬¡

1. [Methods Overview](#methods-overview)
2. [Clustering](#clustering)
3. [Dimensionality Reduction](#dimensionality-reduction)
4. [Classification](#classification)
5. [Anomaly Detection](#anomaly-detection)
6. [Time Series Analysis](#time-series-analysis)
7. [Quick Start](#quick-start)
8. [Best Practices](#best-practices)
9. [Common Pitfalls](#common-pitfalls)
10. [FAQ](#faq)

---

## Methods Overview

### ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®å…¨ä½“åƒ

```
Data Mining Methods
â”‚
â”œâ”€â”€ Clustering (ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)
â”‚   â”œâ”€â”€ K-means
â”‚   â”œâ”€â”€ Hierarchical Clustering
â”‚   â”œâ”€â”€ DBSCAN
â”‚   â””â”€â”€ Gaussian Mixture Models
â”‚
â”œâ”€â”€ Dimensionality Reduction (æ¬¡å…ƒå‰Šæ¸›)
â”‚   â”œâ”€â”€ PCA (Principal Component Analysis)
â”‚   â”œâ”€â”€ Factor Analysis
â”‚   â”œâ”€â”€ t-SNE
â”‚   â””â”€â”€ UMAP
â”‚
â”œâ”€â”€ Classification (åˆ†é¡)
â”‚   â”œâ”€â”€ Random Forest
â”‚   â”œâ”€â”€ Gradient Boosting (XGBoost, LightGBM)
â”‚   â”œâ”€â”€ Support Vector Machine
â”‚   â””â”€â”€ Logistic Regression
â”‚
â”œâ”€â”€ Anomaly Detection (ç•°å¸¸æ¤œçŸ¥)
â”‚   â”œâ”€â”€ Isolation Forest
â”‚   â”œâ”€â”€ Local Outlier Factor (LOF)
â”‚   â”œâ”€â”€ One-Class SVM
â”‚   â””â”€â”€ Statistical methods (Z-score, IQR)
â”‚
â””â”€â”€ Time Series Analysis (æ™‚ç³»åˆ—åˆ†æ)
    â”œâ”€â”€ ARIMA
    â”œâ”€â”€ VAR (Vector Autoregression)
    â”œâ”€â”€ Change Point Detection
    â””â”€â”€ Trend decomposition
```


---

## Clustering

### ç›®çš„

é¡ä¼¼ã—ãŸç‰¹å¾´ã‚’æŒã¤ä¼æ¥­ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€**æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—**ã‚’è­˜åˆ¥ã—ã¾ã™ã€‚

### æˆ¦ç•¥ç ”ç©¶ã§ã®å¿œç”¨

- **Strategic Groups**: åŒæ§˜ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹ä¼æ¥­ç¾¤ã®è­˜åˆ¥
- **Market Segmentation**: ç•°è³ªãªå¸‚å ´ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ç™ºè¦‹
- **Competitive Positioning**: ç«¶äº‰ãƒã‚¸ã‚·ãƒ§ãƒ³ã®å¯è¦–åŒ–
- **Archetype Identification**: ä¼æ¥­ã‚¿ã‚¤ãƒ—ï¼ˆã‚¤ãƒãƒ™ãƒ¼ã‚¿ãƒ¼ã€è¿½éšè€…ç­‰ï¼‰ã®åˆ†é¡

### æ‰‹æ³•æ¯”è¼ƒ

| æ‰‹æ³• | é©ç”¨å ´é¢ | åˆ©ç‚¹ | æ¬ ç‚¹ |
|------|---------|------|------|
| **K-means** | ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒæ—¢çŸ¥ | é«˜é€Ÿã€è§£é‡ˆå®¹æ˜“ | çƒçŠ¶ã‚¯ãƒ©ã‚¹ã‚¿å‰æ |
| **Hierarchical** | éšå±¤æ§‹é€ ã®å¯è¦–åŒ– | ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ  | è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ |
| **DBSCAN** | ä»»æ„å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ | ãƒã‚¤ã‚ºæ¤œå‡ºå¯èƒ½ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å›°é›£ |
| **GMM** | ç¢ºç‡çš„ã‚¯ãƒ©ã‚¹ã‚¿ | æŸ”è»Ÿãªã‚¯ãƒ©ã‚¹ã‚¿å½¢çŠ¶ | è¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ |

### K-means Clustering

**åŸºæœ¬åŸç†**: ãƒ‡ãƒ¼ã‚¿ã‚’ k å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†å‰²ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®åˆ†æ•£ã‚’æœ€å°åŒ–

**å®Ÿè£…ä¾‹**:

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
df = pd.read_csv('firm_financials.csv')
features = ['roa', 'sales_growth', 'rd_intensity', 'leverage']
X = df[features].dropna()

# æ¨™æº–åŒ–ï¼ˆé‡è¦ï¼ï¼‰
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-meanså®Ÿè¡Œ
kmeans = KMeans(
    n_clusters=3,           # ã‚¯ãƒ©ã‚¹ã‚¿æ•°
    random_state=42,        # å†ç¾æ€§ç¢ºä¿
    n_init=10,              # åˆæœŸå€¤è©¦è¡Œå›æ•°
    max_iter=300            # æœ€å¤§åå¾©å›æ•°
)
labels = kmeans.fit_predict(X_scaled)

# çµæœã‚’DataFrameã«è¿½åŠ 
df['cluster'] = labels

# ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹å¾´ã®ç¢ºèª
cluster_stats = df.groupby('cluster')[features].mean()
print(cluster_stats)
```

**æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®š**:

```python
from sklearn.metrics import silhouette_score

# ã‚¨ãƒ«ãƒœãƒ¼æ³•
inertias = []
silhouettes = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, labels))

# ãƒ—ãƒ­ãƒƒãƒˆ
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.plot(range(2, 11), inertias, 'bo-')
ax1.set_xlabel('Number of clusters (k)')
ax1.set_ylabel('Inertia')
ax1.set_title('Elbow Method')

ax2.plot(range(2, 11), silhouettes, 'ro-')
ax2.set_xlabel('Number of clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Analysis')

plt.tight_layout()
plt.show()
```

**è©•ä¾¡æŒ‡æ¨™**:

- **Silhouette Score**: -1 ~ 1ï¼ˆé«˜ã„ã»ã©è‰¯ã„ã€>0.5ãŒç›®å®‰ï¼‰
- **Davies-Bouldin Index**: 0 ~ âˆï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
- **Calinski-Harabasz Index**: é«˜ã„ã»ã©è‰¯ã„

### Hierarchical Clustering

**åŸºæœ¬åŸç†**: éšå±¤çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµ±åˆãƒ»åˆ†å‰²

**å®Ÿè£…ä¾‹**:

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
hc = AgglomerativeClustering(
    n_clusters=3,
    linkage='ward'  # ward, complete, average, single
)
labels = hc.fit_predict(X_scaled)

# ãƒ‡ãƒ³ãƒ‰ãƒ­ã‚°ãƒ©ãƒ ä½œæˆ
linkage_matrix = linkage(X_scaled, method='ward')

plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.xlabel('Sample Index or (Cluster Size)')
plt.ylabel('Distance')
plt.title('Hierarchical Clustering Dendrogram')
plt.show()
```

**Linkageæ–¹æ³•ã®é¸æŠ**:
- **Ward**: ã‚¯ãƒ©ã‚¹ã‚¿å†…åˆ†æ•£æœ€å°åŒ–ï¼ˆæ¨å¥¨ï¼‰
- **Complete**: æœ€å¤§è·é›¢æœ€å°åŒ–ï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆï¼‰
- **Average**: å¹³å‡è·é›¢ä½¿ç”¨ï¼ˆãƒãƒ©ãƒ³ã‚¹ï¼‰
- **Single**: æœ€å°è·é›¢ä½¿ç”¨ï¼ˆãƒã‚§ã‚¤ãƒ³å½¢æˆæ³¨æ„ï¼‰

### DBSCAN

**åŸºæœ¬åŸç†**: å¯†åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€ä»»æ„å½¢çŠ¶å¯¾å¿œ

**å®Ÿè£…ä¾‹**:

```python
from sklearn.cluster import DBSCAN

# DBSCANå®Ÿè¡Œ
dbscan = DBSCAN(
    eps=0.5,          # è¿‘å‚åŠå¾„
    min_samples=5     # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
)
labels = dbscan.fit_predict(X_scaled)

# ãƒã‚¤ã‚ºç‚¹ã®è­˜åˆ¥ï¼ˆlabel=-1ï¼‰
n_noise = (labels == -1).sum()
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

print(f"Clusters: {n_clusters}, Noise points: {n_noise}")
```

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**:
- `eps`: å°ã•ã„â†’å¤šãã®å°ã‚¯ãƒ©ã‚¹ã‚¿ã€å¤§ãã„â†’å°‘æ•°ã®å¤§ã‚¯ãƒ©ã‚¹ã‚¿
- `min_samples`: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®1-2%ãŒç›®å®‰


---

## Dimensionality Reduction

### ç›®çš„

é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’2-3æ¬¡å…ƒã«åœ§ç¸®ã—ã€å¯è¦–åŒ–ãƒ»ç†è§£ã‚’å®¹æ˜“ã«ã—ã¾ã™ã€‚

### æ‰‹æ³•æ¯”è¼ƒ

| æ‰‹æ³• | ç›®çš„ | ç·šå½¢/éç·šå½¢ | è¨ˆç®—é€Ÿåº¦ |
|------|------|------------|---------|
| **PCA** | åˆ†æ•£æœ€å¤§åŒ– | ç·šå½¢ | é«˜é€Ÿ |
| **t-SNE** | è¿‘å‚ä¿å­˜ | éç·šå½¢ | é…ã„ |
| **UMAP** | ãƒˆãƒãƒ­ã‚¸ãƒ¼ä¿å­˜ | éç·šå½¢ | ä¸­é€Ÿ |
| **Factor Analysis** | æ½œåœ¨å› å­ç™ºè¦‹ | ç·šå½¢ | ä¸­é€Ÿ |

### PCA (Principal Component Analysis)

**å®Ÿè£…ä¾‹**:

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# PCAå®Ÿè¡Œ
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# èª¬æ˜ã•ã‚Œã‚‹åˆ†æ•£
print(f"Explained variance ratio: {pca.explained_variance_ratio_}")
print(f"Cumulative variance: {pca.explained_variance_ratio_.sum():.2%}")

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.title('PCA Visualization')
plt.show()

# ä¸»æˆåˆ†ã®è§£é‡ˆ
components_df = pd.DataFrame(
    pca.components_,
    columns=features,
    index=['PC1', 'PC2']
)
print(components_df)
```

### t-SNE

**å®Ÿè£…ä¾‹**:

```python
from sklearn.manifold import TSNE

# t-SNEå®Ÿè¡Œï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
tsne = TSNE(
    n_components=2,
    random_state=42,
    perplexity=30,      # 5-50ãŒä¸€èˆ¬çš„
    learning_rate=200   # 10-1000
)
X_tsne = tsne.fit_transform(X_scaled)

# ã‚¯ãƒ©ã‚¹ã‚¿ã¨çµ„ã¿åˆã‚ã›ã¦å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(
    X_tsne[:, 0], X_tsne[:, 1], 
    c=labels, cmap='viridis', alpha=0.6
)
plt.colorbar(scatter, label='Cluster')
plt.title('t-SNE Visualization with Clusters')
plt.show()
```

---

## Classification

### ç›®çš„

ä¼æ¥­ã‚’äºˆã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ï¼ˆç ´ç¶»äºˆæ¸¬ã€M&Aäºˆæ¸¬ç­‰ï¼‰ã€‚

### Random Forest

**å®Ÿè£…ä¾‹**:

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = df[['roa', 'leverage', 'current_ratio', 'zscore']]
y = df['bankrupt']  # 0/1ãƒ©ãƒ™ãƒ«

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Random Forest
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    random_state=42,
    class_weight='balanced'  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–
)
rf.fit(X_train, y_train)

# äºˆæ¸¬ã¨è©•ä¾¡
y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred))

# ç‰¹å¾´é‡é‡è¦åº¦
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance)
```

### XGBoost

**å®Ÿè£…ä¾‹**:

```python
import xgboost as xgb

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])  # ä¸å‡è¡¡å¯¾ç­–
)
xgb_model.fit(X_train, y_train)

# äºˆæ¸¬ç¢ºç‡
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# ROC-AUC
from sklearn.metrics import roc_auc_score, roc_curve

auc = roc_auc_score(y_test, y_proba)
print(f"AUC: {auc:.3f}")

# ROCæ›²ç·š
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

---

## Anomaly Detection

### ç›®çš„

é€šå¸¸ã¨ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ä¼æ¥­ã‚’æ¤œå‡ºï¼ˆä¸æ­£ä¼šè¨ˆã€ç•°å¸¸å€¤ç­‰ï¼‰ã€‚

### Isolation Forest

**å®Ÿè£…ä¾‹**:

```python
from sklearn.ensemble import IsolationForest

# Isolation Forest
iso_forest = IsolationForest(
    contamination=0.1,  # ç•°å¸¸å€¤ã®å‰²åˆï¼ˆ10%ï¼‰
    random_state=42
)
anomaly_labels = iso_forest.fit_predict(X_scaled)

# ç•°å¸¸ã‚¹ã‚³ã‚¢
anomaly_scores = iso_forest.score_samples(X_scaled)

# ç•°å¸¸ä¼æ¥­ã®æŠ½å‡º
df['anomaly'] = anomaly_labels
df['anomaly_score'] = anomaly_scores

anomalies = df[df['anomaly'] == -1].sort_values('anomaly_score')
print(f"Detected {len(anomalies)} anomalies")
print(anomalies[['firm_name', 'anomaly_score', 'roa', 'leverage']].head())
```

### Statistical Methods

**Z-scoreæ³•**:

```python
from scipy import stats

# Z-scoreè¨ˆç®—
z_scores = np.abs(stats.zscore(df[features]))

# ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°ï¼ˆZ-score > 3ï¼‰
df['is_outlier'] = (z_scores > 3).any(axis=1)

print(f"Outliers: {df['is_outlier'].sum()}")
```

---

## Time Series Analysis

### ç›®çš„

æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æˆ¦ç•¥è»¢æ›ç‚¹ã‚’æ¤œå‡ºã€ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’äºˆæ¸¬ã€‚

### Change Point Detection

**å®Ÿè£…ä¾‹**:

```python
import ruptures as rpt

# ä¼æ¥­ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
firm_data = df[df['firm_id'] == 1].sort_values('year')
signal = firm_data['roa'].values

# Change point detection
model = rpt.Pelt(model="rbf").fit(signal)
change_points = model.predict(pen=10)

# å¯è¦–åŒ–
plt.figure(figsize=(12, 4))
plt.plot(firm_data['year'], signal, label='ROA')
for cp in change_points[:-1]:
    plt.axvline(x=firm_data['year'].iloc[cp], color='r', 
                linestyle='--', label='Change Point' if cp == change_points[0] else '')
plt.xlabel('Year')
plt.ylabel('ROA')
plt.title('Strategy Change Point Detection')
plt.legend()
plt.show()
```

### ARIMA Forecasting

**å®Ÿè£…ä¾‹**:

```python
from statsmodels.tsa.arima.model import ARIMA

# ARIMAãƒ¢ãƒ‡ãƒ«
model = ARIMA(signal, order=(1, 1, 1))
fitted = model.fit()

# äºˆæ¸¬
forecast = fitted.forecast(steps=3)
print(f"3-year forecast: {forecast}")
```

---

## Quick Start

### Example 1: ä¼æ¥­ã‚’3ã¤ã®æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('firm_financials.csv')

# ç‰¹å¾´é‡é¸æŠ
features = ['roa', 'rd_intensity', 'leverage', 'firm_size']
X = df[features].dropna()

# æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
kmeans = KMeans(n_clusters=3, random_state=42)
df['strategic_group'] = kmeans.fit_predict(X_scaled)

# çµæœç¢ºèª
print(df.groupby('strategic_group')[features].mean())
```

### Example 2: ç ´ç¶»ä¼æ¥­ã‚’äºˆæ¸¬

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X = df[['zscore', 'current_ratio', 'leverage', 'profitability']]
y = df['bankrupt_next_year']

# Train/Teståˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# è©•ä¾¡
from sklearn.metrics import classification_report
y_pred = rf.predict(X_test)
print(classification_report(y_test, y_pred))
```


---

## Best Practices

### 1. Feature Engineering

**è²¡å‹™æ¯”ç‡ã®è¨ˆç®—**:
```python
# åç›Šæ€§
df['roa'] = df['net_income'] / df['total_assets']
df['roe'] = df['net_income'] / df['total_equity']
df['profit_margin'] = df['net_income'] / df['sales']

# åŠ¹ç‡æ€§
df['asset_turnover'] = df['sales'] / df['total_assets']
df['inventory_turnover'] = df['cogs'] / df['inventory']

# å®‰å…¨æ€§
df['current_ratio'] = df['current_assets'] / df['current_liabilities']
df['debt_to_equity'] = df['total_debt'] / df['total_equity']

# æˆé•·æ€§
df['sales_growth'] = df.groupby('firm_id')['sales'].pct_change()
df['asset_growth'] = df.groupby('firm_id')['total_assets'].pct_change()
```

**æ¥­ç•Œèª¿æ•´**:
```python
# æ¥­ç•Œå¹³å‡ã¨ã®å·®åˆ†
industry_means = df.groupby(['industry', 'year'])[features].transform('mean')
df_adjusted = df[features] - industry_means
```

**æ™‚ç³»åˆ—ç‰¹å¾´é‡**:
```python
# ãƒ©ã‚°å¤‰æ•°
df['roa_lag1'] = df.groupby('firm_id')['roa'].shift(1)
df['roa_lag2'] = df.groupby('firm_id')['roa'].shift(2)

# ç§»å‹•å¹³å‡
df['roa_ma3'] = df.groupby('firm_id')['roa'].rolling(window=3).mean().reset_index(0, drop=True)

# å¤‰å‹•ä¿‚æ•°
df['roa_volatility'] = df.groupby('firm_id')['roa'].rolling(window=5).std().reset_index(0, drop=True)
```

### 2. Model Validation

**ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**:
```python
from sklearn.model_selection import cross_val_score

# K-fold CV
scores = cross_val_score(
    rf, X_train, y_train, 
    cv=5,  # 5-fold
    scoring='f1'
)
print(f"CV F1 scores: {scores}")
print(f"Mean F1: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

**æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®CV**:
```python
from sklearn.model_selection import TimeSeriesSplit

# Time series split
tscv = TimeSeriesSplit(n_splits=5)

for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
    
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"Score: {score:.3f}")
```

**ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–**:
```python
from imblearn.over_sampling import SMOTE

# SMOTE (Synthetic Minority Over-sampling)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

print(f"Before SMOTE: {y_train.value_counts()}")
print(f"After SMOTE: {pd.Series(y_resampled).value_counts()}")
```

### 3. Interpretation

**SHAP Values**:
```python
import shap

# SHAPèª¬æ˜
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)

# è¦ç´„ãƒ—ãƒ­ãƒƒãƒˆ
shap.summary_plot(shap_values[1], X_test, feature_names=X.columns)

# å€‹åˆ¥äºˆæ¸¬ã®èª¬æ˜
shap.force_plot(explainer.expected_value[1], shap_values[1][0], X_test.iloc[0])
```

**Partial Dependence Plot**:
```python
from sklearn.inspection import partial_dependence, PartialDependenceDisplay

# PDPä½œæˆ
features_to_plot = [0, 1]  # Feature indices
PartialDependenceDisplay.from_estimator(rf, X_train, features_to_plot)
plt.show()
```

---

## Common Pitfalls

### 1. Look-ahead Bias (æœªæ¥æƒ…å ±ã®ä½¿ç”¨)

âŒ **æ‚ªã„ä¾‹**:
```python
# å¹´åº¦tã®å€’ç”£ã‚’äºˆæ¸¬ã™ã‚‹ã®ã«ã€å¹´åº¦tã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
X = df[df['year'] == 2020][features]
y = df[df['year'] == 2020]['bankrupt']  # 2020å¹´ã®å€’ç”£
```

âœ… **è‰¯ã„ä¾‹**:
```python
# å¹´åº¦t-1ã®ãƒ‡ãƒ¼ã‚¿ã§å¹´åº¦tã®å€’ç”£ã‚’äºˆæ¸¬
df['bankrupt_next_year'] = df.groupby('firm_id')['bankrupt'].shift(-1)
X = df[df['year'] == 2019][features]
y = df[df['year'] == 2019]['bankrupt_next_year']
```

### 2. Data Leakage (æƒ…å ±æ¼æ´©)

âŒ **æ‚ªã„ä¾‹**:
```python
# Train/Teståˆ†å‰²å‰ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
X_scaled = scaler.fit_transform(X)
X_train, X_test = train_test_split(X_scaled)
```

âœ… **è‰¯ã„ä¾‹**:
```python
# Train/Teståˆ†å‰²å¾Œã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
X_train, X_test = train_test_split(X)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # fitä¸è¦
```

### 3. Ignoring Class Imbalance

âŒ **æ‚ªã„ä¾‹**:
```python
# ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆå€’ç”£1%ï¼‰ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
# â†’ å…¨ã¦éå€’ç”£ã¨äºˆæ¸¬ã—ã¦Accuracy 99%é”æˆ
```

âœ… **è‰¯ã„ä¾‹**:
```python
# class_weight='balanced'ã‚’ä½¿ç”¨
rf = RandomForestClassifier(class_weight='balanced')
rf.fit(X_train, y_train)

# ã¾ãŸã¯F1-scoreã§è©•ä¾¡
from sklearn.metrics import f1_score
f1 = f1_score(y_test, y_pred)
```

### 4. Overfitting

âŒ **æ‚ªã„ä¾‹**:
```python
# éåº¦ã«è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«
rf = RandomForestClassifier(
    n_estimators=1000,
    max_depth=None,  # åˆ¶é™ãªã—
    min_samples_split=2
)
# Train accuracy: 100%, Test accuracy: 60%
```

âœ… **è‰¯ã„ä¾‹**:
```python
# é©åˆ‡ãªæ­£å‰‡åŒ–
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10
)
# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§æ¤œè¨¼
```

### 5. Ignoring Temporal Structure

âŒ **æ‚ªã„ä¾‹**:
```python
# ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ«
X_train, X_test = train_test_split(X, shuffle=True)
```

âœ… **è‰¯ã„ä¾‹**:
```python
# æ™‚ç³»åˆ—æ§‹é€ ã‚’ä¿æŒ
train_years = [2010, 2011, 2012, 2013, 2014]
test_years = [2015, 2016]

X_train = df[df['year'].isin(train_years)][features]
X_test = df[df['year'].isin(test_years)][features]
```

---

## FAQ

### Q1: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã¯ã©ã†æ±ºã‚ã‚Œã°ã‚ˆã„ã‹ï¼Ÿ

**A**: ä»¥ä¸‹ã®æ–¹æ³•ã‚’ä½µç”¨ï¼š

1. **ã‚¨ãƒ«ãƒœãƒ¼æ³•**: Inertiaã®æ¸›å°‘ãŒç·©ã‚„ã‹ã«ãªã‚‹ç‚¹
2. **Silhouette Score**: æœ€å¤§å€¤ã‚’ä¸ãˆã‚‹k
3. **ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜**: æ—¢å­˜ç ”ç©¶ã§ã®æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—æ•°
4. **ãƒ“ã‚¸ãƒã‚¹åˆ¤æ–­**: å®Ÿå‹™çš„ã«æ„å‘³ã®ã‚ã‚‹åˆ†å‰²æ•°

```python
# è¤‡æ•°æŒ‡æ¨™ã§è©•ä¾¡
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    
    silhouette = silhouette_score(X_scaled, labels)
    davies_bouldin = davies_bouldin_score(X_scaled, labels)
    
    print(f"k={k}: Silhouette={silhouette:.3f}, DB={davies_bouldin:.3f}")
```

### Q2: è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸç‰¹å¾´é‡ã¯ï¼Ÿ

**A**: 4ã¤ã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰é¸æŠï¼š

1. **åç›Šæ€§**: ROA, ROE, Profit Margin
2. **åŠ¹ç‡æ€§**: Asset Turnover, Inventory Turnover
3. **å®‰å…¨æ€§**: Current Ratio, Debt-to-Equity, Z-score
4. **æˆé•·æ€§**: Sales Growth, Asset Growth

**æ¨å¥¨**: å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰1-2å¤‰æ•°ãšã¤é¸æŠï¼ˆåˆè¨ˆ5-8å¤‰æ•°ï¼‰

### Q3: æ¨™æº–åŒ–ã¯å¿…é ˆã‹ï¼Ÿ

**A**: **å¿…é ˆ**ï¼ˆè·é›¢ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã§ã¯ï¼‰

- K-means, Hierarchical, DBSCAN â†’ å¿…é ˆ
- Random Forest, XGBoost â†’ ä¸è¦ï¼ˆãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
- PCA, t-SNE â†’ å¿…é ˆ

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### Q4: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®åŸºæº–ã¯ï¼Ÿ

**A**: ãƒã‚¤ãƒãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ãŒ**10%æœªæº€**ã®å ´åˆã¯å¯¾ç­–å¿…è¦

**å¯¾ç­–æ–¹æ³•**:
1. `class_weight='balanced'` ä½¿ç”¨
2. SMOTEç­‰ã®ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
3. F1-scoreã€AUCç­‰ã®é©åˆ‡ãªè©•ä¾¡æŒ‡æ¨™ä½¿ç”¨
4. ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒè±Šå¯Œãªå ´åˆï¼‰

### Q5: ç‰¹å¾´é‡ã¯ã„ãã¤å¿…è¦ã‹ï¼Ÿ

**A**: **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®10%ç¨‹åº¦**ãŒç›®å®‰

- ã‚µãƒ³ãƒ—ãƒ«100: ç‰¹å¾´é‡10å€‹ç¨‹åº¦
- ã‚µãƒ³ãƒ—ãƒ«1,000: ç‰¹å¾´é‡100å€‹ç¨‹åº¦

**å¤šã™ãã‚‹å ´åˆ**:
- PCAã§æ¬¡å…ƒå‰Šæ¸›
- Feature Selectionï¼ˆé‡è¦åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
- Regularization (Lasso, Ridge)

### Q6: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã¯ï¼Ÿ

**A**: é€šå¸¸**20-30%**

- å°ãƒ‡ãƒ¼ã‚¿ï¼ˆn<500ï¼‰: 30%
- ä¸­ãƒ‡ãƒ¼ã‚¿ï¼ˆ500<n<5000ï¼‰: 20%
- å¤§ãƒ‡ãƒ¼ã‚¿ï¼ˆn>5000ï¼‰: 10-20%

### Q7: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã¯ï¼Ÿ

**A**: Grid Searchã¾ãŸã¯Random Searchã‚’ä½¿ç”¨

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [10, 20, 30]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

print(f"Best params: {grid_search.best_params_}")
print(f"Best F1: {grid_search.best_score_:.3f}")
```

### Q8: PCAã§ä½•æ¬¡å…ƒã¾ã§å‰Šæ¸›ã™ã¹ãã‹ï¼Ÿ

**A**: **ç´¯ç©å¯„ä¸ç‡80-90%**ã‚’ç›®æ¨™

```python
from sklearn.decomposition import PCA

pca = PCA()
pca.fit(X_scaled)

# ç´¯ç©å¯„ä¸ç‡
cumsum = np.cumsum(pca.explained_variance_ratio_)

# 90%é”æˆã«å¿…è¦ãªæ¬¡å…ƒæ•°
n_components = np.argmax(cumsum >= 0.9) + 1
print(f"Components for 90% variance: {n_components}")
```

### Q9: ç•°å¸¸æ¤œçŸ¥ã®é–¾å€¤ã¯ï¼Ÿ

**A**: **Contamination parameter**ã§èª¿æ•´

- å³ã—ã„åŸºæº–: 1-5%
- æ¨™æº–çš„: 5-10%
- ç·©ã„åŸºæº–: 10-20%

```python
iso_forest = IsolationForest(contamination=0.05)  # 5%ã‚’ç•°å¸¸ã¨ã™ã‚‹
```

### Q10: æ™‚ç³»åˆ—äºˆæ¸¬ã®è©•ä¾¡æŒ‡æ¨™ã¯ï¼Ÿ

**A**: **RMSEã€MAEã€MAPE**ã‚’ä½¿ç”¨

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"RMSE: {rmse:.3f}, MAE: {mae:.3f}, MAPE: {mape:.1f}%")
```

---

## ğŸ”— Related Skills

- **core-workflow**: Phase 6 (Variable Construction)
- **statistical-methods**: Feature engineering and validation
- **text-analysis**: Text features for clustering
- **network-analysis**: Network features for classification

---

## ğŸ“š References

### Books
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An Introduction to Statistical Learning*

### Papers
- Porter, M. E. (1980). Competitive Strategy (Strategic Groups)
- Altman, E. I. (1968). Financial Ratios, Discriminant Analysis and the Prediction of Corporate Bankruptcy

### Libraries
- scikit-learn: https://scikit-learn.org/
- XGBoost: https://xgboost.readthedocs.io/
- SHAP: https://github.com/slundberg/shap

---

**ä½œæˆæ—¥**: 2025-11-01  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 4.0  
**ãƒ¡ãƒ³ãƒ†ãƒŠãƒ¼**: Strategic Research Suite Team
