# Strategic Management Research Hub - Data Mining Practical Guide
================================================================

**Version**: 3.1  
**Last Updated**: 2025-11-01  
**Author**: Strategic Management Research Hub  

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æœ¬æ ¼çš„ãªãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã‚’æ®µéšçš„ã«å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

---

## ğŸ“š ç›®æ¬¡

1. [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ5åˆ†ï¼‰](#1-ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ5åˆ†)
2. [æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ](#2-æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ)
3. [ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬](#3-ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬)
4. [ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ](#4-ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ)
5. [ç•°å¸¸æ¤œçŸ¥ï¼ˆæˆ¦ç•¥çš„ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ï¼‰](#5-ç•°å¸¸æ¤œçŸ¥æˆ¦ç•¥çš„ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢)
6. [å› æœæ¨è«–ï¼ˆMLçµ±åˆï¼‰](#6-å› æœæ¨è«–mlçµ±åˆ)
7. [æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ](#7-æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ)
8. [å®Œå…¨è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼](#8-å®Œå…¨è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼)
9. [è«–æ–‡æŠ•ç¨¿æº–å‚™](#9-è«–æ–‡æŠ•ç¨¿æº–å‚™)
10. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#10-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## 1. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ5åˆ†ï¼‰

### ã€ç›®çš„ã€‘
æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æã®å…¨ä½“åƒã‚’æŠŠæ¡

### ã€å‰ææ¡ä»¶ã€‘
```python
# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
pip install pandas numpy scikit-learn matplotlib seaborn
pip install econml  # å› æœæ¨è«–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install xgboost lightgbm  # é«˜åº¦ãªMLç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install shap  # èª¬æ˜å¯èƒ½AIç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
```

### ã€æœ€é€Ÿå®Ÿè¡Œä¾‹ã€‘

```python
import pandas as pd
from scripts.advanced_strategic_datamining import AdvancedStrategicDataMining

# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df_panel = pd.read_stata('./data/final/analysis_panel.dta')

# 2. ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
dm = AdvancedStrategicDataMining(
    data=df_panel,
    firm_id='gvkey',
    time_var='year',
    output_dir='./quick_output/'
)

# 3. æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼ˆ1ã‚³ãƒãƒ³ãƒ‰ï¼‰
groups = dm.strategic_group_analysis(
    features=['rd_intensity', 'capital_intensity', 'international_sales'],
    n_clusters=4
)

# 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ï¼ˆ1ã‚³ãƒãƒ³ãƒ‰ï¼‰
predictions = dm.predict_firm_performance(
    target='roa',
    features=['rd_intensity', 'firm_size', 'leverage'],
    model_type='ensemble'
)

# 5. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
report_path = dm.generate_comprehensive_report()

print(f"âœ… å®Œäº†ï¼ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
```

**å®Ÿè¡Œæ™‚é–“**: ç´„2-5åˆ†ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ï¼‰  
**å‡ºåŠ›**:
- æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
- ç‰¹å¾´é‡é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
- HTMLãƒ¬ãƒãƒ¼ãƒˆ

---

## 2. æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ

### ã€ç†è«–çš„èƒŒæ™¯ã€‘

**Porter (1980)**: Strategic Groups within Industries  
**ç ”ç©¶ç›®çš„**: åŒä¸€ç”£æ¥­å†…ã§ã€é¡ä¼¼ã®æˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹ä¼æ¥­ç¾¤ï¼ˆæˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã‚’ç‰¹å®š

### ã€ç ”ç©¶ã§ã®ä½¿ç”¨ä¾‹ã€‘

#### ä¾‹1: è£½é€ æ¥­ã®æˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ—ç‰¹å®š

```python
from scripts.advanced_strategic_datamining import AdvancedStrategicDataMining
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
df_panel = pd.read_csv('./data/final/manufacturing_firms.csv')

# åˆ†æå®Ÿè¡Œ
dm = AdvancedStrategicDataMining(
    data=df_panel,
    firm_id='gvkey',
    time_var='year',
    output_dir='./strategic_groups_output/'
)

# æˆ¦ç•¥çš„æ¬¡å…ƒã®é¸æŠï¼ˆç†è«–é§†å‹•ï¼‰
strategic_dimensions = [
    'rd_intensity',          # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥
    'advertising_intensity', # å·®åˆ¥åŒ–æˆ¦ç•¥
    'capital_intensity',     # è³‡æœ¬é›†ç´„åº¦
    'vertical_integration',  # å‚ç›´çµ±åˆ
    'international_sales'    # å›½éš›åŒ–æˆ¦ç•¥
]

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
results = dm.strategic_group_analysis(
    features=strategic_dimensions,
    n_clusters=None,  # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’è‡ªå‹•æ±ºå®š
    method='kmeans',
    optimal_k_method='silhouette',
    max_k=8
)

# çµæœã®è§£é‡ˆ
print("ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ:")
print(results['cluster_profiles'])

# å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´åˆ†æ
for idx, row in results['cluster_profiles'].iterrows():
    print(f"\næˆ¦ç•¥çš„ã‚°ãƒ«ãƒ¼ãƒ— {row['cluster']}:")
    print(f"  ä¼æ¥­æ•°: {row['size']} ({row['size_pct']:.1f}%)")
    print(f"  R&Dé›†ç´„åº¦: {row['rd_intensity_mean']:.3f}")
    print(f"  åºƒå‘Šé›†ç´„åº¦: {row['advertising_intensity_mean']:.3f}")
    # ... ä»–ã®æ¬¡å…ƒ
```

#### ä¾‹2: ã‚°ãƒ«ãƒ¼ãƒ—é–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

```python
# ã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦ã‚’ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
df_panel['strategic_group'] = results['cluster_labels']

# ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
group_performance = df_panel.groupby('strategic_group').agg({
    'roa': ['mean', 'std', 'count'],
    'sales_growth': ['mean', 'std']
}).round(4)

print("\nã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
print(group_performance)

# çµ±è¨ˆçš„æ¤œå®šï¼ˆANOVAï¼‰
from scipy.stats import f_oneway

groups = [
    df_panel[df_panel['strategic_group'] == i]['roa'].dropna() 
    for i in range(results['n_clusters'])
]
f_stat, p_value = f_oneway(*groups)

print(f"\nANOVAçµæœ:")
print(f"Fçµ±è¨ˆé‡: {f_stat:.2f}, på€¤: {p_value:.4f}")
if p_value < 0.05:
    print("â†’ ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§æœ‰æ„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ã‚ã‚Š")
```

### ã€è«–æ–‡ã§ã®å ±å‘Šæ–¹æ³•ã€‘

**Table X: Strategic Group Profiles**

| Strategic Group | N | R&D Intensity | Advertising Int. | Capital Int. | ROA (Mean) |
|-----------------|---|---------------|------------------|--------------|------------|
| Group 1 (Innovators) | 85 | 0.087 | 0.023 | 0.412 | 0.064 |
| Group 2 (Cost Leaders) | 132 | 0.015 | 0.008 | 0.581 | 0.052 |
| Group 3 (Differentiators) | 98 | 0.042 | 0.095 | 0.338 | 0.071 |
| Group 4 (Integrators) | 67 | 0.038 | 0.031 | 0.694 | 0.058 |

**Figure X**: Strategic Groups (PCA Projection) â†’ `strategic_groups_pca.png`

---

## 3. ä¼æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹äºˆæ¸¬

### ã€ç†è«–çš„èƒŒæ™¯ã€‘

**Resource-Based View (Barney, 1991)**: ä¼æ¥­å›ºæœ‰è³‡æºãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ±ºå®š  
**ç ”ç©¶ç›®çš„**: æˆ¦ç•¥çš„é¸æŠã‹ã‚‰å°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

### ã€ç ”ç©¶ã§ã®ä½¿ç”¨ä¾‹ã€‘

#### ä¾‹1: R&DæŠ•è³‡ã®åŠ¹æœäºˆæ¸¬

```python
# å°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆ2å¹´å¾ŒROAï¼‰ã‚’äºˆæ¸¬
prediction_results = dm.predict_firm_performance(
    target='roa_lead2',  # 2å¹´å¾Œã®ROA
    features=[
        'rd_intensity_lag1',     # 1æœŸãƒ©ã‚°R&D
        'firm_size_lag1',
        'leverage_lag1',
        'firm_age',
        'industry_concentration',
        'env_dynamism',
        'patent_stock'
    ],
    model_type='ensemble',  # Random Forest + Gradient Boosting + XGBoost
    test_size=0.2,
    cv_folds=5,
    tune_hyperparameters=True
)

# ãƒ¢ãƒ‡ãƒ«æ€§èƒ½
best_model = prediction_results['best_model']
metrics = prediction_results['all_results'][best_model]['metrics']

print(f"\nãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {best_model}")
print(f"ãƒ†ã‚¹ãƒˆRÂ²: {metrics['test_r2']:.4f}")
print(f"ãƒ†ã‚¹ãƒˆRMSE: {metrics['test_rmse']:.4f}")
print(f"CV RÂ² (å¹³å‡Â±æ¨™æº–åå·®): {metrics['cv_r2_mean']:.4f} Â± {metrics['cv_r2_std']:.4f}")
```

#### ä¾‹2: ç‰¹å¾´é‡é‡è¦åº¦ã®è§£é‡ˆ

```python
# ç‰¹å¾´é‡é‡è¦åº¦
importance_df = prediction_results['feature_importance']

print("\nTop 5 Most Important Features:")
print(importance_df.head().to_string(index=False))

# ç†è«–çš„è§£é‡ˆã‚’è¿½åŠ 
interpretation = {
    'rd_intensity_lag1': 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æŠ•è³‡ãŒå°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€å¤§ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ï¼ˆRBVæ”¯æŒï¼‰',
    'firm_size_lag1': 'è¦æ¨¡ã®çµŒæ¸ˆæ€§ãƒ»è³‡æºè±Šå¯Œæ€§',
    'patent_stock': 'è“„ç©ã•ã‚ŒãŸçŸ¥è­˜è³‡ç”£ï¼ˆDynamic Capabilitiesï¼‰',
    # ...
}

for feat in importance_df.head()['feature']:
    print(f"{feat}: {interpretation.get(feat, 'N/A')}")
```

### ã€è«–æ–‡ã§ã®å ±å‘Šæ–¹æ³•ã€‘

**Table X: Performance Prediction Results**

| Model | Test RÂ² | RMSE | CV RÂ² (Mean) | CV RÂ² (SD) |
|-------|---------|------|--------------|------------|
| Random Forest | 0.387 | 0.042 | 0.351 | 0.028 |
| Gradient Boosting | 0.412 | 0.038 | 0.389 | 0.032 |
| **XGBoost** | **0.438** | **0.035** | **0.407** | **0.024** |

**ç ”ç©¶ã¸ã®ç¤ºå”†**:
ã€ŒXGBoostãƒ¢ãƒ‡ãƒ«ã¯ã€1æœŸãƒ©ã‚°ã®R&D intensityã‹ã‚‰2å¹´å¾Œã®ROAã‚’ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§RÂ²=0.438ã®ç²¾åº¦ã§äºˆæ¸¬ã—ãŸã€‚ã“ã‚Œã¯ã€R&DæŠ•è³‡ãŒå°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¼·åŠ›ãªäºˆæ¸¬å› å­ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã€RBVã®äºˆæ¸¬ã‚’æ”¯æŒã™ã‚‹ã€‚ã€

---

## 4. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

### ã€ç›®çš„ã€‘
ã©ã®æˆ¦ç•¥çš„å¤‰æ•°ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹ã‚’å®šé‡åŒ–

### ã€å®Ÿè¡Œä¾‹ã€‘

```python
importance_results = dm.analyze_feature_importance(
    target='roa',
    features=[
        'rd_intensity', 'advertising_intensity', 'capital_intensity',
        'firm_size', 'leverage', 'firm_age', 'diversification',
        'international_ratio', 'alliance_count', 'ma_experience'
    ],
    method='ensemble',  # Random Forest + Gradient Boosting
    top_n=10
)

# çµæœã®å¯è¦–åŒ–
# â†’ è‡ªå‹•ç”Ÿæˆ: feature_importance_plot.png

# ç†è«–çš„å«æ„ã®è­°è«–
top_3 = importance_results.head(3)
print("\nTop 3å¤‰æ•°ã®ç†è«–çš„è§£é‡ˆ:")
for idx, row in top_3.iterrows():
    print(f"{row['feature']}: é‡è¦åº¦ {row['ensemble_importance']:.3f}")
    # ç†è«–çš„è§£é‡ˆã‚’è¿½åŠ ...
```

---

## 5. ç•°å¸¸æ¤œçŸ¥ï¼ˆæˆ¦ç•¥çš„ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ï¼‰

### ã€ç†è«–çš„æ„ç¾©ã€‘

**ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ä¼æ¥­ã®3é¡å‹**:
1. **ä¾‹å¤–çš„æˆåŠŸä¼æ¥­** (Sustained Competitive Advantage)
2. **å¤±æ•—ãƒªã‚¹ã‚¯ä¼æ¥­** (Early Warning Signal)
3. **ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ** (Measurement Error)

### ã€å®Ÿè¡Œä¾‹ã€‘

```python
outliers = dm.detect_strategic_outliers(
    features=[
        'roa', 'sales_growth', 'rd_intensity', 
        'leverage', 'cash_ratio'
    ],
    method='ensemble',  # Isolation Forest + LOF + Elliptic Envelope
    contamination=0.05,  # æœŸå¾…ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ç‡5%
    save_results=True
)

# ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ä¼æ¥­ã®è©³ç´°åˆ†æ
print(f"\næ¤œå‡ºã•ã‚ŒãŸã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢: {len(outliers)} ä¼æ¥­")
print("\nTop 10 Most Unusual Firms:")
print(outliers.nlargest(10, 'outlier_score'))

# ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£å€™è£œ
exceptional_performers = outliers[outliers['roa'] > 0.15]
print(f"\nä¾‹å¤–çš„æˆåŠŸä¼æ¥­ï¼ˆROA>15%ï¼‰: {len(exceptional_performers)} ç¤¾")
for idx, row in exceptional_performers.iterrows():
    print(f"  {row['firm_name']}: ROA={row['roa']:.1%}, outlier_score={row['outlier_score']:.3f}")
```

### ã€è«–æ–‡ã§ã®æ´»ç”¨ã€‘

1. **å®šé‡ç ”ç©¶**: ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ã‚’é™¤å¤–ã—ã¦robustness check
2. **è³ªçš„ç ”ç©¶**: ä¾‹å¤–ä¼æ¥­ã®è©³ç´°ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£
3. **ç†è«–æ§‹ç¯‰**: ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ã‹ã‚‰æ–°ç†è«–ã®æ‰‹ãŒã‹ã‚Š

---

## 6. å› æœæ¨è«–ï¼ˆMLçµ±åˆï¼‰

### ã€å†…ç”Ÿæ€§å•é¡Œã¸ã®å¯¾å‡¦ã€‘

æˆ¦ç•¥ç ”ç©¶ã®æœ€å¤§ã®èª²é¡Œ: **ç›¸é–¢ â‰  å› æœ**

**å…¸å‹çš„ãªå†…ç”Ÿæ€§æºæ³‰**:
- Selection biasï¼ˆä¼æ¥­ãŒæˆ¦ç•¥ã‚’è‡ªå·±é¸æŠï¼‰
- Omitted variable biasï¼ˆè¦³æ¸¬ä¸èƒ½ãªä¼æ¥­ç‰¹æ€§ï¼‰
- Reverse causalityï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ â†’ æˆ¦ç•¥ï¼‰

### ã€Causal Forestï¼ˆç•°è³ªçš„å‡¦ç½®åŠ¹æœï¼‰ã€‘

#### ç†è«–çš„æ„ç¾©
å¾“æ¥ã®å›å¸°åˆ†æã¯ã€Œå¹³å‡çš„åŠ¹æœã€ã‚’æ¨å®šã€‚ã—ã‹ã—ç¾å®Ÿã«ã¯:
- M&AãŒæœ‰åŠ¹ãªä¼æ¥­ã¨ç„¡åŠ¹ãªä¼æ¥­ãŒæ··åœ¨
- R&DæŠ•è³‡ã®åŠ¹æœã¯ä¼æ¥­èƒ½åŠ›ã«ä¾å­˜

**Causal Forest**ã¯ã€**ã©ã®ä¼æ¥­ã«ã¨ã£ã¦å‡¦ç½®ãŒåŠ¹æœçš„ã‹**ã‚’ç™ºè¦‹

#### å®Ÿè£…ä¾‹

```python
from scripts.ml_causal_inference_integrated import CausalMLIntegration

# å› æœæ¨è«–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
causal = CausalMLIntegration(
    data=df_panel,
    firm_id='gvkey',
    time_var='year',
    output_dir='./causal_output/'
)

# M&Aã®ç•°è³ªçš„åŠ¹æœåˆ†æ
cf_results = causal.causal_forest(
    treatment='ma_dummy',           # M&Aå®Ÿæ–½ï¼ˆ0/1ï¼‰
    outcome='roa_change',           # ROAå¤‰åŒ–
    controls=[
        'firm_size', 'leverage', 'firm_age', 'prior_ma_count'
    ],
    heterogeneity_vars=[            # åŠ¹æœã®ç•°è³ªæ€§ã‚’ç”Ÿã‚€å¤‰æ•°
        'firm_size',
        'rd_intensity',
        'prior_ma_experience',
        'industry_dynamism'
    ],
    discrete_treatment=True,
    n_estimators=100
)

# çµæœã®è§£é‡ˆ
print(f"\nå¹³å‡å‡¦ç½®åŠ¹æœï¼ˆATEï¼‰: {cf_results['ate']:.4f}")
print(f"95% CI: [{cf_results['ate_ci'][0]:.4f}, {cf_results['ate_ci'][1]:.4f}]")

# ã©ã®ä¼æ¥­ç‰¹æ€§ãŒM&AåŠ¹æœã‚’å·¦å³ã™ã‚‹ã‹
print("\nç•°è³ªæ€§ãƒ‰ãƒ©ã‚¤ãƒãƒ¼:")
print(cf_results['feature_importance'])

# ä¼æ¥­è¦æ¨¡åˆ¥ã®åŠ¹æœ
heterogeneity = cf_results['heterogeneity_analysis']
size_effect = heterogeneity['firm_size']
print(f"\nä¼æ¥­è¦æ¨¡ã«ã‚ˆã‚‹åŠ¹æœå·®:")
print(f"  å°è¦æ¨¡ä¼æ¥­: ATE = {size_effect['ate_low']:.4f}")
print(f"  å¤§è¦æ¨¡ä¼æ¥­: ATE = {size_effect['ate_high']:.4f}")
print(f"  å·®: {size_effect['difference']:.4f} (p={size_effect['p_value']:.4f})")
```

#### è«–æ–‡ã§ã®å ±å‘Š

**Table X: Heterogeneous M&A Effects (Causal Forest)**

| Firm Characteristic | Low Group ATE | High Group ATE | Difference | p-value |
|---------------------|---------------|----------------|------------|---------|
| Firm Size | 0.018 | 0.042 | 0.024*** | 0.001 |
| R&D Intensity | 0.023 | 0.038 | 0.015** | 0.012 |
| Prior M&A Experience | 0.015 | 0.045 | 0.030*** | 0.000 |

**ç ”ç©¶çš„ç¤ºå”†**:
ã€ŒCausal Foreståˆ†æã«ã‚ˆã‚Šã€M&Aã®åŠ¹æœã¯ä¼æ¥­è¦æ¨¡ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚å¤§è¦æ¨¡ä¼æ¥­ã®ATEï¼ˆ0.042ï¼‰ã¯å°è¦æ¨¡ä¼æ¥­ï¼ˆ0.018ï¼‰ã®2å€ä»¥ä¸Šã§ã‚ã‚Šã€è³‡æºè±Šå¯Œæ€§ãŒM&Açµ±åˆã‚’ä¿ƒé€²ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã™ã‚‹ï¼ˆRBVï¼‰ã€‚ã€

### ã€Double Machine Learningï¼ˆDMLï¼‰ã€‘

é«˜æ¬¡å…ƒçµ±åˆ¶å¤‰æ•°ä¸‹ã§ã®é ‘å¥ãªå› æœæ¨å®š

```python
# DMLæ¨å®š
dml_results = causal.double_machine_learning(
    treatment='rd_intensity',
    outcome='roa_lead2',
    controls=[
        # å¤šæ•°ã®çµ±åˆ¶å¤‰æ•°ï¼ˆ30å€‹ä»¥ä¸Šã‚‚å¯èƒ½ï¼‰
        'firm_size', 'firm_age', 'leverage', 'cash_ratio',
        'tangibility', 'market_to_book', 'sales_growth',
        'industry_concentration', 'industry_rd_mean',
        'gdp_growth', 'interest_rate', 'exchange_rate',
        # ... é«˜æ¬¡å…ƒã§ã‚‚OK
    ],
    discrete_treatment=False,
    cv_folds=5
)

print(f"\nDMLæ¨å®šçµæœ:")
print(f"ATE: {dml_results['ate']:.4f}")
print(f"SE: {dml_results['ate_stderr']:.4f}")
print(f"p-value: {dml_results['p_value']:.4f}")

# å¾“æ¥ã®OLSã¨æ¯”è¼ƒ
if 'ols_comparison' in dml_results:
    ols = dml_results['ols_comparison']
    print(f"\nOLSæ¯”è¼ƒ:")
    print(f"  OLSä¿‚æ•°: {ols['ols_coef']:.4f} (SE: {ols['ols_se']:.4f})")
    print(f"  DMLä¿‚æ•°: {dml_results['ate']:.4f} (SE: {dml_results['ate_stderr']:.4f})")
    print(f"  â†’ DMLã¯confounding biasã«é ‘å¥")
```

### ã€Synthetic Control Methodã€‘

å˜ä¸€å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆã®ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£

```python
# ä¾‹: Appleã®2014å¹´Beatsè²·åãŒã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¸ãˆãŸå½±éŸ¿
sc_results = causal.synthetic_control(
    treated_unit='AAPL',
    treatment_time='2014-05',
    outcome_var='patent_count',
    donor_pool=['MSFT', 'GOOG', 'AMZN', 'FB', 'NFLX', 'INTC']
)

print(f"\nå‡¦ç½®å¾Œå¹³å‡åŠ¹æœ: {sc_results['ate_post']:.2f} ç‰¹è¨±/å¹´")

# Synthetic controlã®æ§‹æˆ
print("\nSynthetic Control Weights:")
print(sc_results['weights'])
```

---

## 7. æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ

### ã€æˆ¦ç•¥çš„è»Œè·¡ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‘

ä¼æ¥­ã®æˆ¦ç•¥çš„é€²åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¡å‹åŒ–

```python
temporal_results = dm.analyze_temporal_patterns(
    variables=['rd_intensity', 'capital_intensity'],
    method='trajectory_clustering',
    save_results=True
)

# è»Œè·¡ã‚¯ãƒ©ã‚¹ã‚¿ã®è§£é‡ˆ
trajectory_clusters = temporal_results['trajectory_clusters']
print(f"\n{trajectory_clusters['trajectory_cluster'].nunique()} ã¤ã®æˆ¦ç•¥çš„è»Œè·¡ã‚’ç‰¹å®š")

# å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨ä¼æ¥­
for cluster_id in trajectory_clusters['trajectory_cluster'].unique():
    cluster_firms = trajectory_clusters[
        trajectory_clusters['trajectory_cluster'] == cluster_id
    ]
    print(f"\nè»Œè·¡ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {len(cluster_firms)} ä¼æ¥­")
    print(f"  ä»£è¡¨ä¼æ¥­: {cluster_firms['firm_name'].head(3).tolist()}")
```

### ã€æˆ¦ç•¥çš„è»¢æ›ç‚¹æ¤œå‡ºã€‘

ä¼æ¥­ã®æˆ¦ç•¥è»¢æ›ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’ç‰¹å®š

```python
change_points = temporal_results['change_points']
print(f"\n{len(change_points)} ä»¶ã®æˆ¦ç•¥çš„è»¢æ›ç‚¹ã‚’æ¤œå‡º")

# é »åº¦ã®é«˜ã„è»¢æ›å¹´
common_years = change_points['time'].value_counts().head()
print("\nè»¢æ›ç‚¹ãŒé›†ä¸­ã™ã‚‹å¹´:")
print(common_years)
# â†’ æ¥­ç•Œã‚·ãƒ§ãƒƒã‚¯ã€è¦åˆ¶å¤‰æ›´ç­‰ã®å¤–éƒ¨ã‚¤ãƒ™ãƒ³ãƒˆã¨å¯¾å¿œ
```

---

## 8. å®Œå…¨è‡ªå‹•åŒ–ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®å®Ÿè¡Œã€‘

```python
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
import yaml

with open('./scripts/datamining_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# è‡ªå‹•å®Ÿè¡Œ
from scripts.advanced_strategic_datamining import AdvancedStrategicDataMining

dm = AdvancedStrategicDataMining(
    data=df_panel,
    firm_id=config['data']['firm_id'],
    time_var=config['data']['time_var'],
    output_dir=config['project']['output_dir'],
    random_state=config['project']['random_seed']
)

# è¨­å®šã«åŸºã¥ãå…¨åˆ†æã‚’è‡ªå‹•å®Ÿè¡Œ
if config['strategic_groups']['enabled']:
    dm.strategic_group_analysis(
        features=config['strategic_groups']['features'],
        method=config['strategic_groups']['method'],
        n_clusters=config['strategic_groups']['n_clusters']
    )

if config['performance_prediction']['enabled']:
    dm.predict_firm_performance(
        target=config['performance_prediction']['target'],
        features=config['performance_prediction']['features'],
        model_type=config['performance_prediction']['model_type']
    )

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
dm.generate_comprehensive_report()
```

---

## 9. è«–æ–‡æŠ•ç¨¿æº–å‚™

### ã€SMJ/AMJæŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã€‘

#### ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°çµæœã®å ±å‘Šè¦ä»¶

- [ ] **æ–¹æ³•è«–ã®é€æ˜æ€§**
  - ä½¿ç”¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ˜è¨˜ï¼ˆRandom Forest, XGBoostç­‰ï¼‰
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å ±å‘Š
  - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹æ³•

- [ ] **ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æŒ‡æ¨™**
  - RÂ² (è¨“ç·´/ãƒ†ã‚¹ãƒˆ)
  - RMSE, MAE
  - CVæ€§èƒ½ï¼ˆå¹³å‡Â±æ¨™æº–åå·®ï¼‰

- [ ] **Robustness Checks**
  - ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã§ã®çµæœï¼ˆæœ€ä½3ç¨®é¡ï¼‰
  - ä»£æ›¿ã‚µãƒ³ãƒ—ãƒ«ã§ã®çµæœ
  - Outlieré™¤å¤–å¾Œã®çµæœ

- [ ] **ç†è«–çš„è§£é‡ˆ**
  - ç‰¹å¾´é‡é‡è¦åº¦ã®ç†è«–çš„æ„å‘³
  - æ—¢å­˜ç†è«–ã¨ã®å¯¾è©±
  - å®Ÿå‹™çš„ç¤ºå”†

#### å› æœæ¨è«–çµæœã®å ±å‘Šè¦ä»¶

- [ ] **å†…ç”Ÿæ€§ã¸ã®å¯¾å‡¦**
  - å†…ç”Ÿæ€§æºæ³‰ã®æ˜ç¤ºçš„è­°è«–
  - ä½¿ç”¨æ‰‹æ³•ã®ç†è«–çš„æ ¹æ‹ 
  - è­˜åˆ¥æˆ¦ç•¥ã®èª¬æ˜

- [ ] **å‡¦ç½®åŠ¹æœã®å ±å‘Š**
  - ATE/ATT with 95% CI
  - på€¤
  - åŠ¹æœã‚µã‚¤ã‚ºã®è§£é‡ˆ

- [ ] **Balance Diagnostics**ï¼ˆPSMä½¿ç”¨æ™‚ï¼‰
  - ãƒãƒƒãƒãƒ³ã‚°å‰å¾Œã®å…±å¤‰é‡ãƒãƒ©ãƒ³ã‚¹
  - Standardized differences
  - Common supportç¢ºèª

- [ ] **Robustness**
  - Alternative matching methods
  - Different caliper widths
  - Sensitivity analysis

### ã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€‘

```python
# è«–æ–‡ç”¨ã®è¡¨ã‚’LaTeXå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
importance_df.to_latex(
    './output/tables/table_feature_importance.tex',
    index=False,
    float_format="%.3f",
    caption="Feature Importance Rankings",
    label="tab:importance"
)

# å›³ã‚’é«˜è§£åƒåº¦ã§ä¿å­˜ï¼ˆå‡ºç‰ˆç”¨ï¼‰
plt.savefig(
    './output/figures/strategic_groups.pdf',
    dpi=600,
    bbox_inches='tight',
    format='pdf'
)
```

---

## 10. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã€ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºç­–ã€‘

#### ã‚¨ãƒ©ãƒ¼1: `MemoryError: Unable to allocate array`

**åŸå› **: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¤§ãã™ãã¦ãƒ¡ãƒ¢ãƒªä¸è¶³

**è§£æ±ºç­–**:
```python
# ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
chunk_size = 10000
results = []

for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    chunk_result = process_chunk(chunk)
    results.append(chunk_result)

final_result = pd.concat(results)
```

#### ã‚¨ãƒ©ãƒ¼2: `ValueError: array must not contain NaNs`

**åŸå› **: æ¬ æå€¤ãŒæ®‹ã£ã¦ã„ã‚‹

**è§£æ±ºç­–**:
```python
# æ¬ æå€¤ã®å‡¦ç†
df_clean = df.dropna(subset=required_features)

# ã¾ãŸã¯ã€è£œå®Œ
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)
```

#### ã‚¨ãƒ©ãƒ¼3: `LinAlgError: Singular matrix`

**åŸå› **: å®Œå…¨ãªå¤šé‡å…±ç·šæ€§

**è§£æ±ºç­–**:
```python
# VIFãƒã‚§ãƒƒã‚¯
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["feature"] = features
vif_data["VIF"] = [variance_inflation_factor(X.values, i) 
                   for i in range(X.shape[1])]

# VIF > 10 ã®å¤‰æ•°ã‚’é™¤å¤–
features_filtered = vif_data[vif_data["VIF"] < 10]["feature"].tolist()
```

#### ã‚¨ãƒ©ãƒ¼4: `Causal Forest: Not enough treated observations`

**åŸå› **: å‡¦ç½®ç¾¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³

**è§£æ±ºç­–**:
```python
# æœ€ä½è¦ä»¶: å‡¦ç½®ç¾¤ â‰¥ 50 observations
print(f"Treated: {df['treatment'].sum()}")
print(f"Control: {(~df['treatment']).sum()}")

# ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ãªã‚‰ã€PSMã‚„DiDã‚’æ¤œè¨
```

### ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€‘

```python
# ä¸¦åˆ—å‡¦ç†ã®æœ‰åŠ¹åŒ–
dm = AdvancedStrategicDataMining(
    data=df_panel,
    firm_id='gvkey',
    time_var='year',
    output_dir='./output/'
)

# scikit-learnãƒ¢ãƒ‡ãƒ«ã§n_jobs=-1ï¼ˆå…¨CPUã‚³ã‚¢ä½¿ç”¨ï¼‰
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=200,
    n_jobs=-1,  # å…¨CPUã‚³ã‚¢ä½¿ç”¨
    random_state=42
)
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

### ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°æ‰‹æ³•

- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An Introduction to Statistical Learning*. Springer.

### å› æœæ¨è«–

- Pearl, J. (2009). *Causality: Models, Reasoning and Inference* (2nd ed.). Cambridge University Press.
- Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences*. Cambridge University Press.
- Athey, S., & Imbens, G. W. (2016). Recursive partitioning for heterogeneous causal effects. *PNAS*, 113(27), 7353-7360.

### æˆ¦ç•¥çµŒå–¶ç ”ç©¶ã§ã®å¿œç”¨

- Ketchen, D. J., & Shook, C. L. (1996). The application of cluster analysis in strategic management research. *Strategic Management Journal*, 17(6), 441-458.
- Short, J. C., Ketchen, D. J., & Palmer, T. B. (2002). The role of sampling in strategic management research. *Organizational Research Methods*, 5(3), 220-239.

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å®Œèµ°**: å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ä¾‹ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
2. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: è‡ªåˆ†ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«åˆã‚ã›ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
3. **è«–æ–‡åŸ·ç­†**: çµæœã‚’`academic-paper-creation` skillã§æ–‡æ›¸åŒ–
4. **å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: Phase 8ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã‚¬ã‚¤ãƒ‰ã«å¾“ã†

**Support**: strategic-management-research-hub v3.1 skillã«è³ªå•ã—ã¦ãã ã•ã„

---

**Last Updated**: 2025-11-01  
**Version**: 3.1  

#ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚° #æ©Ÿæ¢°å­¦ç¿’ #å› æœæ¨è«– #æˆ¦ç•¥çµŒå–¶ç ”ç©¶ #å®Ÿè¨¼åˆ†æ
