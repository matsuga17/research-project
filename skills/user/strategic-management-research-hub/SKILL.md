---
name: strategic-management-research-hub
description: Advanced empirical research system for strategic management and organizational theory. Comprehensive workflow covering data discovery, collection strategy, panel dataset construction, publication-grade quality assurance (Benford's Law, structural breaks, power analysis), and reproducible documentation. Integrates firm-level data (financial, governance, innovation, competitive dynamics), industry analysis, and theoretical framework development. Supports top-tier journal standards (SMJ, AMJ, OS, ASQ) with complete AEA-compliant documentation and Docker reproducibility. Perfect for competitive strategy, organizational design, resource-based view, dynamic capabilities, and institutional theory research.
version: 3.0
---

# Strategic Management Research Hub v3.0

## ğŸ¯ Overview

äº‹æ¥­æˆ¦ç•¥è«–ï¼ˆBusiness Strategyï¼‰ã¨çµ„ç¹”æˆ¦ç•¥è«–ï¼ˆOrganizational Strategyï¼‰åˆ†é‡ã«ãŠã‘ã‚‹å®šé‡çš„å®Ÿè¨¼ç ”ç©¶ã®ãŸã‚ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã€‚ãƒ‡ãƒ¼ã‚¿ç™ºè¦‹ã‹ã‚‰è«–æ–‡åŸ·ç­†ã¾ã§ã€ç ”ç©¶ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ä½“ç³»çš„ã«ç®¡ç†ã—ã€ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æ²è¼‰åŸºæº–ã‚’æº€ãŸã™ç ”ç©¶ã®å®Ÿç¾ã‚’æ”¯æ´ã—ã¾ã™ã€‚

**ä¸»è¦ç‰¹å¾´**ï¼š
- âœ… **8ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼**ï¼šæ§‹æƒ³â†’ãƒ‡ãƒ¼ã‚¿æ¢ç´¢â†’åé›†â†’å“è³ªä¿è¨¼â†’åˆ†æâ†’ç†è«–æ§‹ç¯‰â†’åŸ·ç­†â†’æŠ•ç¨¿æº–å‚™
- âœ… **æˆ¦ç•¥è«–ç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**ï¼šç«¶äº‰æˆ¦ç•¥ã€çµ„ç¹”èƒ½åŠ›ã€åˆ¶åº¦ç’°å¢ƒã€ç”£æ¥­æ§‹é€ åˆ†æ
- âœ… **Publication-Ready QA**ï¼šçµ±è¨ˆçš„æ¤œå‡ºåŠ›åˆ†æã€Benford's Lawã€æ§‹é€ å¤‰åŒ–æ¤œå®š
- âœ… **å®Œå…¨å†ç¾æ€§**ï¼šAEAæº–æ‹ ã®ãƒ‡ãƒ¼ã‚¿ç³»è­œè¿½è·¡ã€Dockerç’°å¢ƒã€pytestæ¤œè¨¼
- âœ… **ç†è«–æ§‹ç¯‰æ”¯æ´**ï¼šRBVã€Dynamic Capabilitiesã€Institutional Theoryçµ±åˆ
- âœ… **å›½éš›ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸**ï¼šåŒ—ç±³ãƒ»æ¬§å·ãƒ»ã‚¢ã‚¸ã‚¢11ã‚«å›½+ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹

## When to Use This Skill

ä»¥ä¸‹ã®ç ”ç©¶ãƒ†ãƒ¼ãƒã«å–ã‚Šçµ„ã‚€éš›ã«ã“ã®ã‚¹ã‚­ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

### ã€å„ªå…ˆåº¦ï¼šæœ€é«˜ã€‘ç«¶äº‰æˆ¦ç•¥ç ”ç©¶
- æŒç¶šçš„ç«¶äº‰å„ªä½ã®æºæ³‰åˆ†æ
- å·®åˆ¥åŒ–æˆ¦ç•¥vs.ã‚³ã‚¹ãƒˆãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—æˆ¦ç•¥
- ãƒ–ãƒ«ãƒ¼ã‚ªãƒ¼ã‚·ãƒ£ãƒ³æˆ¦ç•¥ãƒ»ä¾¡å€¤å‰µé€ 
- ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æˆ¦ç•¥ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœ
- å‚ç›´çµ±åˆvs.ã‚¢ã‚¦ãƒˆã‚½ãƒ¼ã‚·ãƒ³ã‚°æˆ¦ç•¥

### ã€å„ªå…ˆåº¦ï¼šæœ€é«˜ã€‘çµ„ç¹”èƒ½åŠ›ãƒ»è³‡æºãƒ™ãƒ¼ã‚¹ç ”ç©¶
- Dynamic Capabilitiesï¼ˆå‹•çš„èƒ½åŠ›ï¼‰ã®æ¸¬å®šã¨åŠ¹æœ
- çµ„ç¹”å­¦ç¿’ãƒ»çŸ¥è­˜ç®¡ç†
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ãƒ»R&DåŠ¹ç‡æ€§
- çµ„ç¹”çš„ç›¸è£œæ€§ï¼ˆComplementaritiesï¼‰
- VRINè³‡æºã®ç‰¹å®šã¨æ¸¬å®š

### ã€å„ªå…ˆåº¦ï¼šé«˜ã€‘çµ„ç¹”ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ»æ§‹é€ ç ”ç©¶
- çµ„ç¹”æ§‹é€ ï¼ˆæ©Ÿèƒ½åˆ¥ãƒ»äº‹æ¥­éƒ¨åˆ¶ãƒ»ãƒãƒˆãƒªã‚¯ã‚¹ï¼‰ã¨æ¥­ç¸¾
- é›†æ¨©åŒ–vs.åˆ†æ¨©åŒ–ã®åŠ¹æœ
- ã‚¹ãƒ‘ãƒ³ã‚ªãƒ–ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«æœ€é©åŒ–
- çµ„ç¹”ã®æŸ”è»Ÿæ€§ãƒ»é©å¿œæ€§

### ã€å„ªå…ˆåº¦ï¼šé«˜ã€‘åˆ¶åº¦ç†è«–ãƒ»ç’°å¢ƒé©å¿œç ”ç©¶
- åˆ¶åº¦çš„åŒå‹åŒ–ï¼ˆIsomorphismï¼‰ã®å®Ÿè¨¼
- æ­£å½“æ€§ç²å¾—æˆ¦ç•¥
- åˆ¶åº¦çš„èµ·æ¥­å®¶ç²¾ç¥
- Cross-country institutional differences

### ã€å„ªå…ˆåº¦ï¼šä¸­ã€‘å¤šè§’åŒ–ãƒ»å›½éš›åŒ–æˆ¦ç•¥
- é–¢é€£å¤šè§’åŒ–vs.éé–¢é€£å¤šè§’åŒ–
- å›½éš›åŒ–ã®æ®µéšã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±åˆã¨ãƒ­ãƒ¼ã‚«ãƒ«é©å¿œ
- æ–°èˆˆå¸‚å ´å‚å…¥æˆ¦ç•¥

### ã€å„ªå…ˆåº¦ï¼šä¸­ã€‘M&Aãƒ»æˆ¦ç•¥çš„ææº
- M&Aã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ¹æœ
- æ–‡åŒ–çµ±åˆã®æˆåŠŸè¦å› 
- ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªç®¡ç†
- Joint VentureåŠ¹æœåˆ†æ

**é©ç”¨ã—ãªã„å ´åˆ**ï¼š
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ç ”ç©¶ï¼ˆæ¶ˆè²»è€…è¡Œå‹•ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ç ”ç©¶ï¼‰
- ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ç ”ç©¶ï¼ˆè³‡æœ¬æ§‹é€ ãƒ»ä¼æ¥­ä¾¡å€¤è©•ä¾¡ã®ã¿ï¼‰
- HRç ”ç©¶ï¼ˆå€‹äººãƒ¬ãƒ™ãƒ«ã®æ…‹åº¦ãƒ»è¡Œå‹•ï¼‰â€»çµ„ç¹”ãƒ¬ãƒ™ãƒ«å¤‰æ•°ã¨ã®çµ±åˆã¯å¯
- ç´”ç²‹ãªç”£æ¥­çµ„ç¹”è«–ï¼ˆIOçµŒæ¸ˆå­¦ï¼‰â€»æˆ¦ç•¥è«–ã¸ã®ç¤ºå”†ãŒã‚ã‚Œã°å¯

---

## Core Workflow: 8-Phase System

### Phase 1: Research Design & Theoretical Positioning

**ç›®çš„**ï¼šç ”ç©¶ã®ç†è«–çš„åŸºç›¤ã‚’ç¢ºç«‹ã—ã€ãƒ‡ãƒ¼ã‚¿è¦ä»¶ã‚’æ˜ç¢ºåŒ–

#### 1.1 ç†è«–çš„ãƒ¬ãƒ³ã‚ºã®é¸æŠ

**ä¸»è¦ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ï¼š

**A. Resource-Based View (RBV) & Dynamic Capabilities**
```
é©ç”¨ç ”ç©¶ï¼š
- ä¼æ¥­å›ºæœ‰è³‡æºã¨ç«¶äº‰å„ªä½
- èƒ½åŠ›ã®ç•°è³ªæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ã®æ¸¬å®š

å¿…è¦ãƒ‡ãƒ¼ã‚¿ï¼š
- R&Dæ”¯å‡ºã€ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿
- ç„¡å½¢è³‡ç”£ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ä¾¡å€¤ã€çµ„ç¹”è³‡æœ¬ï¼‰
- å¾“æ¥­å“¡ã‚¹ã‚­ãƒ«æŒ‡æ¨™ï¼ˆæ•™è‚²æ°´æº–ã€çµŒé¨“å¹´æ•°ï¼‰
- çµ„ç¹”ãƒ—ãƒ­ã‚»ã‚¹æŒ‡æ¨™ï¼ˆè£½å“é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ã€Time-to-marketï¼‰

ä»£è¡¨çš„å¤‰æ•°ï¼š
- R&D intensity = R&Dæ”¯å‡º / å£²ä¸Šé«˜
- Patent stock = Î£ ç‰¹è¨±ä»¶æ•° Ã— æ¸›ä¾¡å„Ÿå´ç‡
- Human capital intensity = å¾“æ¥­å“¡çµ¦ä¸ç·é¡ / å¾“æ¥­å“¡æ•°
- Absorptive capacity = R&D intensity Ã— å¤–éƒ¨é€£æºæ•°
```

**B. Competitive Strategy (Porter Framework)**
```
é©ç”¨ç ”ç©¶ï¼š
- ç”£æ¥­æ§‹é€ ã¨ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
- Five Forcesåˆ†æã®å®šé‡åŒ–
- æˆ¦ç•¥ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ

å¿…è¦ãƒ‡ãƒ¼ã‚¿ï¼š
- ç”£æ¥­é›†ä¸­åº¦ï¼ˆHHIï¼‰
- å‚å…¥éšœå£æŒ‡æ¨™ï¼ˆå›ºå®šè³‡ç”£é›†ç´„åº¦ã€è¦åˆ¶ï¼‰
- è²·ã„æ‰‹ãƒ»å£²ã‚Šæ‰‹äº¤æ¸‰åŠ›ï¼ˆé¡§å®¢é›†ä¸­åº¦ã€ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼æ•°ï¼‰
- ä»£æ›¿è²¡è„…å¨ï¼ˆç”£æ¥­é–“ç«¶åˆåˆ†æï¼‰

ä»£è¡¨çš„å¤‰æ•°ï¼š
- HHI (Herfindahl Index) = Î£(å¸‚å ´ã‚·ã‚§ã‚¢Â²)
- Entry barrier = å›ºå®šè³‡ç”£ / ç·è³‡ç”£
- Buyer power = Top 5é¡§å®¢å£²ä¸Š / ç·å£²ä¸Š
- Product differentiation = åºƒå‘Šè²» / å£²ä¸Šé«˜
```

**C. Institutional Theory**
```
é©ç”¨ç ”ç©¶ï¼š
- åˆ¶åº¦ç’°å¢ƒã¨çµ„ç¹”è¡Œå‹•
- æ­£å½“æ€§ã¨ç”Ÿå­˜
- å›½éš›æ¯”è¼ƒç ”ç©¶

å¿…è¦ãƒ‡ãƒ¼ã‚¿ï¼š
- æ³•åˆ¶åº¦æŒ‡æ¨™ï¼ˆä¸–ç•ŒéŠ€è¡Œ Doing Businessï¼‰
- æ–‡åŒ–æ¬¡å…ƒï¼ˆHofstedeã‚¹ã‚³ã‚¢ï¼‰
- è¦åˆ¶ç’°å¢ƒï¼ˆæ¥­ç•Œè¦åˆ¶æ•°ã€æ”¿åºœä»‹å…¥åº¦ï¼‰
- èªè¨¼ãƒ»æ ¼ä»˜ã‘ï¼ˆISOå–å¾—ã€ESGè©•ä¾¡ï¼‰

ä»£è¡¨çš„å¤‰æ•°ï¼š
- Rule of law index (World Bank)
- Regulatory quality (World Bank)
- Cultural distance = Î£âˆš[(Hofstedeæ¬¡å…ƒå·®)Â²]
- Certification adoption rate
```

**D. Transaction Cost Economics (TCE)**
```
é©ç”¨ç ”ç©¶ï¼š
- Make-or-buyæ±ºå®š
- å‚ç›´çµ±åˆvs.ã‚¢ã‚¦ãƒˆã‚½ãƒ¼ã‚·ãƒ³ã‚°
- ã‚¬ãƒãƒŠãƒ³ã‚¹æ§‹é€ é¸æŠ

å¿…è¦ãƒ‡ãƒ¼ã‚¿ï¼š
- å–å¼•ç‰¹æ€§ï¼ˆè³‡ç”£ç‰¹æ®Šæ€§ã€ä¸ç¢ºå®Ÿæ€§ã€é »åº¦ï¼‰
- å‚ç›´çµ±åˆåº¦
- å¥‘ç´„å½¢æ…‹ãƒ‡ãƒ¼ã‚¿
- ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼é–¢ä¿‚ãƒ‡ãƒ¼ã‚¿

ä»£è¡¨çš„å¤‰æ•°ï¼š
- Vertical integration = ä»˜åŠ ä¾¡å€¤ / å£²ä¸Šé«˜
- Asset specificity = å°‚ç”¨è¨­å‚™æŠ•è³‡ / ç·å›ºå®šè³‡ç”£
- Transaction frequency = å¹´é–“å–å¼•å›æ•°
- Environmental uncertainty = å£²ä¸Šå¤‰å‹•ä¿‚æ•°
```

**E. Organizational Learning & Knowledge Management**
```
é©ç”¨ç ”ç©¶ï¼š
- å­¦ç¿’æ›²ç·šåŠ¹æœ
- çŸ¥è­˜ç§»è»¢
- ãƒŠãƒ¬ãƒƒã‚¸ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ åŠ¹æœ

å¿…è¦ãƒ‡ãƒ¼ã‚¿ï¼š
- ç”Ÿç”£é‡ç´¯ç©ãƒ‡ãƒ¼ã‚¿
- å¾“æ¥­å“¡æ•™è‚²æŠ•è³‡
- ç‰¹è¨±å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æŒ‡æ¨™

ä»£è¡¨çš„å¤‰æ•°ï¼š
- Learning rate = log(å˜ä½ã‚³ã‚¹ãƒˆ) / log(ç´¯ç©ç”Ÿç”£é‡)
- Knowledge stock = Î£ ç‰¹è¨± Ã— å¼•ç”¨æ•°
- Knowledge transfer = å­ä¼šç¤¾é–“ç‰¹è¨±å¼•ç”¨
- Training intensity = æ•™è‚²æŠ•è³‡ / äººä»¶è²»
```

#### 1.2 Research Question Refinement

**SMARTåŸºæº–ã«ã‚ˆã‚‹ç²¾ç·»åŒ–**ï¼š
- **Specific**: ã€Œçµ„ç¹”èƒ½åŠ›ã¯æ¥­ç¸¾ã«å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿã€â†’ã€ŒDynamic Capabilitiesï¼ˆè£½å“é–‹ç™ºé€Ÿåº¦ã€å¸‚å ´é©å¿œé€Ÿåº¦ï¼‰ã¯ã€ç’°å¢ƒå‹•æ…‹æ€§ãŒé«˜ã„ç”£æ¥­ã«ãŠã„ã¦ã€ROAã«ã©ã®ç¨‹åº¦å½±éŸ¿ã™ã‚‹ã‹ï¼Ÿã€
- **Measurable**: ã™ã¹ã¦ã®æ¦‚å¿µãŒå®šé‡åŒ–å¯èƒ½
- **Achievable**: åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼å¯èƒ½
- **Relevant**: ç†è«–ã¨å®Ÿå‹™ã®ä¸¡æ–¹ã«è²¢çŒ®
- **Time-bound**: åˆ†ææœŸé–“ã‚’æ˜ç¢ºåŒ–ï¼ˆä¾‹ï¼š2010-2023å¹´ï¼‰

**è‰¯ã„RQã®ä¾‹**ï¼š
```
RQ1: è£½å“é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ã®çŸ­ç¸®ï¼ˆDynamic Capabilityã®ä»£ç†å¤‰æ•°ï¼‰ã¯ã€
     æŠ€è¡“å¤‰åŒ–ãŒæ¿€ã—ã„ç”£æ¥­ã«ãŠã„ã¦ã€ä¼æ¥­ã®å£²ä¸Šæˆé•·ç‡ã«ã©ã®ç¨‹åº¦å¯„ä¸ã™ã‚‹ã‹ï¼Ÿ
     
æ¸¬å®šï¼š
- DV: å£²ä¸Šæˆé•·ç‡ï¼ˆå¹´æ¬¡ï¼‰
- IV: è£½å“é–‹ç™ºã‚µã‚¤ã‚¯ãƒ«ï¼ˆæ–°è£½å“æŠ•å…¥é »åº¦ï¼‰
- Moderator: ç”£æ¥­æŠ€è¡“å¤‰åŒ–ç‡ï¼ˆç‰¹è¨±æ›´æ–°é€Ÿåº¦ï¼‰
- Controls: ä¼æ¥­è¦æ¨¡ã€R&DæŠ•è³‡ã€å¹´é½¢ã€ãƒ¬ãƒãƒ¬ãƒƒã‚¸

æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœï¼š
- ä¸»åŠ¹æœï¼šé–‹ç™ºã‚µã‚¤ã‚¯ãƒ«çŸ­ç¸® â†’ å£²ä¸Šæˆé•·ç‡å‘ä¸Š
- äº¤äº’ä½œç”¨ï¼šæŠ€è¡“å¤‰åŒ–ãŒé€Ÿã„ç”£æ¥­ã§åŠ¹æœãŒå¢—å¹…
```

#### 1.3 Variable Conceptualization Matrix

ã™ã¹ã¦ã®å¤‰æ•°ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®5æ¬¡å…ƒã§å®šç¾©ï¼š

| å¤‰æ•° | ç†è«–çš„å®šç¾© | æ“ä½œçš„å®šç¾© | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | æ¸¬å®šãƒ¬ãƒ™ãƒ« | æœŸå¾…ç¬¦å· |
|------|------------|------------|--------------|------------|----------|
| ROA | ç·è³‡ç”£åç›Šç‡ | ç´”åˆ©ç›Š/ç·è³‡ç”£ | Compustat | Ratio | - |
| R&D Intensity | ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æŠ•è³‡ | R&Dæ”¯å‡º/å£²ä¸Šé«˜ | Compustat | Ratio | + |
| Firm Age | çµ„ç¹”æ…£æ€§ | è¨­ç«‹å¹´ã‹ã‚‰ã®çµŒéå¹´æ•° | Compustat | Count | +/- |
| Diversification | äº‹æ¥­å¤šæ§˜æ€§ | Entropy index | Compustat Segments | Ratio | +/- |
| Env. Dynamism | ç’°å¢ƒä¸ç¢ºå®Ÿæ€§ | å£²ä¸Šå¤‰å‹•ä¿‚æ•°ï¼ˆ5å¹´ï¼‰ | Compustat | Ratio | Moderator |

**é‡è¦**ï¼šå„å¤‰æ•°ã®ç†è«–çš„æ ¹æ‹ ã‚’æ–‡çŒ®ã§è£ä»˜ã‘ã‚‹ã€‚

---

### Phase 2: Strategic Data Source Discovery

**ç›®çš„**ï¼šæˆ¦ç•¥ç ”ç©¶ã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ç‰¹å®šã—ã€æœ€é©ãªçµ„ã¿åˆã‚ã›ã‚’è¨­è¨ˆ

#### 2.1 Primary Data Sources for Strategy Research

**A. ä¼æ¥­è²¡å‹™ãƒ»å¸‚å ´ãƒ‡ãƒ¼ã‚¿**

**1. North America (ç±³å›½ãƒ»ã‚«ãƒŠãƒ€)**

**Compustat North America**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½ãƒ»ã‚«ãƒŠãƒ€ä¸Šå ´ä¼æ¥­ 20,000ç¤¾ä»¥ä¸Š
æœŸé–“ï¼š1950å¹´ä»£ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šWRDSï¼ˆå¤§å­¦å¥‘ç´„ï¼‰

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- è²¡å‹™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå¤šè§’åŒ–åˆ†æï¼‰
- æ­´å²çš„ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒã‚¤ãƒãƒ«ãƒã‚¤ã‚¢ã‚¹å¯¾ç­–ï¼‰

ä¸»è¦ãƒ†ãƒ¼ãƒ–ãƒ«ï¼š
- fundaï¼šå¹´æ¬¡è²¡å‹™ãƒ‡ãƒ¼ã‚¿
- fundaqï¼šå››åŠæœŸè²¡å‹™ãƒ‡ãƒ¼ã‚¿
- seg_annualï¼šã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤šè§’åŒ–æ¸¬å®šï¼‰
- co_hdrï¼šä¼æ¥­ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ï¼ˆè¨­ç«‹å¹´ã€SICï¼‰

æˆ¦ç•¥å¤‰æ•°ã®æ§‹ç¯‰ä¾‹ï¼š
```sql
-- å¤šè§’åŒ–æŒ‡æ¨™ï¼šEntropy Index
WITH segment_sales AS (
    SELECT gvkey, fyear, 
           stype1, sales,
           SUM(sales) OVER (PARTITION BY gvkey, fyear) as total_sales
    FROM comp.seg_annual
    WHERE stype1 = 'BUSSEG'  -- Business segment
)
SELECT gvkey, fyear,
       -SUM((sales/total_sales) * LN(sales/total_sales)) as entropy_index
FROM segment_sales
GROUP BY gvkey, fyear;
```

**CRSP (Center for Research in Security Prices)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½æ ªå¼å¸‚å ´ãƒ‡ãƒ¼ã‚¿
æœŸé–“ï¼š1926å¹´ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šWRDS

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- å¸‚å ´ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆTobin's Qï¼‰
- ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£ï¼ˆM&Aç™ºè¡¨ã€æˆ¦ç•¥è»¢æ›ï¼‰
- ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆBetaã€Volatilityï¼‰

Tobin's Qè¨ˆç®—ä¾‹ï¼š
```python
# (å¸‚å ´ä¾¡å€¤ + è² å‚µç°¿ä¾¡) / ç·è³‡ç”£ç°¿ä¾¡
df['tobins_q'] = (df['market_cap'] + df['total_debt']) / df['total_assets']
```
```

**2. Europe**

**Orbis (Bureau van Dijk)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šæ¬§å·ä¼æ¥­400ä¸‡ç¤¾ä»¥ä¸Šï¼ˆä¸Šå ´ãƒ»éä¸Šå ´ï¼‰
å¼·ã¿ï¼šæ‰€æœ‰æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã€ä¸­å°ä¼æ¥­ã‚«ãƒãƒ¬ãƒƒã‚¸
ã‚¢ã‚¯ã‚»ã‚¹ï¼šå¤§å­¦å¥‘ç´„ã¾ãŸã¯BvDç›´æ¥å¥‘ç´„

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- Family businessç ”ç©¶
- Ownership structureã¨æˆ¦ç•¥
- Private firms vs. Public firmsæ¯”è¼ƒ
- Cross-border M&A

æ‰€æœ‰æ§‹é€ å¤‰æ•°ï¼š
- Family ownership %
- Institutional ownership %
- Foreign ownership %
- Ownership concentration (HHI)
```

**3. Asia-Pacific**

**æ—¥æœ¬ï¼šNEEDS-FinancialQUEST**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šæ—¥æœ¬ä¸Šå ´ä¼æ¥­3,800ç¤¾ä»¥ä¸Š
æœŸé–“ï¼š1950å¹´ä»£ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šæ—¥çµŒå¥‘ç´„

ç‰¹å¾´ï¼š
- æ—¥æœ¬çš„çµŒå–¶ç ”ç©¶ã«æœ€é©
- ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆä¼æ¥­ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æï¼‰
- æ ªå¼æŒã¡åˆã„ãƒ‡ãƒ¼ã‚¿

æ—¥æœ¬ç‰¹æœ‰å¤‰æ•°ï¼š
- Keiretsu affiliation
- Main bank relationship
- Cross-shareholding ratio
```

**ä¸­å›½ï¼šCSMAR**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šä¸­å›½Aæ ªãƒ»Bæ ªä¸Šå ´ä¼æ¥­
æœŸé–“ï¼š1990å¹´ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šå¤§å­¦å¥‘ç´„

åˆ¶åº¦ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- å›½æœ‰ä¼æ¥­vs.æ°‘é–“ä¼æ¥­
- æ”¿æ²»çš„ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³
- ç§»è¡ŒçµŒæ¸ˆã«ãŠã‘ã‚‹æˆ¦ç•¥

ä¸­å›½ç‰¹æœ‰å¤‰æ•°ï¼š
- State ownership %
- Political connection (Communist Party membership)
- Government subsidy amount
```

**4. Global Multi-Country**

**Worldscope (Refinitiv)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š70ã‚«å›½ã€70,000ç¤¾ä»¥ä¸Š
å¼·ã¿ï¼šæ¨™æº–åŒ–ã•ã‚ŒãŸè²¡å‹™ãƒ‡ãƒ¼ã‚¿
ã‚¢ã‚¯ã‚»ã‚¹ï¼šThomson Reuterså¥‘ç´„

å›½éš›æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- ã‚¯ãƒ­ã‚¹ã‚«ãƒ³ãƒˆãƒªãƒ¼æ¯”è¼ƒ
- æ–°èˆˆå¸‚å ´vs.å…ˆé€²å¸‚å ´
- åˆ¶åº¦ç’°å¢ƒã®å½±éŸ¿

æ³¨æ„ç‚¹ï¼š
- ä¼šè¨ˆåŸºæº–ã®é•ã„ï¼ˆUS GAAP vs. IFRSï¼‰
- ãƒ‡ãƒ¼ã‚¿å“è³ªã®å›½åˆ¥å·®
â†’ Country fixed effectsã§å¯¾å¿œ
```

**B. ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»æŠ€è¡“ãƒ‡ãƒ¼ã‚¿**

**USPTO PatentsView**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½ç‰¹è¨± å…¨ä»¶ï¼ˆ1976ã€œï¼‰
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™ï¼ˆBulk Download / APIï¼‰
URLï¼šhttps://patentsview.org/

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›æ¸¬å®š
- æŠ€è¡“å¤šè§’åŒ–
- çŸ¥è­˜ãƒ•ãƒ­ãƒ¼ãƒ»ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼
- ã‚ªãƒ¼ãƒ—ãƒ³ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

ä¸»è¦æŒ‡æ¨™ï¼š
```python
# ç‰¹è¨±ã‚¹ãƒˆãƒƒã‚¯ï¼ˆæ¸›ä¾¡å„Ÿå´è€ƒæ…®ï¼‰
patent_stock_t = Î£(patents_{t-i} Ã— (1-Î´)^i)  # Î´=0.15ãŒæ¨™æº–

# æŠ€è¡“å¤šè§’åŒ–ï¼ˆEntropyï¼‰
tech_diversity = -Î£(p_i Ã— ln(p_i))  # p_i = IPCåˆ†é¡iã®å‰²åˆ

# Forward citationsï¼ˆå½±éŸ¿åŠ›ï¼‰
citation_impact = è¢«å¼•ç”¨æ•° / æ¥­ç•Œå¹³å‡

# Generality indexï¼ˆæ±ç”¨æ€§ï¼‰
generality = 1 - Î£(è¢«å¼•ç”¨ã®æŠ€è¡“ã‚¯ãƒ©ã‚¹é›†ä¸­åº¦Â²)
```

**Integration with Compustat**:
```python
# ä¼æ¥­åãƒãƒƒãƒãƒ³ã‚°ï¼ˆfuzzy matchingï¼‰
from fuzzywuzzy import fuzz

# CUSIP, ticker, ä¼æ¥­åã§æ®µéšçš„ãƒãƒƒãƒãƒ³ã‚°
matched = match_companies(
    patents_df['assignee_name'],
    compustat_df['company_name'],
    threshold=85
)
```

**PATSTAT (European Patent Office)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹è¨±ï¼ˆ90ã‚«å›½ä»¥ä¸Šï¼‰
ã‚¢ã‚¯ã‚»ã‚¹ï¼šæœ‰æ–™ï¼ˆDVDè³¼å…¥ã¾ãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¥‘ç´„ï¼‰

ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ï¼š
- å›½éš›ç‰¹è¨±å‡ºé¡˜ï¼ˆPCTï¼‰
- ç‰¹è¨±ãƒ•ã‚¡ãƒŸãƒªãƒ¼åˆ†æ
- ã‚¯ãƒ­ã‚¹ãƒœãƒ¼ãƒ€ãƒ¼æŠ€è¡“ç§»è»¢
```

**Kogan et al. Patent Value Dataset**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½ç‰¹è¨±ã®å¸‚å ´ä¾¡å€¤æ¨å®š
æœŸé–“ï¼š1926-2010ï¼ˆæ›´æ–°ç‰ˆã‚ã‚Šï¼‰
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™ï¼ˆå­¦è¡“ã‚µã‚¤ãƒˆï¼‰

å¼•ç”¨ï¼šKogan, L., Papanikolaou, D., Seru, A., & Stoffman, N. (2017)

ç‰¹å¾´ï¼š
- æ ªä¾¡åå¿œã‹ã‚‰ç‰¹è¨±ä¾¡å€¤ã‚’æ¨å®š
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆæœã®çµŒæ¸ˆçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆæ¸¬å®š

ä½¿ç”¨ä¾‹ï¼š
```python
# ç‰¹è¨±ä¾¡å€¤åŠ é‡ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™
weighted_innovation = Î£(patent_value_i) / ç·è³‡ç”£
```

**C. M&Aãƒ»æˆ¦ç•¥çš„ææºãƒ‡ãƒ¼ã‚¿**

**SDC Platinum (Thomson Reuters)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«M&Aã€JVã€IPO
æœŸé–“ï¼š1970å¹´ä»£ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šWRDS

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- M&Aæˆ¦ç•¥ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- Acquisition premiums
- å¤šè§’åŒ–M&A vs. é–¢é€£M&A
- Cross-border M&A

ä¸»è¦å¤‰æ•°ï¼š
- Deal valueï¼ˆå–å¼•é¡ï¼‰
- Payment methodï¼ˆç¾é‡‘ãƒ»æ ªå¼ãƒ»æ··åˆï¼‰
- Hostile vs. Friendly
- Related vs. Unrelated (SICåˆ†é¡)
- Cultural distanceï¼ˆå›½éš›M&Aï¼‰

åˆ†æä¾‹ï¼š
```stata
* M&Aã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ¹æœ
reg roa_change ma_dummy related_ma size leverage i.year i.industry, ///
    vce(cluster firm_id)
```
```

**Zephyr (Bureau van Dijk)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«M&Aã€ç‰¹ã«æ¬§å·ãƒ»ã‚¢ã‚¸ã‚¢å¼·ã„
æœŸé–“ï¼š1997å¹´ã€œç¾åœ¨

æ¬§å·ãƒ»ã‚¢ã‚¸ã‚¢æˆ¦ç•¥ç ”ç©¶ã«æ¨å¥¨ï¼š
- Family business M&A
- Private equity deals
- Emerging market M&A
```

**D. ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿**

**ISS (Institutional Shareholder Services)**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šå–ç· å½¹ä¼šæ§‹æˆã€è­°æ±ºæ¨©è¡Œä½¿çµæœ
æœŸé–“ï¼š1996å¹´ã€œç¾åœ¨
ã‚¢ã‚¯ã‚»ã‚¹ï¼šå¤§å­¦å¥‘ç´„

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- å–ç· å½¹ä¼šæ§‹æˆã¨æˆ¦ç•¥é¸æŠ
- CEOç‰¹æ€§ï¼ˆDualityã€åœ¨ä»»æœŸé–“ï¼‰
- å–ç· å½¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- Board interlocks

ä¸»è¦å¤‰æ•°ï¼š
- Board size
- Independent directors ratio
- CEO duality (Chairmanå…¼ä»»)
- Director tenure
- Board expertise diversity
```

**E. ESGãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿**

**MSCI ESG Ratings**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š14,000ç¤¾ä»¥ä¸Š
ã‚¢ã‚¯ã‚»ã‚¹ï¼šæœ‰æ–™ï¼ˆé«˜é¡ï¼‰

æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- CSRæˆ¦ç•¥ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- Stakeholder management
- æ­£å½“æ€§ç²å¾—æˆ¦ç•¥

ä»£æ›¿ç„¡æ–™ã‚½ãƒ¼ã‚¹ï¼š
- CDP (Carbon Disclosure Project)ï¼šæ°—å€™å¤‰å‹•ãƒ‡ãƒ¼ã‚¿
- GRI Databaseï¼šã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆ
```

**F. ç”£æ¥­ãƒ»ãƒã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿**

**Bureau of Economic Analysis (BEA) - U.S.**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½ç”£æ¥­çµ±è¨ˆ
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™
URLï¼šhttps://www.bea.gov/

ç”£æ¥­åˆ†æã§ã®ä½¿ç”¨ï¼š
- Input-Output Tablesï¼ˆç”£æ¥­é–“å–å¼•ï¼‰
- ç”£æ¥­ãƒ¬ãƒ™ãƒ«ç”Ÿç”£æ€§
- GDP by industry

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- Vertical integrationæ±ºå®šè¦å› 
- ç”£æ¥­æ§‹é€ ã¨ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
```

**OECD STAN Database**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šOECDåŠ ç›Ÿå›½ã®ç”£æ¥­çµ±è¨ˆ
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™
URLï¼šhttps://stats.oecd.org/

å›½éš›æ¯”è¼ƒç ”ç©¶ï¼š
- Cross-countryç”£æ¥­æ§‹é€ 
- è¦åˆ¶ç’°å¢ƒæŒ‡æ¨™
- R&D intensity by industry
```

**World Bank Enterprise Surveys**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š140ã‚«å›½ã€ä¼æ¥­ãƒ¬ãƒ™ãƒ«èª¿æŸ»
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™
URLï¼šhttps://www.enterprisesurveys.org/

åˆ¶åº¦ãƒ»é€”ä¸Šå›½ç ”ç©¶ï¼š
- ãƒ“ã‚¸ãƒã‚¹ç’°å¢ƒæŒ‡æ¨™
- è¦åˆ¶è² æ‹…
- Corruption perceptions

åˆ¶åº¦å¤‰æ•°ï¼š
- Days to start a business
- Number of procedures
- Bribery incidence
```

#### 2.2 Free & Low-Cost Data Sourcesï¼ˆã‚¼ãƒ­äºˆç®—ç ”ç©¶ï¼‰

**å®Œå…¨ç„¡æ–™ã§å…¥æ‰‹å¯èƒ½ãªé«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**ï¼š

**A. ã‚¢ã‚¸ã‚¢åœ°åŸŸï¼ˆ11ã‚«å›½/åœ°åŸŸï¼‰**

**1. æ—¥æœ¬ ğŸ‡¯ğŸ‡µ**
```
EDINETï¼ˆé‡‘èåºï¼‰ï¼š
- æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€æ±ºç®—çŸ­ä¿¡
- APIï¼šhttps://disclosure2.edinet-fsa.go.jp/
- è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã€å½¹å“¡æƒ…å ±

JPXï¼ˆæ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ï¼š
- æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
- ä¸Šå ´ä¼æ¥­ä¸€è¦§ã€ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³

e-Statï¼ˆç·å‹™çœçµ±è¨ˆå±€ï¼‰ï¼š
- ç”£æ¥­çµ±è¨ˆã€ä¼æ¥­çµ±è¨ˆ
- APIï¼šhttps://www.e-stat.go.jp/api/

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- æ—¥æœ¬ä¼æ¥­ã®å¤šè§’åŒ–æˆ¦ç•¥
- ç³»åˆ—ãƒ»ä¼æ¥­ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æ
- é•·æœŸé›‡ç”¨ã¨çµ„ç¹”èƒ½åŠ›
```

**2. éŸ“å›½ ğŸ‡°ğŸ‡·**
```
DARTï¼ˆé‡‘èç›£ç£é™¢ï¼‰ï¼š
- è²¡å‹™è«¸è¡¨ã€äº‹æ¥­å ±å‘Šæ›¸
- APIï¼šhttps://opendart.fss.or.kr/
- å®Œå…¨ç„¡æ–™ã€ç™»éŒ²ã®ã¿å¿…è¦

KRXï¼ˆéŸ“å›½å–å¼•æ‰€ï¼‰ï¼š
- æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆCSVï¼‰
- ä¸Šå ´ä¼æ¥­æƒ…å ±

KOSTATï¼ˆçµ±è¨ˆåºï¼‰ï¼š
- ç”£æ¥­çµ±è¨ˆ
- APIåˆ©ç”¨å¯èƒ½

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- è²¡é–¥ï¼ˆChaebolï¼‰æ§‹é€ åˆ†æ
- æ”¿åºœã¨ã®é–¢ä¿‚ã¨æˆ¦ç•¥
- è¼¸å‡ºå¿—å‘ä¼æ¥­ã®æˆé•·æˆ¦ç•¥
```

**3. ä¸­å›½ ğŸ‡¨ğŸ‡³**
```
CNINFOï¼ˆå·¨æ½®è³‡è¨Šç½‘ï¼‰ï¼š
- è²¡å‹™è«¸è¡¨ã€å®šæœŸå ±å‘Š
- URLï¼šhttp://www.cninfo.com.cn/
- HTMLãƒ‘ãƒ¼ã‚¹å¿…è¦

Tushareï¼š
- æ ªä¾¡ãƒ»è²¡å‹™ãƒ‡ãƒ¼ã‚¿API
- åŸºæœ¬æ©Ÿèƒ½ç„¡æ–™
- URLï¼šhttps://tushare.pro/

AKShareï¼š
- å®Œå…¨ç„¡æ–™ã®Python API
- ç™»éŒ²ä¸è¦
- GitHubï¼šhttps://github.com/akfamily/akshare

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- å›½æœ‰ä¼æ¥­æ”¹é©ã¨æˆ¦ç•¥
- æ–°èˆˆå¸‚å ´å‚å…¥æˆ¦ç•¥
- æ”¿æ²»çš„ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®åŠ¹æœ
```

**B. ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹**

**World Bank Open Data**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š200ã‚«å›½ä»¥ä¸Šã€1,400æŒ‡æ¨™
APIï¼šhttps://data.worldbank.org/
Pythonï¼šwbdata package

åˆ¶åº¦ç ”ç©¶ã§ã®ä½¿ç”¨ï¼š
- Ease of Doing Business
- Governance indicators
- GDP, FDI flows
- Rule of law index

ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ï¼š
```python
import wbdata
# Rule of law indexå–å¾—
rol_data = wbdata.get_dataframe(
    {"RL.EST": "rule_of_law"},
    country=["US", "CN", "JP"]
)
```
```

**IMF Data**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒã‚¯ãƒ­æŒ‡æ¨™
APIï¼šhttps://data.imf.org/
ç„¡æ–™ã‚¢ã‚¯ã‚»ã‚¹

ç‚ºæ›¿ãƒ¬ãƒ¼ãƒˆã€ã‚¤ãƒ³ãƒ•ãƒ¬ã€å›½éš›åæ”¯ãƒ‡ãƒ¼ã‚¿
```

**OECD Data**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šOECDåŠ ç›Ÿå›½
APIï¼šhttps://data.oecd.org/
ç„¡æ–™ã‚¢ã‚¯ã‚»ã‚¹

ç”£æ¥­åˆ¥R&Dæ”¯å‡ºã€TFPã€è¦åˆ¶æŒ‡æ¨™
```

**SEC EDGAR**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šç±³å›½ä¸Šå ´ä¼æ¥­ã®å…¨å±Šå‡ºæ›¸é¡
APIï¼šhttps://www.sec.gov/edgar/sec-api-documentation
å®Œå…¨ç„¡æ–™

10-K, 10-Q, 8-K, Proxy statements
â†’ ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã€MD&Aåˆ†æã€ãƒªã‚¹ã‚¯é–‹ç¤º
```

#### 2.3 Data Source Selection Matrix

å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä»¥ä¸‹ã®7æ¬¡å…ƒã§è©•ä¾¡ï¼š

| ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | å¤‰æ•°ã‚«ãƒãƒ¬ãƒƒã‚¸ | æ™‚ç³»åˆ—é•·ã• | åœ°ç†çš„ç¯„å›² | ã‚¢ã‚¯ã‚»ã‚¹å®¹æ˜“æ€§ | ã‚³ã‚¹ãƒˆ | ãƒ‡ãƒ¼ã‚¿å“è³ª | æˆ¦ç•¥ç ”ç©¶é©åˆåº¦ |
|--------------|----------------|------------|------------|----------------|--------|------------|----------------|
| Compustat | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | åŒ—ç±³ | â˜…â˜…â˜…â˜†â˜† | $$$ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| CRSP | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | åŒ—ç±³ | â˜…â˜…â˜…â˜†â˜† | $$$ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| Orbis | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† | æ¬§å·++ | â˜…â˜…â˜…â˜†â˜† | $$$ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† |
| PatentsView | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | ç±³å›½ | â˜…â˜…â˜…â˜…â˜… | ç„¡æ–™ | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |
| EDINET | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | æ—¥æœ¬ | â˜…â˜…â˜…â˜…â˜… | ç„¡æ–™ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| World Bank | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜† | å…¨ä¸–ç•Œ | â˜…â˜…â˜…â˜…â˜… | ç„¡æ–™ | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜†â˜† |

**Decision Rules**:
- ç·åˆã‚¹ã‚³ã‚¢ â‰¥35/42 â†’ Primary source
- ã‚¹ã‚³ã‚¢ 28-34 â†’ Secondary source
- ã‚¹ã‚³ã‚¢ <28 â†’ è£œå®Œçš„ä½¿ç”¨ã®ã¿

---

### Phase 3: Sample Construction & Collection Strategy Design

**ç›®çš„**ï¼šç†è«–çš„ã«å¦¥å½“ã§ã€çµ±è¨ˆçš„ã«ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ã‚’è¨­è¨ˆ

#### 3.1 Sample Selection Criteria Development

**ç†è«–é§†å‹•å‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ï¼š

**ã‚¹ãƒ†ãƒƒãƒ—1ï¼šæ¯é›†å›£ã®å®šç¾©**
```
ä¾‹ï¼šç«¶äº‰æˆ¦ç•¥ç ”ç©¶
æ¯é›†å›£ï¼šç±³å›½è£½é€ æ¥­ä¸Šå ´ä¼æ¥­ï¼ˆSIC 2000-3999ï¼‰
æœŸé–“ï¼š2000-2023å¹´
ç†ç”±ï¼šè£½é€ æ¥­ã¯æˆ¦ç•¥ã®åŠ¹æœãŒæ˜ç¢ºã€ãƒ‡ãƒ¼ã‚¿å…¥æ‰‹å®¹æ˜“
```

**ã‚¹ãƒ†ãƒƒãƒ—2ï¼šç†è«–çš„åŒ…å«åŸºæº–**
```
å¿…é ˆåŸºæº–ï¼š
1. ä¸»è¦å–å¼•æ‰€ä¸Šå ´ï¼ˆNYSE, NASDAQ, AMEXï¼‰
   ç†ç”±ï¼šãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ã€æµå‹•æ€§ç¢ºä¿
   
2. é€£ç¶š3å¹´ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ä¿æœ‰
   ç†ç”±ï¼šãƒ©ã‚°å¤‰æ•°ã€å›ºå®šåŠ¹æœæ¨å®šã«å¿…è¦
   
3. ç·è³‡ç”£ â‰¥ $10Mï¼ˆ2023å¹´å®Ÿè³ªä¾¡æ ¼ï¼‰
   ç†ç”±ï¼šæ¥µå°ä¼æ¥­ã®ç•°å¸¸å€¤æ’é™¤

4. éé‡‘èãƒ»éå…¬ç›Šä¼æ¥­
   ç†ç”±ï¼šä¼šè¨ˆåŸºæº–ãƒ»è¦åˆ¶ãŒç•°ãªã‚‹
```

**ã‚¹ãƒ†ãƒƒãƒ—3ï¼šç†è«–çš„é™¤å¤–åŸºæº–**
```
é™¤å¤–ï¼š
1. è² ã®æ ªä¸»è³‡æœ¬ä¼æ¥­
   ç†ç”±ï¼šè²¡å‹™å›°é›£ä¼æ¥­ã€ROEè¨ˆç®—ä¸èƒ½

2. æ¥µç«¯ãƒ¬ãƒãƒ¬ãƒƒã‚¸ï¼ˆDebt/Assets > 1.5ï¼‰
   ç†ç”±ï¼šç•°å¸¸è³‡æœ¬æ§‹é€ 

3. M&Aå¹´ï¼ˆè²·åå´Â±1å¹´ã€è¢«è²·åå´å…¨æœŸé–“ï¼‰
   ç†ç”±ï¼šè²¡å‹™è«¸è¡¨ã®éé€£ç¶šæ€§
   â€»M&Aç ”ç©¶ã§ã¯é€†ã«å¯¾è±¡ã¨ã™ã‚‹

4. IPOå¾Œ3å¹´æœªæº€
   ç†ç”±ï¼šçµ„ç¹”çš„æ··ä¹±æœŸ
```

**ã‚¹ãƒ†ãƒƒãƒ—4ï¼šã‚µãƒã‚¤ãƒãƒ«ãƒã‚¤ã‚¢ã‚¹å¯¾ç­–**

âš ï¸ **Critical for Strategy Research**

```python
# CRSPã®delistingæƒ…å ±ã‚’çµ±åˆ
delisting_codes = {
    200-399: "Merger",
    400-490: "Exchange",
    500-599: "Liquidation"
}

# ãƒ‡ãƒªã‚¹ãƒˆä¼æ¥­ã‚‚åˆ†æã‚µãƒ³ãƒ—ãƒ«ã«å«ã‚ã‚‹
# ãƒ‡ãƒªã‚¹ãƒˆæ—¥ã¾ã§ ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
df_with_delisted = pd.merge(
    df_active,
    df_delisted[df_delisted['dlstdt'] >= sample_start],
    how='outer'
)

# ã‚µãƒã‚¤ãƒãƒ«ãƒã‚¤ã‚¢ã‚¹ã®æ¤œè¨¼
print(f"Active firms: {len(df_active)}")
print(f"Delisted firms: {len(df_delisted)}")
print(f"Delisting rate: {len(df_delisted)/(len(df_active)+len(df_delisted))*100:.1f}%")

# ã‚‚ã—delisting rate > 20% â†’ æ·±åˆ»ãªãƒã‚¤ã‚¢ã‚¹ã®å¯èƒ½æ€§
```

#### 3.2 Statistical Power Analysisï¼ˆäº‹å‰æ¤œå®šåŠ›åˆ†æï¼‰

**Why Critical**ï¼š
- Top journalsã¯äº‹å‰ç™»éŒ²ã‚’æ¨å¥¨
- Underpowered studiesã¯å½é™°æ€§ãƒªã‚¹ã‚¯
- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºæ±ºå®šã®å®¢è¦³çš„æ ¹æ‹ 

**æ‰‹é †**ï¼š

**1. æœŸå¾…åŠ¹æœé‡ã®è¨­å®š**
```python
from statsmodels.stats.power import TTestIndPower

# å…ˆè¡Œç ”ç©¶ã®ãƒ¡ã‚¿åˆ†æã‹ã‚‰åŠ¹æœé‡ã‚’æ¨å®š
# ä¾‹ï¼šR&D â†’ ROAã®åŠ¹æœ
# Cohen's d = 0.35 (small to medium)

# å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
analysis = TTestIndPower()
sample_size = analysis.solve_power(
    effect_size=0.35,
    alpha=0.05,
    power=0.80,  # 80%æ¤œå‡ºåŠ›ï¼ˆæ¨™æº–ï¼‰
    alternative='two-sided'
)

print(f"Required N per group: {sample_size:.0f}")
# â†’ ç´„130ç¤¾/ã‚°ãƒ«ãƒ¼ãƒ—å¿…è¦
```

**2. å›å¸°åˆ†æã®æ¤œå‡ºåŠ›**
```python
from data_quality_checker import SampleSizeCalculator

calc = SampleSizeCalculator()

# å¤šå¤‰é‡å›å¸°ã®æ¤œå‡ºåŠ›åˆ†æ
result = calc.regression_sample_size(
    num_predictors=8,  # ç‹¬ç«‹å¤‰æ•°+çµ±åˆ¶å¤‰æ•°
    expected_r2=0.15,  # å…ˆè¡Œç ”ç©¶ã‹ã‚‰æ¨å®š
    power=0.80,
    alpha=0.05
)

print(f"Required N: {result['recommended_n']}")
print(f"Minimum N: {result['minimum_n']}")
print(f"Conservative N: {result['conservative_n']}")
```

**3. ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºåŠ›**
```python
# ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ç‰¹æœ‰ã®è€ƒæ…®äº‹é …
panel_result = calc.panel_data_sample_size(
    num_firms=300,
    num_periods=10,
    effect_size='medium',
    power=0.85,
    clustering=True  # Clustered SEsè€ƒæ…®
)

print(f"Effective N: {panel_result['effective_n']}")
print(f"Design effect: {panel_result['design_effect']:.2f}")
# â†’ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§å®ŸåŠ¹ã‚µãƒ³ãƒ—ãƒ«æ¸›å°‘ã‚’è€ƒæ…®
```

**4. Minimum Detectable Effect (MDE)**
```python
# åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«ã§æ¤œå‡ºå¯èƒ½ãªæœ€å°åŠ¹æœ
mde = analysis.solve_power(
    nobs1=250,  # å®Ÿéš›ã®åˆ©ç”¨å¯èƒ½ã‚µãƒ³ãƒ—ãƒ«
    alpha=0.05,
    power=0.80,
    alternative='two-sided'
)

print(f"Minimum Detectable Effect (Cohen's d): {mde:.3f}")

# ã‚‚ã—MDE > ç†è«–çš„ã«é‡è¦ãªåŠ¹æœé‡ â†’ ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³
```

**å ±å‘Šä¾‹**ï¼š
```
ã€Œå…ˆè¡Œç ”ç©¶ã®ãƒ¡ã‚¿åˆ†æï¼ˆSmith et al., 2020ï¼‰ã«åŸºã¥ãã€
R&D intensityãŒROAã«ä¸ãˆã‚‹åŠ¹æœé‡ã‚’d=0.35ã¨æ¨å®šã—ãŸã€‚
80%æ¤œå‡ºåŠ›ï¼ˆÎ±=0.05ï¼‰ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€1ã‚°ãƒ«ãƒ¼ãƒ—ã‚ãŸã‚Š
130ç¤¾ã€è¨ˆ260ç¤¾ã®ã‚µãƒ³ãƒ—ãƒ«ãŒå¿…è¦ã¨ç®—å‡ºã•ã‚ŒãŸã€‚
æœ¬ç ”ç©¶ã®æœ€çµ‚ã‚µãƒ³ãƒ—ãƒ«ï¼ˆN=312ç¤¾ï¼‰ã¯ã€ååˆ†ãªçµ±è¨ˆçš„æ¤œå‡ºåŠ›
ï¼ˆå®Ÿç¾æ¤œå‡ºåŠ›=87%ï¼‰ã‚’æœ‰ã—ã¦ã„ã‚‹ã€‚ã€
```

#### 3.3 Data Extraction Scripts

**A. SQL-based Extraction (WRDS)**

```sql
-- Compustatè²¡å‹™ãƒ‡ãƒ¼ã‚¿ + ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±
-- å¤šè§’åŒ–ç ”ç©¶ç”¨

-- Step 1: åŸºæœ¬è²¡å‹™ãƒ‡ãƒ¼ã‚¿
CREATE TABLE strategy_sample AS
SELECT 
    a.gvkey,
    a.fyear,
    a.datadate,
    a.conm AS firm_name,
    a.fic AS incorporation_country,
    
    -- Performance variables
    a.at AS total_assets,
    a.sale AS sales,
    a.ni AS net_income,
    a.ebitda,
    a.oibdp AS operating_income,
    
    -- Strategy variables
    a.xrd AS rd_expenditure,
    a.xad AS advertising_expense,
    a.capx AS capex,
    a.emp AS employees,
    
    -- Financial controls
    a.ceq AS common_equity,
    a.dltt AS long_term_debt,
    a.dlc AS short_term_debt,
    a.che AS cash,
    a.ppent AS ppe,
    
    -- Industry
    a.sich AS sic_code,
    
    -- Market data (link to CRSP)
    b.prcc_f AS stock_price_fy,
    b.csho AS shares_outstanding
    
FROM 
    comp.funda a
LEFT JOIN 
    comp.funda b ON a.gvkey = b.gvkey AND a.fyear = b.fyear
    
WHERE 
    a.fyear BETWEEN 2000 AND 2023
    AND a.indfmt = 'INDL'      -- Industrial format
    AND a.datafmt = 'STD'       -- Standardized
    AND a.popsrc = 'D'          -- Domestic
    AND a.consol = 'C'          -- Consolidated
    AND a.sich BETWEEN 2000 AND 3999  -- Manufacturing
    AND a.at > 10               -- Total assets > $10M
;

-- Step 2: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆå¤šè§’åŒ–æ¸¬å®šï¼‰
CREATE TABLE diversification_data AS
SELECT 
    gvkey,
    fyear,
    stype1,  -- Segment type
    sics1 AS segment_sic,
    sales AS segment_sales,
    SUM(sales) OVER (PARTITION BY gvkey, fyear) AS total_sales
FROM 
    comp.seg_annual
WHERE 
    fyear BETWEEN 2000 AND 2023
    AND stype1 = 'BUSSEG'  -- Business segment
;

-- Step 3: Entropy indexè¨ˆç®—
CREATE TABLE entropy_index AS
SELECT 
    gvkey,
    fyear,
    COUNT(DISTINCT segment_sic) AS num_segments,
    -SUM((segment_sales/total_sales) * 
         LN(segment_sales/total_sales)) AS entropy_index,
    -- Related diversification: Same 2-digit SIC
    SUM(CASE WHEN LEFT(segment_sic,2) = 
              (SELECT LEFT(segment_sic,2) 
               FROM diversification_data d2 
               WHERE d2.gvkey = d1.gvkey 
                 AND d2.fyear = d1.fyear 
               ORDER BY segment_sales DESC LIMIT 1)
        THEN segment_sales ELSE 0 END) / total_sales 
        AS related_diversification_ratio
FROM 
    diversification_data d1
GROUP BY 
    gvkey, fyear
;
```

**B. Python API Extraction (ç„¡æ–™ã‚½ãƒ¼ã‚¹)**

```python
import requests
import pandas as pd
import time
from ratelimit import limits, sleep_and_retry

# æ—¥æœ¬ä¼æ¥­ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆEDINET APIï¼‰
class EDINETCollector:
    def __init__(self):
        self.base_url = "https://disclosure2.edinet-fsa.go.jp/api/v2"
        
    @sleep_and_retry
    @limits(calls=10, period=60)  # Rate limiting
    def get_document_list(self, date):
        """æŒ‡å®šæ—¥ã®æå‡ºæ›¸é¡ä¸€è¦§å–å¾—"""
        endpoint = f"{self.base_url}/documents.json"
        params = {
            'date': date,  # YYYY-MM-DD
            'type': 2      # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
        }
        response = requests.get(endpoint, params=params)
        return response.json()
    
    def extract_financial_data(self, doc_id):
        """è²¡å‹™ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        endpoint = f"{self.base_url}/documents/{doc_id}"
        params = {'type': 5}  # XBRLå½¢å¼
        response = requests.get(endpoint, params=params)
        
        # XBRLãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        # å®Ÿéš›ã¯xbrl-parserãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨æ¨å¥¨
        return self._parse_xbrl(response.content)
    
    def collect_sample(self, start_date, end_date, industry_codes):
        """ã‚µãƒ³ãƒ—ãƒ«ä¼æ¥­ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿åé›†"""
        date_range = pd.date_range(start_date, end_date, freq='D')
        
        all_data = []
        for date in date_range:
            docs = self.get_document_list(date.strftime('%Y-%m-%d'))
            
            for doc in docs.get('results', []):
                # ç”£æ¥­ãƒ•ã‚£ãƒ«ã‚¿
                if doc['ordinanceCode'] in industry_codes:
                    financial_data = self.extract_financial_data(doc['docID'])
                    all_data.append(financial_data)
                    
            time.sleep(0.5)  # Respectful scraping
        
        return pd.DataFrame(all_data)

# ä½¿ç”¨ä¾‹
collector = EDINETCollector()
df_japan = collector.collect_sample(
    start_date='2023-01-01',
    end_date='2023-12-31',
    industry_codes=['010']  # è£½é€ æ¥­
)
```

**C. Patent Data Integration**

```python
import requests
import pandas as pd

class PatentsViewCollector:
    def __init__(self):
        self.api_url = "https://api.patentsview.org/patents/query"
    
    def collect_firm_patents(self, firm_name, start_year, end_year):
        """ä¼æ¥­ã®ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿åé›†"""
        query = {
            "q": {
                "_and": [
                    {"assignee_organization": firm_name},
                    {"_gte": {"patent_date": f"{start_year}-01-01"}},
                    {"_lte": {"patent_date": f"{end_year}-12-31"}}
                ]
            },
            "f": [
                "patent_number",
                "patent_date",
                "patent_title",
                "patent_abstract",
                "cited_patent_number",
                "uspc_mainclass_id",
                "cpc_subgroup_id"
            ],
            "o": {"per_page": 10000}
        }
        
        response = requests.post(self.api_url, json=query)
        data = response.json()
        
        return pd.DataFrame(data.get('patents', []))
    
    def calculate_innovation_metrics(self, patents_df):
        """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™ã®è¨ˆç®—"""
        metrics = {}
        
        # ç‰¹è¨±æ•°
        metrics['patent_count'] = len(patents_df)
        
        # æŠ€è¡“å¤šè§’åŒ–ï¼ˆEntropyï¼‰
        tech_classes = patents_df['uspc_mainclass_id'].value_counts()
        probs = tech_classes / tech_classes.sum()
        metrics['tech_diversity'] = -sum(probs * np.log(probs))
        
        # Citation impact
        citations = patents_df['cited_patent_number'].apply(len)
        metrics['avg_citations'] = citations.mean()
        metrics['citation_std'] = citations.std()
        
        # Generality index
        # ï¼ˆè¢«å¼•ç”¨ã®æŠ€è¡“ã‚¯ãƒ©ã‚¹åˆ†æ•£ï¼‰
        
        return metrics

# ä½¿ç”¨ä¾‹ï¼šCompustatã¨çµ±åˆ
patents_collector = PatentsViewCollector()

for firm in compustat_df['firm_name'].unique():
    patents = patents_collector.collect_firm_patents(
        firm, 2000, 2023
    )
    metrics = patents_collector.calculate_innovation_metrics(patents)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’Compustatãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸
    compustat_df.loc[
        compustat_df['firm_name'] == firm, 
        'patent_count'
    ] = metrics['patent_count']
```

---

### Phase 4: Data Cleaning & Variable Construction

**ç›®çš„**ï¼šRaw dataã‚’åˆ†æå¯èƒ½ãªå½¢å¼ã«å¤‰æ›

#### 4.1 Financial Data Standardization

**A. é€šè²¨ãƒ»å˜ä½ã®çµ±ä¸€**

```python
# Compustatã¯millionsã€World Bankã¯current USD
# â†’ ã™ã¹ã¦thousands of USD ã«çµ±ä¸€

df['total_assets_thousands'] = df['at'] * 1000  # Compustat
df['gdp_thousands'] = df['gdp_current_usd'] / 1000  # World Bank
```

**B. ã‚¤ãƒ³ãƒ•ãƒ¬èª¿æ•´**

```python
import wbdata

# GDP deflatorã‚’å–å¾—
deflator = wbdata.get_dataframe(
    {"NY.GDP.DEFL.ZS": "gdp_deflator"},
    country="US"
)

# 2023å¹´å®Ÿè³ªä¾¡æ ¼ã«å¤‰æ›
base_year = 2023
df['real_sales'] = df['sales'] * (
    deflator.loc[base_year, 'gdp_deflator'] / 
    df['year'].map(deflator['gdp_deflator'])
)
```

**C. Winsorizationï¼ˆå¤–ã‚Œå€¤å‡¦ç†ï¼‰**

```python
from scipy.stats.mstats import winsorize

# é€£ç¶šå¤‰æ•°ã‚’1%ile and 99%ileã§winsorize
continuous_vars = ['roa', 'leverage', 'tobins_q', 'rd_intensity']

for var in continuous_vars:
    df[f'{var}_winsor'] = winsorize(
        df[var], 
        limits=[0.01, 0.01],
        nan_policy='omit'
    )
    
# Winsorizeå‰å¾Œã®è¨˜è¿°çµ±è¨ˆã‚’æ¯”è¼ƒ
print(df[continuous_vars].describe())
print(df[[f'{v}_winsor' for v in continuous_vars]].describe())
```

#### 4.2 Strategic Variable Construction

**A. Performance Variables**

```python
# ROA (Return on Assets)
df['roa'] = df['net_income'] / df['total_assets']

# ROE (Return on Equity)
df['roe'] = df['net_income'] / df['common_equity']

# Tobin's Q (market-based performance)
df['tobins_q'] = (
    df['market_cap'] + df['total_debt'] - df['cash']
) / df['total_assets']

# ROS (Return on Sales)
df['ros'] = df['net_income'] / df['sales']

# Asset Turnover
df['asset_turnover'] = df['sales'] / df['total_assets']
```

**B. Innovation & Dynamic Capabilities**

```python
# R&D Intensity
df['rd_intensity'] = df['rd_expenditure'] / df['sales']
df['rd_intensity'].fillna(0, inplace=True)  # Missing = 0
df['rd_missing_dummy'] = df['rd_expenditure'].isna().astype(int)

# Patent Stock (with depreciation)
depreciation_rate = 0.15
max_lag = 10

df['patent_stock'] = 0
for lag in range(max_lag):
    df['patent_stock'] += (
        df.groupby('firm_id')['patent_count']
        .shift(lag)
        .fillna(0) * (1 - depreciation_rate) ** lag
    )

# Citation-weighted patent stock
df['citation_weighted_patent_stock'] = 0
for lag in range(max_lag):
    df['citation_weighted_patent_stock'] += (
        (df.groupby('firm_id')['patent_count'].shift(lag) *
         df.groupby('firm_id')['avg_citations'].shift(lag))
        .fillna(0) * (1 - depreciation_rate) ** lag
    )

# New product introduction rate (proxy for dynamic capability)
# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–°è¦äº‹æ¥­å‚å…¥ã‚’æ¤œå‡º
df['new_segment_entry'] = (
    df.groupby('firm_id')['num_segments']
    .diff()
    .clip(lower=0)  # å¢—åŠ ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
)
```

**C. Diversification Strategies**

```python
# Entropy Indexï¼ˆç·åˆå¤šè§’åŒ–ï¼‰
# Phase 3ã®SQLçµæœã‚’ä½¿ç”¨ã€ã¾ãŸã¯Pythonã§è¨ˆç®—

def calculate_entropy(segment_sales):
    """Entropy indexè¨ˆç®—"""
    total = segment_sales.sum()
    shares = segment_sales / total
    return -sum(shares * np.log(shares))

df['entropy_index'] = df.groupby(['firm_id', 'year'])['segment_sales'].apply(
    calculate_entropy
).reset_index(drop=True)

# Related vs. Unrelated Diversification
df['related_div_ratio'] = df.groupby(['firm_id', 'year']).apply(
    lambda x: (x[x['segment_sic'].str[:2] == 
                 x['segment_sic'].iloc[0][:2]]['segment_sales'].sum() / 
               x['segment_sales'].sum())
).reset_index(drop=True)

df['unrelated_div_ratio'] = 1 - df['related_div_ratio']

# Herfindahl Indexï¼ˆé›†ä¸­åº¦ã€Entropyã®é€†ï¼‰
df['herfindahl_diversification'] = df.groupby(['firm_id', 'year']).apply(
    lambda x: sum((x['segment_sales'] / x['segment_sales'].sum()) ** 2)
).reset_index(drop=True)
```

**D. Competitive Strategy Variables**

```python
# Cost Leadership indicators
df['asset_intensity'] = df['total_assets'] / df['sales']
df['labor_intensity'] = df['emp'] / df['sales']  # å¾“æ¥­å“¡æ•°/å£²ä¸Š

# Differentiation indicators
df['advertising_intensity'] = df['advertising_expense'] / df['sales']
df['advertising_intensity'].fillna(0, inplace=True)

# R&D intensityã¯æ—¢ã«è¨ˆç®—æ¸ˆã¿

# Price premium (industry-adjusted)
industry_median_price = df.groupby(['industry', 'year'])['sales'].transform('median')
df['price_premium'] = df['sales'] / industry_median_price

# Product differentiation score (composite)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
diff_indicators = ['advertising_intensity', 'rd_intensity', 'price_premium']
df['differentiation_score'] = scaler.fit_transform(
    df[diff_indicators]
).mean(axis=1)
```

**E. Organizational Structure Variables**

```python
# Vertical Integration
# ä»˜åŠ ä¾¡å€¤ / å£²ä¸Šé«˜
df['vertical_integration'] = (
    df['sales'] - df['cogs'] - df['sga_external']
) / df['sales']

# Span of Control
# çµ„ç¹”éšå±¤ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆOrbis, ä¼æ¥­é–‹ç¤ºè³‡æ–™ï¼‰
df['span_of_control'] = df['num_employees'] / df['num_managers']

# Organizational Complexity
# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã€åœ°ç†çš„åˆ†æ•£ã€è£½å“ãƒ©ã‚¤ãƒ³æ•°
df['org_complexity'] = (
    df['num_segments'] + 
    df['num_countries'] + 
    df['num_product_lines']
) / 3  # æ­£è¦åŒ–
```

**F. Environmental Variables**

```python
# Environmental Dynamismï¼ˆç’°å¢ƒå‹•æ…‹æ€§ï¼‰
# å£²ä¸Šã®å¤‰å‹•ä¿‚æ•°ï¼ˆéå»5å¹´ï¼‰

def calc_dynamism(series, window=5):
    """ç’°å¢ƒå‹•æ…‹æ€§è¨ˆç®—"""
    std = series.rolling(window).std()
    mean = series.rolling(window).mean()
    return std / mean  # Coefficient of variation

df['env_dynamism'] = df.groupby('firm_id')['sales'].transform(
    lambda x: calc_dynamism(x, window=5)
)

# Environmental Munificenceï¼ˆç’°å¢ƒå¯›å®¹æ€§ï¼‰
# ç”£æ¥­å£²ä¸Šæˆé•·ç‡

df['env_munificence'] = df.groupby(['industry', 'year'])['sales'].transform(
    lambda x: (x.sum() - x.shift(1).sum()) / x.shift(1).sum()
)

# Environmental Complexityï¼ˆç’°å¢ƒè¤‡é›‘æ€§ï¼‰
# ç”£æ¥­å†…ä¼æ¥­æ•°ã€æŠ€è¡“å¤šæ§˜æ€§

industry_firms = df.groupby(['industry', 'year'])['firm_id'].nunique()
df['env_complexity'] = df[['industry', 'year']].merge(
    industry_firms.rename('num_competitors'),
    left_on=['industry', 'year'],
    right_index=True
)['num_competitors']
```

**G. Institutional Variables**

```python
# Country-level institutional variables
# World Bankã®Governance Indicatorsã¨çµ±åˆ

import wbdata

# Rule of Law
rule_of_law = wbdata.get_dataframe(
    {"RL.EST": "rule_of_law"},
    country=df['country'].unique().tolist()
)

df = df.merge(
    rule_of_law,
    left_on=['country', 'year'],
    right_index=True,
    how='left'
)

# Regulatory Quality
reg_quality = wbdata.get_dataframe(
    {"RQ.EST": "regulatory_quality"},
    country=df['country'].unique().tolist()
)

df = df.merge(
    reg_quality,
    left_on=['country', 'year'],
    right_index=True,
    how='left'
)

# Cultural Distance (Hofstede)
# æ‰‹å‹•ã§å–å¾—ï¼šhttps://geerthofstede.com/research-and-vsm/dimension-data-matrix/

hofstede_df = pd.read_csv('hofstede_scores.csv')

def calculate_cultural_distance(country1, country2, hofstede_df):
    """Kogut & Singh (1988) cultural distance"""
    dimensions = ['power_distance', 'individualism', 'masculinity', 
                  'uncertainty_avoidance']
    
    distance = 0
    for dim in dimensions:
        diff = (hofstede_df.loc[country1, dim] - 
                hofstede_df.loc[country2, dim]) ** 2
        distance += diff / hofstede_df[dim].var()
    
    return distance / len(dimensions)

# Home country vs. Host country cultural distance
df['cultural_distance'] = df.apply(
    lambda row: calculate_cultural_distance(
        row['home_country'], 
        row['host_country'], 
        hofstede_df
    ),
    axis=1
)
```

#### 4.3 Time Alignment & Lagged Variables

```python
# Fiscal year to calendar year alignment
df['calendar_year'] = pd.to_datetime(df['datadate']).dt.year

# Reporting lagè€ƒæ…®ï¼ˆ4ãƒ¶æœˆå¾Œã«æƒ…å ±å…¥æ‰‹å¯èƒ½ï¼‰
df['analysis_year'] = df['calendar_year']
df.loc[df['datadate'].dt.month <= 3, 'analysis_year'] += 1

# Lagged independent variablesï¼ˆå†…ç”Ÿæ€§å¯¾ç­–ï¼‰
lag_vars = ['rd_intensity', 'advertising_intensity', 'firm_size', 
            'leverage', 'firm_age']

for var in lag_vars:
    df[f'{var}_lag1'] = df.groupby('firm_id')[var].shift(1)
    df[f'{var}_lag2'] = df.groupby('firm_id')[var].shift(2)

# Lead dependent variablesï¼ˆå°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼‰
df['roa_lead1'] = df.groupby('firm_id')['roa'].shift(-1)
df['roa_lead2'] = df.groupby('firm_id')['roa'].shift(-2)

# Change variablesï¼ˆå·®åˆ†ï¼‰
df['roa_change'] = df.groupby('firm_id')['roa'].diff()
df['sales_growth'] = df.groupby('firm_id')['sales'].pct_change()
```

---

### Phase 5: Multi-Source Data Integration

**ç›®çš„**ï¼šè¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆã—ã€ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰

#### 5.1 Identifier Matching Strategy

**å„ªå…ˆé †ä½**ï¼š
1. **Perfect match**ï¼šGVKEY, PERMNO, CUSIP
2. **High-confidence fuzzy match**ï¼šä¼æ¥­åï¼ˆâ‰¥90%é¡ä¼¼åº¦ï¼‰
3. **Manual verification**ï¼šé‡è¦ä¼æ¥­ã®æ‰‹å‹•ç¢ºèª

**A. Compustat - CRSP Link**

```python
# WRDSã®CCM (Compustat-CRSP Merged) link tableä½¿ç”¨
ccm_link = wrds_conn.raw_sql("""
    SELECT gvkey, lpermno as permno, linkdt, linkenddt
    FROM crsp.ccmxpf_lnkhist
    WHERE linktype IN ('LU', 'LC')  -- Primary links
      AND linkprim IN ('P', 'C')    -- Primary links
""")

# Time-variant linkã‚’è€ƒæ…®ã—ã¦ãƒãƒ¼ã‚¸
df_merged = pd.merge_asof(
    df_compustat.sort_values('datadate'),
    ccm_link,
    left_on='datadate',
    right_on='linkdt',
    by='gvkey',
    direction='backward'
)

# LinkæœŸé–“å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
df_merged = df_merged[
    (df_merged['datadate'] >= df_merged['linkdt']) &
    (df_merged['datadate'] <= df_merged['linkenddt'])
]
```

**B. Compustat - Patents Matching**

```python
from fuzzywuzzy import fuzz, process
import re

def clean_company_name(name):
    """ä¼æ¥­åã®æ¨™æº–åŒ–"""
    # å°æ–‡å­—åŒ–
    name = name.lower()
    # æ³•äººæ ¼å‰Šé™¤
    suffixes = ['inc', 'corp', 'corporation', 'company', 'co', 
                'ltd', 'limited', 'llc', 'plc']
    for suffix in suffixes:
        name = re.sub(rf'\b{suffix}\b\.?', '', name)
    # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»è¨˜å·å‰Šé™¤
    name = re.sub(r'[^\w\s]', '', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name

# ä¼æ¥­åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
df_compustat['clean_name'] = df_compustat['conm'].apply(clean_company_name)
df_patents['clean_name'] = df_patents['assignee_organization'].apply(clean_company_name)

# Fuzzy matching
def match_companies(patents_df, compustat_df, threshold=85):
    """Fuzzy matchingã§ä¼æ¥­ã‚’ãƒãƒƒãƒãƒ³ã‚°"""
    matches = []
    
    compustat_names = compustat_df['clean_name'].unique()
    
    for idx, row in patents_df.iterrows():
        patent_name = row['clean_name']
        
        # Best matchæ¤œç´¢
        best_match, score = process.extractOne(
            patent_name, 
            compustat_names,
            scorer=fuzz.token_sort_ratio
        )
        
        if score >= threshold:
            gvkey = compustat_df[
                compustat_df['clean_name'] == best_match
            ]['gvkey'].iloc[0]
            
            matches.append({
                'patent_name': patent_name,
                'compustat_name': best_match,
                'gvkey': gvkey,
                'match_score': score
            })
    
    return pd.DataFrame(matches)

# ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
name_matches = match_companies(df_patents, df_compustat, threshold=85)

# ä½ã‚¹ã‚³ã‚¢ãƒãƒƒãƒã¯æ‰‹å‹•æ¤œè¨¼
manual_review = name_matches[name_matches['match_score'] < 90]
print(f"Manual review needed: {len(manual_review)} cases")

# ç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ã¨Compustatã‚’ãƒãƒ¼ã‚¸
df_patents_matched = df_patents.merge(
    name_matches[['patent_name', 'gvkey']],
    left_on='clean_name',
    right_on='patent_name',
    how='inner'
)
```

**C. Panel Data Construction**

```python
# 1. ä¼æ¥­-å¹´ãƒ¬ãƒ™ãƒ«ã«é›†ç´„
# ç‰¹è¨±ï¼šå¹´æ¬¡é›†è¨ˆ
patents_annual = df_patents_matched.groupby(['gvkey', 'year']).agg({
    'patent_number': 'count',
    'cited_patent_number': lambda x: x.apply(len).mean(),
    'uspc_mainclass_id': lambda x: x.nunique()
}).rename(columns={
    'patent_number': 'patent_count',
    'cited_patent_number': 'avg_citations',
    'uspc_mainclass_id': 'tech_classes'
}).reset_index()

# 2. Compustatã¨ãƒãƒ¼ã‚¸
df_panel = pd.merge(
    df_compustat,
    patents_annual,
    on=['gvkey', 'year'],
    how='left'  # ç‰¹è¨±ãªã—ä¼æ¥­ã‚‚ä¿æŒ
)

# ç‰¹è¨±ãªã—ä¼æ¥­ã¯0åŸ‹ã‚
df_panel[['patent_count', 'avg_citations', 'tech_classes']].fillna(0, inplace=True)

# 3. CRSPãƒ‡ãƒ¼ã‚¿è¿½åŠ 
df_panel = pd.merge(
    df_panel,
    df_crsp[['permno', 'year', 'ret', 'prc', 'shrout']],
    on=['permno', 'year'],
    how='left'
)

# 4. M&Aãƒ‡ãƒ¼ã‚¿è¿½åŠ 
df_panel = pd.merge(
    df_panel,
    df_ma[['gvkey', 'year', 'ma_count', 'ma_value']],
    on=['gvkey', 'year'],
    how='left'
)

# 5. ãƒã‚¯ãƒ­ãƒ»åˆ¶åº¦ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
df_panel = pd.merge(
    df_panel,
    df_macro[['country', 'year', 'gdp_growth', 'rule_of_law']],
    on=['country', 'year'],
    how='left'
)

print(f"Final panel: {df_panel['firm_id'].nunique()} firms, "
      f"{df_panel['year'].nunique()} years, "
      f"{len(df_panel)} observations")
```

#### 5.2 Merge Validation

```python
# 1. Record count check
print("Merge validation:")
print(f"Pre-merge Compustat: {len(df_compustat)}")
print(f"Pre-merge Patents: {df_patents_matched['gvkey'].nunique()} firms")
print(f"Post-merge Panel: {len(df_panel)}")

# 2. Key variable preservation
assert df_compustat['at'].sum() == df_panel['at'].sum(), \
    "Total assets sum changed!"

# 3. Missing pattern analysis
merge_stats = df_panel.groupby('_merge')['gvkey'].count()
print("\nMerge statistics:")
print(merge_stats)
print(f"Match rate: {merge_stats['both']/len(df_panel)*100:.1f}%")

# 4. Systematic bias test
from scipy.stats import ttest_ind

matched = df_panel[df_panel['_merge'] == 'both']
unmatched = df_panel[df_panel['_merge'] != 'both']

t_stat, p_val = ttest_ind(
    matched['total_assets'].dropna(),
    unmatched['total_assets'].dropna()
)

print(f"\nSize difference test: t={t_stat:.2f}, p={p_val:.4f}")
if p_val < 0.05:
    print("âš ï¸ Warning: Matched firms significantly different from unmatched")

# 5. Temporal coverage check
coverage = df_panel.groupby('year').agg({
    'gvkey': 'nunique',
    'patent_count': lambda x: (x > 0).sum()
})
print("\nTemporal coverage:")
print(coverage)
```

---

### Phase 6: Advanced Quality Assurance

**ç›®çš„**ï¼šPublication-readyå“è³ªåŸºæº–ã‚’æº€ãŸã™ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼

ã“ã®ãƒ•ã‚§ãƒ¼ã‚ºã¯ã€ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆSMJ, AMJ, ASQ, OSï¼‰ã®æŸ»èª­åŸºæº–ã‚’æº€ãŸã™ãŸã‚ã«**å¿…é ˆ**ã§ã™ã€‚

#### 6.1 Multivariate Outlier Detection (Ensemble)

```python
from data_quality_checker import AdvancedQualityAssurance

# QAã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
qa = AdvancedQualityAssurance(
    df_panel,
    firm_id='gvkey',
    time_var='year',
    verbose=True
)

# åŒ…æ‹¬çš„QAå®Ÿè¡Œ
qa_report = qa.run_comprehensive_qa()

# ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢æ¤œå‡ºçµæœ
outlier_summary = qa_report['outliers']
print(f"Total outliers: {outlier_summary['total_outliers']}")
print(f"High confidence: {outlier_summary['high_confidence_outliers']}")
print(f"Methods used: {outlier_summary['methods']}")

# ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ãƒ•ãƒ©ã‚°ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ ã•ã‚Œã‚‹
# df_panel['outlier_flag'] = 1 if outlier
# df_panel['outlier_confidence'] = 0.0-1.0

# é«˜ä¿¡é ¼åº¦ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢ã®èª¿æŸ»
high_conf_outliers = df_panel[
    df_panel['outlier_confidence'] >= 0.67
]

print(f"\nTop 10 outliers:")
print(high_conf_outliers.nlargest(10, 'outlier_confidence')[
    ['firm_name', 'year', 'roa', 'total_assets', 'outlier_confidence']
])
```

**å‡¦ç†æ–¹é‡**ï¼š
- High confidence (3/3 methods): å€‹åˆ¥èª¿æŸ» â†’ ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ãªã‚‰é™¤å¤–
- Medium confidence (2/3 methods): ãƒ•ãƒ©ã‚°ä»˜ãã§ä¿æŒ
- Low confidence (1/3 methods): ãã®ã¾ã¾ä¿æŒ

#### 6.2 Benford's Law Testï¼ˆä¸æ­£æ¤œå‡ºï¼‰

```python
# Benford's Lawæ¤œå®šï¼ˆè‡ªå‹•å®Ÿè¡Œæ¸ˆã¿ï¼‰
benford_result = qa_report['benfords_law']

if not benford_result['conforms_to_benford']:
    print("âš ï¸ WARNING: Benford's Law violation detected")
    print(f"   Ï‡Â² = {benford_result['chi2_statistic']:.2f}")
    print(f"   p-value = {benford_result['p_value']:.4f}")
    
    # ç–‘ã‚ã—ã„å¤‰æ•°ã‚’ç‰¹å®š
    for var, test in benford_result['variable_tests'].items():
        if test['p_value'] < 0.05:
            print(f"   Suspicious variable: {var} (p={test['p_value']:.4f})")
    
    print("\n   Next steps:")
    print("   1. Verify data collection process")
    print("   2. Check for artificial constraints")
    print("   3. Review data source documentation")
    print("   4. Document in limitations section")
```

**Benford's Lawä¾‹å¤–**ï¼š
- äººç‚ºçš„åˆ¶ç´„ï¼ˆæœ€ä½è³‡æœ¬é‡‘è¦ä»¶ãªã©ï¼‰
- å°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆn<100ï¼‰
- IDç•ªå·ãƒ»ã‚³ãƒ¼ãƒ‰

#### 6.3 Structural Break Detection (Chow Test)

```python
# æ§‹é€ å¤‰åŒ–æ¤œå®š
break_result = qa_report['structural_breaks']

if break_result['breaks_detected'] > 0:
    print(f"ğŸ” {break_result['breaks_detected']} structural breaks detected:")
    
    for bp in break_result['break_points']:
        print(f"\n   Year {bp['time']}:")
        print(f"   F-statistic: {bp['f_statistic']:.2f}")
        print(f"   p-value: {bp['p_value']:.4f}")
        print(f"   Variables affected: {', '.join(bp['affected_vars'])}")
    
    # æ—¢çŸ¥ã®ã‚¤ãƒ™ãƒ³ãƒˆç¢ºèª
    known_events = {
        2008: "Financial Crisis",
        2001: "Dot-com Crash",
        2020: "COVID-19",
        2002: "Sarbanes-Oxley Act"
    }
    
    for bp in break_result['break_points']:
        year = bp['time']
        if year in known_events:
            print(f"   â†’ Known event: {known_events[year]}")
        else:
            print(f"   â†’ Unknown cause - investigate")
```

**å¯¾å‡¦æ³•**ï¼š
- æ—¢çŸ¥ã‚¤ãƒ™ãƒ³ãƒˆ â†’ Post-period dummyã§çµ±åˆ¶
- ä¼šè¨ˆåŸºæº–å¤‰æ›´ â†’ å¤‰æ›´å‰å¾Œã§åˆ†æã‚’åˆ†ã‘ã‚‹
- ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ â†’ è©²å½“æœŸé–“é™¤å¤–

#### 6.4 Accounting Identity Verification

```python
# ä¼šè¨ˆæ’ç­‰å¼ãƒã‚§ãƒƒã‚¯
# Balance Sheet: Assets = Liabilities + Equity

df_panel['bs_error'] = abs(
    df_panel['at'] - (df_panel['lt'] + df_panel['ceq'])
)
df_panel['bs_error_pct'] = df_panel['bs_error'] / df_panel['at']

bs_violations = df_panel[df_panel['bs_error_pct'] > 0.01]

print(f"Balance Sheet violations (>1%): {len(bs_violations)} "
      f"({len(bs_violations)/len(df_panel)*100:.2f}%)")

if len(bs_violations) / len(df_panel) > 0.05:
    print("âš ï¸ WARNING: >5% balance sheet errors detected")
    print("   Possible data source issue")
    
    # å¹´åº¦åˆ¥ã‚¨ãƒ©ãƒ¼ç‡
    annual_errors = df_panel.groupby('year')['bs_error_pct'].agg([
        ('error_rate', lambda x: (x > 0.01).sum() / len(x))
    ])
    print("\nError rate by year:")
    print(annual_errors[annual_errors['error_rate'] > 0.05])
```

**è¨±å®¹ç¯„å›²**ï¼š
- <1%: ä¸¸ã‚èª¤å·®ï¼ˆè¨±å®¹ï¼‰
- 1-5%: ãƒ•ãƒ©ã‚°ä»˜ãã§ä¿æŒ
- >5%: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å•é¡Œ â†’ èª¿æŸ»ãƒ»é™¤å¤–

#### 6.5 Panel Balance & Attrition Analysis

```python
# ãƒ‘ãƒãƒ«ãƒãƒ©ãƒ³ã‚¹åˆ†æ
balance_analysis = qa_report['selection_bias']

print("Panel Structure:")
print(f"Total firms: {balance_analysis['total_firms']}")
print(f"Balanced panel firms: {balance_analysis['balanced_firms']}")
print(f"Balance rate: {balance_analysis['balance_rate']:.1f}%")
print(f"Attrition rate: {balance_analysis['attrition_rate']:.1f}%")

# é«˜Attritionç‡ã®è­¦å‘Š
if balance_analysis['attrition_rate'] > 30:
    print("\nâš ï¸ WARNING: High attrition rate (>30%)")
    print("   Survival bias likely present")
    print("   Consider:")
    print("   - Heckman selection model")
    print("   - Inverse probability weighting")
    print("   - Explicit modeling of exit")

# Attritorã®ç‰¹æ€§åˆ†æ
df_panel['attrite'] = df_panel.groupby('firm_id')['year'].transform(
    lambda x: 1 if x.max() < df_panel['year'].max() else 0
)

from scipy.stats import ttest_ind

attrite_firms = df_panel[df_panel['attrite'] == 1]
survive_firms = df_panel[df_panel['attrite'] == 0]

print("\nAttrite vs. Survive comparison:")
for var in ['roa', 'total_assets', 'leverage']:
    t, p = ttest_ind(
        attrite_firms[var].dropna(),
        survive_firms[var].dropna()
    )
    print(f"{var}: t={t:.2f}, p={p:.4f}")
    if p < 0.05:
        print(f"  â†’ {var} significantly different")
```

#### 6.6 Statistical Power - Post-Hoc Check

```python
# å®Ÿç¾æ¤œå‡ºåŠ›ã®ç¢ºèª
from statsmodels.stats.power import TTestIndPower

analysis = TTestIndPower()

# å®Ÿéš›ã®åŠ¹æœé‡ã‚’è¨ˆç®—
effect_size = (
    df_panel.groupby('treatment')['roa'].mean().diff().iloc[-1] /
    df_panel['roa'].std()
)

# å®Ÿç¾æ¤œå‡ºåŠ›
achieved_power = analysis.solve_power(
    effect_size=effect_size,
    nobs1=df_panel['treatment'].value_counts().min(),
    alpha=0.05,
    alternative='two-sided'
)

print(f"Achieved Statistical Power: {achieved_power:.2%}")

if achieved_power < 0.80:
    print("âš ï¸ WARNING: Study is underpowered (<80%)")
    print(f"   Current power: {achieved_power:.2%}")
    print(f"   Effect size: {effect_size:.3f}")
    print("   Consider:")
    print("   - Increasing sample size")
    print("   - Adjusting expectations")
    print("   - Reporting as exploratory")
```

#### 6.7 Quality Assurance Documentation

```python
# QAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
qa.generate_report(
    output_formats=['html', 'pdf', 'json'],
    output_dir='./qa_reports/'
)

# ä¸»è¦çµ±è¨ˆã®ä¿å­˜
qa_summary = {
    'sample_size': len(df_panel),
    'num_firms': df_panel['firm_id'].nunique(),
    'time_span': f"{df_panel['year'].min()}-{df_panel['year'].max()}",
    'outliers_detected': qa_report['outliers']['total_outliers'],
    'outlier_rate': qa_report['outliers']['total_outliers'] / len(df_panel),
    'benford_conformance': qa_report['benfords_law']['conforms_to_benford'],
    'structural_breaks': qa_report['structural_breaks']['breaks_detected'],
    'attrition_rate': balance_analysis['attrition_rate'],
    'achieved_power': achieved_power
}

# JSONä¿å­˜
import json
with open('./qa_reports/qa_summary.json', 'w') as f:
    json.dump(qa_summary, f, indent=2)

print("\nâœ… Quality Assurance Complete")
print(f"   Reports saved to: ./qa_reports/")
```

---

### Phase 7: Statistical Analysis & Theory Testing

**ç›®çš„**ï¼šä»®èª¬æ¤œè¨¼ã¨ç†è«–æ§‹ç¯‰

#### 7.1 Descriptive Statistics & Correlations

```python
# è¨˜è¿°çµ±è¨ˆï¼ˆTable 1ï¼‰
desc_stats = df_panel[key_variables].describe().T
desc_stats['N'] = df_panel[key_variables].count()

print("Table 1: Descriptive Statistics")
print(desc_stats[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']])

# ç›¸é–¢è¡Œåˆ—ï¼ˆTable 2ï¼‰
corr_matrix = df_panel[key_variables].corr()

# VIFï¼ˆå¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯ï¼‰
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = df_panel[independent_vars].dropna()
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) 
                   for i in range(X.shape[1])]

print("\nVariance Inflation Factors:")
print(vif_data)

# VIF > 10 â†’ æ·±åˆ»ãªå¤šé‡å…±ç·šæ€§
high_vif = vif_data[vif_data['VIF'] > 10]
if len(high_vif) > 0:
    print("\nâš ï¸ WARNING: High multicollinearity detected:")
    print(high_vif)
```

#### 7.2 Panel Regression Models

```python
import statsmodels.formula.api as smf
from linearmodels.panel import PanelOLS

# ãƒ‡ãƒ¼ã‚¿ã‚’panelã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
df_panel = df_panel.set_index(['firm_id', 'year'])

# Model 1: Pooled OLS (Baseline)
model1 = smf.ols(
    'roa ~ rd_intensity_lag1 + firm_size + leverage + firm_age',
    data=df_panel
).fit(cov_type='cluster', cov_kwds={'groups': df_panel.index.get_level_values(0)})

print("Model 1: Pooled OLS")
print(model1.summary())

# Model 2: Fixed Effects
model2 = PanelOLS.from_formula(
    'roa ~ rd_intensity_lag1 + firm_size + leverage + firm_age + EntityEffects + TimeEffects',
    data=df_panel
).fit(cov_type='clustered', cluster_entity=True)

print("\nModel 2: Fixed Effects")
print(model2.summary)

# Model 3: Interaction Effectsï¼ˆç†è«–æ¤œè¨¼ï¼‰
model3 = PanelOLS.from_formula(
    'roa ~ rd_intensity_lag1 * env_dynamism + firm_size + leverage + firm_age + EntityEffects + TimeEffects',
    data=df_panel
).fit(cov_type='clustered', cluster_entity=True)

print("\nModel 3: Moderation Analysis")
print(model3.summary)

# Interaction plot
import matplotlib.pyplot as plt

# ç’°å¢ƒå‹•æ…‹æ€§ã®é«˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã‘
high_dyn = df_panel[df_panel['env_dynamism'] > df_panel['env_dynamism'].median()]
low_dyn = df_panel[df_panel['env_dynamism'] <= df_panel['env_dynamism'].median()]

plt.figure(figsize=(10, 6))
plt.scatter(high_dyn['rd_intensity_lag1'], high_dyn['roa'], 
            alpha=0.3, label='High Dynamism')
plt.scatter(low_dyn['rd_intensity_lag1'], low_dyn['roa'], 
            alpha=0.3, label='Low Dynamism')
plt.xlabel('R&D Intensity (t-1)')
plt.ylabel('ROA')
plt.legend()
plt.title('Moderating Effect of Environmental Dynamism')
plt.savefig('./figures/interaction_plot.png', dpi=300)
```

#### 7.3 Robustness Checks

**A. Alternative Specifications**

```python
# DVæ›¿æ›¿ï¼ˆROA â†’ ROE, Tobin's Qï¼‰
robustness_models = {}

for dv in ['roe', 'tobins_q', 'ros']:
    formula = f'{dv} ~ rd_intensity_lag1 + firm_size + leverage + firm_age + EntityEffects + TimeEffects'
    model = PanelOLS.from_formula(formula, data=df_panel).fit(
        cov_type='clustered', cluster_entity=True
    )
    robustness_models[dv] = model
    
print("Robustness Check: Alternative DVs")
for dv, model in robustness_models.items():
    print(f"\n{dv.upper()}:")
    print(f"  R&D coefficient: {model.params['rd_intensity_lag1']:.4f}")
    print(f"  p-value: {model.pvalues['rd_intensity_lag1']:.4f}")
```

**B. Alternative Samples**

```python
# ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒ«åˆ†æ
subsamples = {
    'exclude_crisis': df_panel[~df_panel['year'].isin([2008, 2009, 2020])],
    'exclude_outliers': df_panel[df_panel['outlier_flag'] == 0],
    'balanced_only': df_panel.groupby('firm_id').filter(
        lambda x: len(x) == df_panel['year'].nunique()
    )
}

for name, subsample in subsamples.items():
    subsample = subsample.set_index(['firm_id', 'year'])
    model = PanelOLS.from_formula(
        'roa ~ rd_intensity_lag1 + firm_size + leverage + firm_age + EntityEffects + TimeEffects',
        data=subsample
    ).fit(cov_type='clustered', cluster_entity=True)
    
    print(f"\nRobustness: {name}")
    print(f"  N: {len(subsample)}")
    print(f"  R&D coef: {model.params['rd_intensity_lag1']:.4f} (p={model.pvalues['rd_intensity_lag1']:.4f})")
```

**C. Endogeneity Tests**

```python
# Hausman test (FE vs. RE)
from linearmodels.panel import RandomEffects

re_model = RandomEffects.from_formula(
    'roa ~ rd_intensity_lag1 + firm_size + leverage + firm_age + EntityEffects',
    data=df_panel
).fit()

# Hausmançµ±è¨ˆé‡
hausman_stat = model2.comp arison(re_model)
print(f"\nHausman Test: Ï‡Â²={hausman_stat['statistic']:.2f}, p={hausman_stat['pvalue']:.4f}")

if hausman_stat['pvalue'] < 0.05:
    print("  â†’ FE preferred (reject RE)")
else:
    print("  â†’ RE acceptable")
```

**D. Instrumental Variables (if needed)**

```python
from linearmodels.iv import IV2SLS

# Instrument: ç”£æ¥­å¹³å‡R&D intensity
df_panel['industry_avg_rd'] = df_panel.groupby(['industry', 'year'])['rd_intensity'].transform('mean')

iv_model = IV2SLS.from_formula(
    'roa ~ [rd_intensity_lag1 ~ industry_avg_rd] + firm_size + leverage + firm_age',
    data=df_panel
).fit(cov_type='clustered', clusters=df_panel.index.get_level_values(0))

print("\nIV Model:")
print(iv_model.summary)

# First-stage F-stat check
print(f"First-stage F-stat: {iv_model.first_stage.diagnostics['f.stat']:.2f}")
if iv_model.first_stage.diagnostics['f.stat'] < 10:
    print("âš ï¸ WARNING: Weak instrument (F < 10)")
```

---

### Phase 8: Documentation & Replication Package

**ç›®çš„**ï¼šå®Œå…¨å†ç¾å¯èƒ½ãªç ”ç©¶ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆ

#### 8.1 Data Dictionary

```python
# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªè‡ªå‹•ç”Ÿæˆ
data_dict = []

for var in df_panel.columns:
    data_dict.append({
        'Variable': var,
        'Description': variable_descriptions.get(var, ''),  # äº‹å‰å®šç¾©
        'Source': variable_sources.get(var, ''),
        'Unit': variable_units.get(var, ''),
        'Construction': variable_formulas.get(var, ''),
        'N': df_panel[var].count(),
        'Mean': df_panel[var].mean(),
        'SD': df_panel[var].std(),
        'Min': df_panel[var].min(),
        'Max': df_panel[var].max()
    })

dd_df = pd.DataFrame(data_dict)
dd_df.to_excel('./documentation/data_dictionary.xlsx', index=False)
dd_df.to_csv('./documentation/data_dictionary.csv', index=False)

print("Data dictionary saved")
```

#### 8.2 Replication Package Structure

```bash
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # åŸãƒ‡ãƒ¼ã‚¿ï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰
â”‚   â”‚   â”œâ”€â”€ compustat_raw.csv
â”‚   â”‚   â”œâ”€â”€ patents_raw.csv
â”‚   â”‚   â””â”€â”€ README.md           # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †
â”‚   â”œâ”€â”€ processed/              # ä¸­é–“å‡¦ç†ãƒ‡ãƒ¼ã‚¿
â”‚   â”‚   â”œâ”€â”€ cleaned_financials.csv
â”‚   â”‚   â”œâ”€â”€ patent_metrics.csv
â”‚   â”‚   â””â”€â”€ industry_vars.csv
â”‚   â””â”€â”€ final/                  # åˆ†æç”¨æœ€çµ‚ãƒ‡ãƒ¼ã‚¿
â”‚       â”œâ”€â”€ analysis_panel.dta
â”‚       â”œâ”€â”€ analysis_panel.csv
â”‚       â””â”€â”€ analysis_panel.parquet
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_download_data.py     # ãƒ‡ãƒ¼ã‚¿å–å¾—
â”‚   â”œâ”€â”€ 02_clean_financials.py  # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
â”‚   â”œâ”€â”€ 03_construct_variables.py  # å¤‰æ•°æ§‹ç¯‰
â”‚   â”œâ”€â”€ 04_merge_datasets.py    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
â”‚   â”œâ”€â”€ 05_quality_checks.py    # å“è³ªä¿è¨¼
â”‚   â”œâ”€â”€ 06_descriptive_stats.py # è¨˜è¿°çµ±è¨ˆ
â”‚   â”œâ”€â”€ 07_main_analysis.py     # ãƒ¡ã‚¤ãƒ³åˆ†æ
â”‚   â”œâ”€â”€ 08_robustness_checks.py # é ‘å¥æ€§ãƒã‚§ãƒƒã‚¯
â”‚   â””â”€â”€ utils/                  # ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
â”‚       â”œâ”€â”€ data_cleaning.py
â”‚       â”œâ”€â”€ variable_construction.py
â”‚       â””â”€â”€ qa_tools.py
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ tables/                 # è«–æ–‡ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«
â”‚   â”‚   â”œâ”€â”€ table1_descriptives.tex
â”‚   â”‚   â”œâ”€â”€ table2_correlations.tex
â”‚   â”‚   â”œâ”€â”€ table3_main_results.tex
â”‚   â”‚   â””â”€â”€ table4_robustness.tex
â”‚   â”œâ”€â”€ figures/                # è«–æ–‡ç”¨å›³
â”‚   â”‚   â”œâ”€â”€ figure1_conceptual_model.png
â”‚   â”‚   â”œâ”€â”€ figure2_interaction_plot.png
â”‚   â”‚   â””â”€â”€ figure3_marginal_effects.png
â”‚   â””â”€â”€ logs/                   # å®Ÿè¡Œãƒ­ã‚°
â”‚       â”œâ”€â”€ qa_log_20250131.txt
â”‚       â””â”€â”€ analysis_log_20250131.txt
â”œâ”€â”€ documentation/
â”‚   â”œâ”€â”€ data_dictionary.xlsx    # ãƒ‡ãƒ¼ã‚¿è¾æ›¸
â”‚   â”œâ”€â”€ codebook.pdf            # ç ”ç©¶æ‰‹é †æ›¸
â”‚   â”œâ”€â”€ variable_definitions.md # å¤‰æ•°å®šç¾©è©³ç´°
â”‚   â”œâ”€â”€ qa_report.html          # å“è³ªä¿è¨¼ãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â””â”€â”€ sample_construction.md  # ã‚µãƒ³ãƒ—ãƒ«æ§‹ç¯‰æ‰‹é †
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_integrity.py  # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_variable_construction.py
â”‚   â””â”€â”€ test_merge_logic.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile              # å†ç¾ç’°å¢ƒ
â”‚   â””â”€â”€ requirements.txt        # Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”œâ”€â”€ README.md                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
â”œâ”€â”€ REPLICATION.md              # å†ç¾æ‰‹é †
â”œâ”€â”€ LICENSE.md
â””â”€â”€ requirements.txt
```

#### 8.3 REPLICATION.md Template

```markdown
# Replication Instructions

## Paper
[Author]. ([Year]). "[Title]". *Journal*, Volume(Issue), pages.

## System Requirements
- Python 3.9+
- R 4.2+ (optional, for additional analyses)
- RAM: 16GB minimum, 32GB recommended
- Storage: 50GB free space

## Data Access

### Required Subscriptions
1. **WRDS (Wharton Research Data Services)**
   - Compustat North America
   - CRSP
   - Access: University subscription
   - Username/password required

2. **PatentsView**
   - Free bulk download
   - URL: https://patentsview.org/download/
   - No authentication required

### Free Data Sources
- World Bank API
- USPTO PatentsView
- See `data/raw/README.md` for download instructions

## Installation

### Option 1: Docker (Recommended)
```bash
# Build container
docker build -t strategic-research ./docker/

# Run container
docker run -it -v $(pwd):/workspace strategic-research
```

### Option 2: Local Installation
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Execution

### Full Replication (2-4 hours)
```bash
# Run all scripts in sequence
bash run_all.sh
```

### Step-by-Step
```bash
# 1. Download raw data
python scripts/01_download_data.py

# 2. Clean and process
python scripts/02_clean_financials.py
python scripts/03_construct_variables.py
python scripts/04_merge_datasets.py

# 3. Quality checks
python scripts/05_quality_checks.py

# 4. Analysis
python scripts/06_descriptive_stats.py
python scripts/07_main_analysis.py
python scripts/08_robustness_checks.py
```

### Testing
```bash
# Run test suite
pytest tests/
```

## Expected Output

### Tables
- Table 1: Descriptive Statistics (N=15,234 firm-years)
- Table 2: Correlation Matrix
- Table 3: Main Regression Results
- Table 4: Robustness Checks

### Figures
- Figure 1: Conceptual Model
- Figure 2: Interaction Plot (R&D Ã— Environment Dynamism)
- Figure 3: Marginal Effects

### Datasets
- `data/final/analysis_panel.dta`: Stata format (10.5 MB)
- `data/final/analysis_panel.csv`: CSV format (15.2 MB)

## Deviations from Published Results

Minor numerical differences (<0.001) may occur due to:
- Rounding in intermediate steps
- Random seed in bootstrap procedures
- Software version differences

Replication should match published results within 1% for all coefficients.

## Troubleshooting

### Issue: WRDS Connection Failed
**Solution**: Check username/password in `config/wrds_credentials.ini`

### Issue: Memory Error
**Solution**: Increase virtual memory or process data in chunks

### Issue: Missing Patents Data
**Solution**: Re-download from PatentsView bulk files

## Citation
If you use this replication package, please cite:
```
[Author] ([Year]). "Replication Package for: [Title]". 
[Repository URL]. DOI: [DOI if applicable]
```

## Contact
[Your Name]
[Email]
[Institution]

Last updated: 2025-01-31
```

#### 8.4 Pytest Test Suite

```python
# tests/test_data_integrity.py

import pytest
import pandas as pd

@pytest.fixture
def analysis_data():
    """Load final analysis dataset"""
    return pd.read_stata('./data/final/analysis_panel.dta')

def test_no_missing_key_vars(analysis_data):
    """Ensure no missing values in key variables"""
    key_vars = ['gvkey', 'year', 'roa', 'total_assets']
    assert analysis_data[key_vars].notna().all().all(), \
        "Missing values detected in key variables"

def test_year_range(analysis_data):
    """Verify year range"""
    assert analysis_data['year'].min() == 2000, "Start year incorrect"
    assert analysis_data['year'].max() == 2023, "End year incorrect"

def test_balance_sheet_identity(analysis_data):
    """Test accounting identity: Assets = Liabilities + Equity"""
    df = analysis_data.copy()
    df['bs_error'] = abs(df['at'] - (df['lt'] + df['ceq'])) / df['at']
    
    error_rate = (df['bs_error'] > 0.01).sum() / len(df)
    assert error_rate < 0.05, \
        f"Balance sheet error rate too high: {error_rate:.2%}"

def test_no_negative_assets(analysis_data):
    """Assets should be non-negative"""
    assert (analysis_data['at'] >= 0).all(), \
        "Negative assets detected"

def test_winsorization_bounds(analysis_data):
    """Verify winsorization applied correctly"""
    for var in ['roa', 'leverage', 'tobins_q']:
        p1 = analysis_data[var].quantile(0.01)
        p99 = analysis_data[var].quantile(0.99)
        
        # Winsorized valuesã¯1%ileã€œ99%ileå†…
        assert (analysis_data[var] >= p1).all(), \
            f"{var}: Values below 1st percentile"
        assert (analysis_data[var] <= p99).all(), \
            f"{var}: Values above 99th percentile"

def test_panel_structure(analysis_data):
    """Verify panel structure"""
    firms = analysis_data['gvkey'].nunique()
    years = analysis_data['year'].nunique()
    
    expected_max_obs = firms * years
    actual_obs = len(analysis_data)
    
    # Unbalanced panelãªã®ã§ã€actual < expected
    assert actual_obs <= expected_max_obs, \
        "More observations than possible in panel"
    
    # æœ€ä½3å¹´ã®ãƒ‡ãƒ¼ã‚¿ä¿æœ‰ã‚’ç¢ºèª
    firm_years = analysis_data.groupby('gvkey')['year'].count()
    assert (firm_years >= 3).all(), \
        "Some firms have fewer than 3 years of data"

def test_lagged_variables(analysis_data):
    """Verify lagged variables constructed correctly"""
    df = analysis_data.sort_values(['gvkey', 'year'])
    
    # tæœŸã®ãƒ©ã‚°å¤‰æ•° = t-1æœŸã®å®Ÿç¸¾å€¤
    for gvkey in df['gvkey'].unique()[:10]:  # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚§ãƒƒã‚¯
        firm_data = df[df['gvkey'] == gvkey]
        
        for i in range(1, len(firm_data)):
            expected_lag = firm_data.iloc[i-1]['rd_intensity']
            actual_lag = firm_data.iloc[i]['rd_intensity_lag1']
            
            # NaNã¯é™¤å¤–
            if pd.notna(expected_lag) and pd.notna(actual_lag):
                assert abs(expected_lag - actual_lag) < 0.0001, \
                    f"Lag construction error for {gvkey}"

# tests/test_variable_construction.py

def test_entropy_index_range(analysis_data):
    """Entropy indexã¯0ä»¥ä¸Š"""
    assert (analysis_data['entropy_index'] >= 0).all(), \
        "Negative entropy detected"
    
def test_tobins_q_calculation(analysis_data):
    """Tobin's Qè¨ˆç®—æ¤œè¨¼"""
    df = analysis_data.dropna(subset=['market_cap', 'total_debt', 'cash', 'at'])
    
    calculated_q = (df['market_cap'] + df['total_debt'] - df['cash']) / df['at']
    
    # è¨ˆç®—å€¤ã¨ä¿å­˜å€¤ã®ä¸€è‡´ç¢ºèª
    assert ((calculated_q - df['tobins_q']).abs() < 0.01).all(), \
        "Tobin's Q calculation mismatch"

def test_roa_calculation(analysis_data):
    """ROAè¨ˆç®—æ¤œè¨¼"""
    df = analysis_data.dropna(subset=['ni', 'at'])
    
    calculated_roa = df['ni'] / df['at']
    
    assert ((calculated_roa - df['roa']).abs() < 0.0001).all(), \
        "ROA calculation mismatch"
```

---

### Phase 7.5: Theoretical Contribution Articulation

**ç›®çš„**ï¼šç ”ç©¶ã®ç†è«–çš„è²¢çŒ®ã‚’æ˜ç¢ºåŒ–

#### 7.5.1 Theory Building Framework

**A. ç†è«–çš„è²¢çŒ®ã®3é¡å‹ï¼ˆå†æ²ãƒ»è©³ç´°åŒ–ï¼‰**

**Type 1: Theoretical Challengeï¼ˆç†è«–ã¸ã®æŒ‘æˆ¦ï¼‰**
```
æ—¢å­˜ç†è«–ãŒèª¬æ˜ã§ããªã„ç¾è±¡ã®æç¤º

ä¾‹ï¼šDynamic Capabilitiesç ”ç©¶
æ—¢å­˜ç†è«–ï¼šPorter (1980) - ã‚³ã‚¹ãƒˆãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã¨å·®åˆ¥åŒ–ã¯æ’ä»–çš„
è¦³å¯Ÿç¾è±¡ï¼šã‚­ãƒ¼ã‚¨ãƒ³ã‚¹ã¯é«˜ä¾¡æ ¼ãƒ»é«˜ã‚·ã‚§ã‚¢ã‚’ä¸¡ç«‹
ç†è«–çš„æŒ‘æˆ¦ï¼šã€Œçµ„ç¹”çš„ç›¸è£œæ€§ã«ã‚ˆã‚Šã€ä¸¡æˆ¦ç•¥ã®çµ±åˆãŒå¯èƒ½ã§ã‚ã‚‹ã€

è²¢çŒ®ï¼šPorterã®å‰æï¼ˆstrategic trade-offï¼‰ã®é™ç•Œæ¡ä»¶ã‚’æ˜ç¤º
```

**Type 2: Theoretical Extensionï¼ˆç†è«–ã®æ‹¡å¼µï¼‰**
```
ç†è«–ã®é©ç”¨ç¯„å›²ã‚’æ–°ã—ã„æ–‡è„ˆã«æ‹¡å¤§

ä¾‹ï¼šInstitutional Theoryç ”ç©¶
æ—¢å­˜ç†è«–ï¼šDiMaggio & Powell (1983) - åˆ¶åº¦çš„åŒå‹åŒ–ï¼ˆå…ˆé€²å›½ï¼‰
æ‹¡å¼µï¼šæ–°èˆˆå¸‚å ´ã«ãŠã‘ã‚‹åˆ¶åº¦çš„ãƒœã‚¤ãƒ‰ï¼ˆinstitutional voidsï¼‰
æ–°æ¦‚å¿µï¼šã€Œåˆ¶åº¦çš„èµ·æ¥­å®¶ç²¾ç¥ã€- ä¼æ¥­ãŒèƒ½å‹•çš„ã«åˆ¶åº¦ã‚’å½¢æˆ

è²¢çŒ®ï¼šç†è«–ãŒæƒ³å®šã—ãªã„æ–‡è„ˆï¼ˆweak institutionsï¼‰ã§ã®æ–°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
```

**Type 3: Theoretical Integrationï¼ˆç†è«–ã®çµ±åˆï¼‰**
```
ç•°ãªã‚‹ç†è«–çš„è¦–ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‹

ä¾‹ï¼šRBV Ã— Institutional Theory
RBVï¼šä¼æ¥­å†…éƒ¨è³‡æºãŒç«¶äº‰å„ªä½ã®æºæ³‰
åˆ¶åº¦ç†è«–ï¼šå¤–éƒ¨åˆ¶åº¦ç’°å¢ƒãŒçµ„ç¹”è¡Œå‹•ã‚’è¦å®š

çµ±åˆï¼šã€Œåˆ¶åº¦ç’°å¢ƒãŒã€ã©ã®è³‡æºãŒVRINã¨ãªã‚‹ã‹ã‚’è¦å®šã™ã‚‹ã€
â†’ æ–‡åŒ–çš„æ–‡è„ˆä¾å­˜çš„ãªresource valueã®æ¦‚å¿µ

è²¢çŒ®ï¼šMicroï¼ˆä¼æ¥­ï¼‰ã¨Macroï¼ˆåˆ¶åº¦ï¼‰ã®ãƒ–ãƒªãƒƒã‚¸ç†è«–
```

#### 7.5.2 Conceptual Model Development

```python
# æ¦‚å¿µãƒ¢ãƒ‡ãƒ«ã®å¯è¦–åŒ–
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch, FancyArrowPatch

fig, ax = plt.subplots(figsize=(12, 8))

# Independent Variable
iv_box = FancyBboxPatch((1, 4), 2, 1, boxstyle="round,pad=0.1", 
                         edgecolor='black', facecolor='lightblue')
ax.add_patch(iv_box)
ax.text(2, 4.5, 'R&D Intensity\n(Dynamic Capability)', 
        ha='center', va='center', fontsize=10, weight='bold')

# Dependent Variable
dv_box = FancyBboxPatch((7, 4), 2, 1, boxstyle="round,pad=0.1",
                         edgecolor='black', facecolor='lightgreen')
ax.add_patch(dv_box)
ax.text(8, 4.5, 'Firm Performance\n(ROA)', 
        ha='center', va='center', fontsize=10, weight='bold')

# Moderator
mod_box = FancyBboxPatch((4, 6.5), 2, 1, boxstyle="round,pad=0.1",
                          edgecolor='black', facecolor='lightyellow')
ax.add_patch(mod_box)
ax.text(5, 7, 'Environmental\nDynamism', 
        ha='center', va='center', fontsize=10, weight='bold')

# Controls
ctrl_box = FancyBboxPatch((4, 1.5), 2, 1, boxstyle="round,pad=0.1",
                           edgecolor='gray', facecolor='lightgray')
ax.add_patch(ctrl_box)
ax.text(5, 2, 'Controls:\nSize, Age, Leverage', 
        ha='center', va='center', fontsize=9)

# Arrows
# Main effect
main_arrow = FancyArrowPatch((3, 4.5), (7, 4.5), 
                              arrowstyle='->', mutation_scale=20, 
                              lw=2, color='blue')
ax.add_patch(main_arrow)
ax.text(5, 4.7, 'H1 (+)', ha='center', fontsize=9, color='blue')

# Moderation
mod_arrow1 = FancyArrowPatch((5, 6.5), (3, 5.2),
                              arrowstyle='->', mutation_scale=15, 
                              lw=1.5, color='red', linestyle='dashed')
ax.add_patch(mod_arrow1)

mod_arrow2 = FancyArrowPatch((5, 6.5), (7, 5.2),
                              arrowstyle='->', mutation_scale=15, 
                              lw=1.5, color='red', linestyle='dashed')
ax.add_patch(mod_arrow2)
ax.text(6, 6, 'H2 (moderation)', ha='center', fontsize=9, color='red')

# Control arrows
ctrl_arrow = FancyArrowPatch((5, 2.5), (7, 4),
                              arrowstyle='->', mutation_scale=15, 
                              lw=1, color='gray', linestyle='dotted')
ax.add_patch(ctrl_arrow)

ax.set_xlim(0, 10)
ax.set_ylim(0, 9)
ax.axis('off')
plt.title('Conceptual Model', fontsize=14, weight='bold')
plt.tight_layout()
plt.savefig('./figures/conceptual_model.png', dpi=300, bbox_inches='tight')
plt.show()

print("Conceptual model saved to ./figures/conceptual_model.png")
```

#### 7.5.3 Hypothesis Development Template

```markdown
## Hypotheses

### Main Effect Hypothesis

**H1: R&D intensity positively affects firm performance.**

**Theoretical Rationale:**
Resource-Based View (Barney, 1991)ã¯ã€ä¼æ¥­å›ºæœ‰ã®è³‡æºãŒç«¶äº‰å„ªä½ã®æºæ³‰ã¨ãªã‚‹ã¨ä¸»å¼µã™ã‚‹ã€‚R&DæŠ•è³‡ã¯çŸ¥è­˜è³‡ç”£ã‚’è“„ç©ã—ã€VRINï¼ˆValuable, Rare, Inimitable, Non-substitutableï¼‰è³‡æºã¨ãªã‚‹ï¼ˆDierickx & Cool, 1989ï¼‰ã€‚Dynamic Capabilitiesç†è«–ï¼ˆTeece et al., 1997ï¼‰ã¯ã€R&Dèƒ½åŠ›ãŒç’°å¢ƒå¤‰åŒ–ã¸ã®é©å¿œã‚’å¯èƒ½ã«ã—ã€æŒç¶šçš„ç«¶äº‰å„ªä½ã‚’ç”Ÿã¿å‡ºã™ã¨è«–ã˜ã‚‹ã€‚

å®Ÿè¨¼çš„ã«ã¯ã€Grilichesï¼ˆ1981ï¼‰ãŒç‰¹è¨±ã‚¹ãƒˆãƒƒã‚¯ã¨TFPã®æ­£ã®é–¢ä¿‚ã‚’ã€Del Monte & Papagniï¼ˆ2003ï¼‰ãŒR&D intensityã¨ROAã®æ­£ã®é–¢ä¿‚ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã‚‰ã®ç†è«–çš„ãƒ»å®Ÿè¨¼çš„æ ¹æ‹ ã‹ã‚‰ã€R&D intensityãŒfirm performanceã«æ­£ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨äºˆæ¸¬ã™ã‚‹ã€‚

**Operational Hypothesis:**
$$
H_1: \beta_{\text{R&D intensity}} > 0 \text{ in } \text{ROA}_{it} = \alpha + \beta_1 \text{R&D}_{i,t-1} + \gamma X_{it} + \epsilon_{it}
$$

### Moderation Hypothesis

**H2: Environmental dynamism positively moderates the R&D-performance relationship.**

**Theoretical Rationale:**
Contingency Theoryï¼ˆLawrence & Lorsch, 1967ï¼‰ã¯ã€çµ„ç¹”æ§‹é€ ã¨ç’°å¢ƒã®é©åˆãŒé‡è¦ã§ã‚ã‚‹ã¨ä¸»å¼µã™ã‚‹ã€‚Dynamic environmentsï¼ˆé«˜ã„æŠ€è¡“å¤‰åŒ–ç‡ã€ä¸ç¢ºå®Ÿæ€§ï¼‰ã§ã¯ã€ç¶™ç¶šçš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãŒç«¶äº‰å„ªä½ã«ä¸å¯æ¬ ã¨ãªã‚‹ï¼ˆEisenhardt & Martin, 2000ï¼‰ã€‚

Static environmentsã§ã¯ã€æ—¢å­˜è£½å“ã®åŠ¹ç‡çš„ç”Ÿç”£ãŒã‚ˆã‚Šé‡è¦ã§ã‚ã‚Šã€R&DæŠ•è³‡ã®åç›Šæ€§ã¯ç›¸å¯¾çš„ã«ä½ã„ï¼ˆPorter, 1980ï¼‰ã€‚å¯¾ç…§çš„ã«ã€dynamic environmentsã§ã¯ã€R&DæŠ•è³‡ã«ã‚ˆã‚Šæ–°è£½å“ã‚’è¿…é€Ÿã«å¸‚å ´æŠ•å…¥ã§ãã‚‹ä¼æ¥­ãŒé«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ï¼ˆTeece, 2007ï¼‰ã€‚

**Operational Hypothesis:**
$$
H_2: \beta_{\text{R&D} \times \text{Dynamism}} > 0 \text{ in } \text{ROA}_{it} = \alpha + \beta_1 \text{R&D}_{i,t-1} + \beta_2 \text{Dyn}_{jt} + \beta_3 \text{R&D}_{i,t-1} \times \text{Dyn}_{jt} + \gamma X_{it} + \epsilon_{it}
$$
```

---

## Integration with Other Skills

ã“ã®ã‚¹ã‚­ãƒ«ã¯ã€ä»¥ä¸‹ã®æ—¢å­˜ã‚¹ã‚­ãƒ«ã¨çµ±åˆã—ã¦ä½¿ç”¨ã§ãã¾ã™ï¼š

### 1. academic-paper-creation skill
```
ä½¿ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼šPhase 7-8ï¼ˆåŸ·ç­†ãƒ»æŠ•ç¨¿æº–å‚™ï¼‰
çµ±åˆæ–¹æ³•ï¼š
- æœ¬ã‚¹ã‚­ãƒ«ã§åˆ†æå®Œäº†å¾Œã€academic-paper-creationã‚¹ã‚­ãƒ«ã§è«–æ–‡åŸ·ç­†
- ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã‚’æ±äº¬å¤§å­¦å¼•ç”¨ã‚¹ã‚¿ã‚¤ãƒ«ã§æ–‡æ›¸åŒ–
- 30,000å­—è¦æ¨¡ã®æœ¬æ ¼çš„è«–æ–‡ä½œæˆ

ã‚³ãƒãƒ³ãƒ‰ä¾‹ï¼š
ã€ŒPhase 7ã®åˆ†æçµæœã‚’åŸºã«ã€academic-paper-creation skillã‚’ä½¿ç”¨ã—ã¦
 SMJæŠ•ç¨¿ç”¨ã®è«–æ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€
```

### 2. xlsx skill
```
ä½¿ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°ï¼šå…¨ãƒ•ã‚§ãƒ¼ã‚º
çµ±åˆæ–¹æ³•ï¼š
- Phase 2: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è©•ä¾¡ãƒãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆ
- Phase 6: Quality Assuranceçµæœã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆåŒ–
- Phase 7: è¨˜è¿°çµ±è¨ˆãƒ»ç›¸é–¢è¡Œåˆ—ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

ã‚³ãƒãƒ³ãƒ‰ä¾‹ï¼š
ã€ŒTable 1ã®è¨˜è¿°çµ±è¨ˆã‚’xlsx skillã§ä½œæˆã—ã¦ãã ã•ã„ã€
```

### 3. K-Dense-AI scientific-skills
```
ä½¿ç”¨å¯èƒ½ã‚¹ã‚­ãƒ«ï¼š
- scientific-databases: PubMed, ArXivè«–æ–‡æ¤œç´¢
- exploratory-data-analysis: è‡ªå‹•EDAå®Ÿè¡Œ
- statistical-power-analysis: æ¤œå‡ºåŠ›åˆ†æã®è©³ç´°åŒ–

çµ±åˆæ–¹æ³•ï¼š
Phase 1ã§ç†è«–æ–‡çŒ®ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã«scientific-databasesä½¿ç”¨
Phase 6ã§exploratory-data-analysis skillä½µç”¨

ã‚³ãƒãƒ³ãƒ‰ä¾‹ï¼š
ã€Œscientific-databases skillã§Dynamic Capabilitiesç†è«–ã®æœ€æ–°è«–æ–‡ã‚’æ¤œç´¢ã€
```

---

## Quick Start Guide

### ã€åˆå¿ƒè€…ã€‘ã¯ã˜ã‚ã¦ã®æˆ¦ç•¥ç ”ç©¶

```
Step 1: ç ”ç©¶ãƒ†ãƒ¼ãƒã‚’ä¼ãˆã‚‹
ã€Œæ—¥æœ¬ã®è£½é€ æ¥­ä¼æ¥­ã«ãŠã‘ã‚‹å‚ç›´çµ±åˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚ã‚’ç ”ç©¶ã—ãŸã„ã€

Step 2: ã‚¹ã‚­ãƒ«ãŒPhase 1ã‚’å®Ÿè¡Œ
â†’ ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ææ¡ˆ
â†’ ãƒ‡ãƒ¼ã‚¿è¦ä»¶ã®æ˜ç¢ºåŒ–
â†’ å¤‰æ•°ãƒªã‚¹ãƒˆæç¤º

Step 3: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
ã€Œç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ä½¿ç”¨ã€â†’ EDINETãƒ‡ãƒ¼ã‚¿åé›†

Step 4: è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯
â†’ Phase 6ã®QAè‡ªå‹•å®Ÿè¡Œ

Step 5: åˆ†æå®Ÿè¡Œ
â†’ Phase 7ã®ãƒ‘ãƒãƒ«å›å¸°

Step 6: è«–æ–‡åŸ·ç­†
â†’ academic-paper-creation skillã¨çµ±åˆ
```

### ã€ä¸­ç´šè€…ã€‘åŠ¹ç‡çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```
1. è¤‡æ•°ãƒ•ã‚§ãƒ¼ã‚ºã®ä¸¦è¡Œå®Ÿè¡Œ
ã€ŒPhase 2ã®ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿè¡Œã—ãªãŒã‚‰ã€Phase 3ã®ã‚µãƒ³ãƒ—ãƒ«è¨­è¨ˆã‚’é–‹å§‹ã€

2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
ã€ŒCompustatãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã€‚Phase 4ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã€

3. ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
ã€Œautomotive industryï¼ˆSIC 37XXï¼‰ã®ã¿ã«çµã£ã¦åˆ†æã€

4. é«˜åº¦ãªQA
ã€ŒBenford's Law testã«åŠ ãˆã¦ã€publication bias testã‚‚å®Ÿè¡Œã€
```

### ã€ä¸Šç´šè€…ã€‘è«–æ–‡æŠ•ç¨¿æº–å‚™

```
1. ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åŸºæº–ã§ã®QA
ã€ŒSMJæŠ•ç¨¿æº–å‚™ï¼šã™ã¹ã¦ã®robustness checksã‚’å®Ÿè¡Œã€

2. å®Œå…¨å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
ã€ŒAEAæº–æ‹ ã®replication packageã‚’ä½œæˆã€

3. ç†è«–çš„è²¢çŒ®ã®æ˜ç¢ºåŒ–
ã€Œæ—¢å­˜RBVç†è«–ã¸ã®ç†è«–çš„æŒ‘æˆ¦ã¨ã—ã¦ã€conceptual modelã‚’ç²¾ç·»åŒ–ã€

4. Multiple Submission Prep
ã€ŒSMJ, AMJ, OSã®3èªŒç”¨ã«cover letterã¨highlightsã‚’ä½œæˆã€
```

---

## Common Pitfalls & Solutions

### Pitfall 1: ã‚µãƒã‚¤ãƒãƒ«ãƒã‚¤ã‚¢ã‚¹ã®ç„¡è¦–
**å•é¡Œ**: ç¾å­˜ä¼æ¥­ã®ã¿åˆ†æ â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹éå¤§è©•ä¾¡
**è§£æ±º**: CRSPã®delisting dataã‚’çµ±åˆï¼ˆPhase 3.1ï¼‰

### Pitfall 2: Look-ahead Bias
**å•é¡Œ**: tæœŸã®DVã«tæœŸã®IVã‚’ä½¿ç”¨ â†’ å†…ç”Ÿæ€§
**è§£æ±º**: IVã‚’1-2æœŸãƒ©ã‚°ï¼ˆPhase 4.3ï¼‰

### Pitfall 3: æ¤œå‡ºåŠ›ä¸è¶³
**å•é¡Œ**: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ â†’ Type II error
**è§£æ±º**: äº‹å‰æ¤œå‡ºåŠ›åˆ†æï¼ˆPhase 3.2ï¼‰

### Pitfall 4: å¤šé‡å…±ç·šæ€§
**å•é¡Œ**: é«˜ç›¸é–¢å¤‰æ•°ã®åŒæ™‚æŠ•å…¥ â†’ ä¿‚æ•°ä¸å®‰å®š
**è§£æ±º**: VIFãƒã‚§ãƒƒã‚¯ã€PCAã€ã¾ãŸã¯å¤‰æ•°é¸æŠï¼ˆPhase 7.1ï¼‰

### Pitfall 5: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç„¡è¦–
**å•é¡Œ**: ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã§æ¨™æº–èª¤å·®éå°è©•ä¾¡
**è§£æ±º**: Clustered SEsï¼ˆfirm-levelï¼‰å¿…é ˆï¼ˆPhase 7.2ï¼‰

### Pitfall 6: ç†è«–çš„è²¢çŒ®ä¸æ˜ç¢º
**å•é¡Œ**: ã€Œèˆˆå‘³æ·±ã„ç™ºè¦‹ã€ã ã‘ã§ã¯ä¸ååˆ†
**è§£æ±º**: æ—¢å­˜ç†è«–ã¨ã®æ˜ç¢ºãªå¯¾è©±ï¼ˆPhase 7.5ï¼‰

---

## Frequently Asked Questions

### Q1: ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã ã‘ã§ top journal publishableãªç ”ç©¶ã¯å¯èƒ½ã‹ï¼Ÿ
**A**: Yes. ç‰¹ã«ã‚¢ã‚¸ã‚¢ç ”ç©¶ã§ã¯ååˆ†å¯èƒ½ã€‚
- æ—¥æœ¬ï¼šEDINETï¼ˆè²¡å‹™ï¼‰+ JPXï¼ˆæ ªä¾¡ï¼‰+ e-Statï¼ˆç”£æ¥­çµ±è¨ˆï¼‰
- éŸ“å›½ï¼šDART + KRX
- ä¸­å›½ï¼šCNINFO + AKShare
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼šWorld Bank + PatentsView

å®Ÿä¾‹ï¼šKim et al. (2021, SMJ) - éŸ“å›½DART dataã®ã¿ä½¿ç”¨

### Q2: çµ±è¨ˆã‚½ãƒ•ãƒˆã¯ä½•ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ
**A**: Pythonæ¨å¥¨ï¼ˆæœ¬ã‚¹ã‚­ãƒ«ã¯Pythonå‰æï¼‰
- ç†ç”±ï¼šãƒ‡ãƒ¼ã‚¿åé›†ã€œåˆ†æã¾ã§ä¸€è²«ã—ã¦å®Ÿè¡Œå¯èƒ½
- ä»£æ›¿ï¼šStataã‚‚å¯ï¼ˆãƒ‘ãƒãƒ«åˆ†æã«å¼·ã„ï¼‰
- Rï¼šå¯ï¼ˆfixest packageãŒå„ªç§€ï¼‰

### Q3: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯æœ€ä½ä½•ç¤¾å¿…è¦ã‹ï¼Ÿ
**A**: 
- Minimum: 100ç¤¾Ã—3å¹´ = 300 observations
- Recommended: 200ç¤¾Ã—5å¹´ = 1,000 observations
- Ideal: 500ç¤¾Ã—10å¹´ = 5,000 observations

ãŸã ã—ã€æ¤œå‡ºåŠ›åˆ†æï¼ˆPhase 3.2ï¼‰ã§å®¢è¦³çš„ã«æ±ºå®šã™ã¹ãã€‚

### Q4: Fixed Effects vs. Random Effects?
**A**: æˆ¦ç•¥ç ”ç©¶ã§ã¯**Fixed Effectsæ¨å¥¨**
- ç†ç”±ï¼šä¼æ¥­å›ºæœ‰ã®è¦³å¯Ÿä¸èƒ½ãªç•°è³ªæ€§ã‚’çµ±åˆ¶
- Hausman testã§çµ±è¨ˆçš„ã«æ¤œè¨¼
- å¤šãã®top journalsãŒFEã‚’æ¨™æº–ã¨ã—ã¦ã„ã‚‹

### Q5: å†…ç”Ÿæ€§ã¸ã®å¯¾å‡¦æ³•ã¯ï¼Ÿ
**A**: è¤‡æ•°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½µç”¨ï¼š
1. **Lagged IVs**: æœ€ã‚‚ç°¡ä¾¿ï¼ˆ1-2æœŸãƒ©ã‚°ï¼‰
2. **Fixed Effects**: æ™‚ä¸å¤‰ç•°è³ªæ€§ã‚’çµ±åˆ¶
3. **Instrumental Variables**: å¼·ã„IVãŒã‚ã‚Œã°æœ€å–„
4. **Difference-in-Differences**: è‡ªç„¶å®Ÿé¨“ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰
5. **Propensity Score Matching**: å‡¦ç½®åŠ¹æœæ¨å®š

ã©ã‚Œã‹1ã¤ã§ã¯ãªãã€è¤‡æ•°ã®æ–¹æ³•ã§robustnessã‚’ç¤ºã™ã€‚

### Q6: ã©ã®ãã‚‰ã„ã®æ™‚é–“ãŒã‹ã‹ã‚‹ã‹ï¼Ÿ
**A**: 
- Phase 1ï¼ˆæ§‹æƒ³ï¼‰: 1-2é€±é–“
- Phase 2-3ï¼ˆãƒ‡ãƒ¼ã‚¿æ¢ç´¢ãƒ»åé›†ï¼‰: 2-4é€±é–“
- Phase 4-5ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»çµ±åˆï¼‰: 2-3é€±é–“
- Phase 6ï¼ˆQAï¼‰: 1é€±é–“
- Phase 7ï¼ˆåˆ†æï¼‰: 1-2é€±é–“
- Phase 8ï¼ˆæ–‡æ›¸åŒ–ï¼‰: 1é€±é–“

**åˆè¨ˆ: 2-3ãƒ¶æœˆï¼ˆåˆå›ç ”ç©¶ï¼‰**

çµŒé¨“è€…ãªã‚‰1-1.5ãƒ¶æœˆã«çŸ­ç¸®å¯èƒ½ã€‚

### Q7: ãƒ‡ãƒ¼ã‚¿å–å¾—ã®æ³•çš„ãƒ»å€«ç†çš„å•é¡Œã¯ï¼Ÿ
**A**: 
**åˆæ³•**ï¼š
- å…¬é–‹ãƒ‡ãƒ¼ã‚¿ï¼ˆEDINET, SEC EDGARç­‰ï¼‰ã®APIåˆ©ç”¨
- å¥‘ç´„ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ©ç”¨ï¼ˆWRDSç­‰ï¼‰
- å­¦è¡“ç›®çš„ã®å¼•ç”¨

**æ³¨æ„**ï¼š
- Web scrapingã¯å„ã‚µã‚¤ãƒˆã®Terms of Serviceç¢ºèª
- Robots.txtã‚’éµå®ˆ
- Rate limitingã‚’å®Ÿè£…

**é•æ³•/éå€«ç†çš„**ï¼š
- èªè¨¼å›é¿
- éåº¦ãªã‚µãƒ¼ãƒãƒ¼è² è·
- äºŒæ¬¡é…å¸ƒï¼ˆå¥‘ç´„é•åï¼‰

### Q8: ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ï¼ˆSMJ, AMJï¼‰ã®åŸºæº–ã¯ï¼Ÿ
**A**: æœ¬ã‚¹ã‚­ãƒ«ã®Phase 6ã‚’å®Œå…¨å®Ÿè¡Œã™ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿å“è³ªåŸºæº–ã¯æº€ãŸã™ã€‚

è¿½åŠ è¦ä»¶ï¼š
- **ç†è«–çš„è²¢çŒ®**ï¼šæ—¢å­˜ç†è«–ã¸ã®æŒ‘æˆ¦ãƒ»æ‹¡å¼µãƒ»çµ±åˆ
- **Robustness**: 5ç¨®é¡ä»¥ä¸Šã®robustness checks
- **Replication**: å®Œå…¨ãªreplication package
- **Power Analysis**: äº‹å‰ç™»éŒ²æ¨å¥¨
- **Ethics**: IRB approvalï¼ˆå¿…è¦ãªå ´åˆï¼‰

---

## Version History

**v3.0 (2025-10-31)**
- ğŸ¯ æˆ¦ç•¥è«–ãƒ»çµ„ç¹”è«–ã«ç‰¹åŒ–ï¼ˆPhase 1æ‹¡å¼µï¼‰
- ğŸ“Š çµ±è¨ˆçš„æ¤œå‡ºåŠ›åˆ†æã®çµ±åˆï¼ˆPhase 3.2æ–°è¨­ï¼‰
- ğŸ”¬ Publication-grade QAï¼ˆPhase 6å¤§å¹…å¼·åŒ–ï¼‰
- ğŸŒ ã‚¢ã‚¸ã‚¢11ã‚«å›½ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¿½åŠ 
- ğŸ¤ ç†è«–æ§‹ç¯‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è¿½åŠ ï¼ˆPhase 7.5æ–°è¨­ï¼‰
- ğŸ“¦ å®Œå…¨å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆPhase 8æ‹¡å¼µï¼‰
- âœ… Pytest test suiteè¿½åŠ 
- ğŸ³ Dockerç’°å¢ƒå¯¾å¿œ

**v2.0 (2025-10-30)**  
ï¼ˆcorporate-research-data-hub skill v2.0ç›¸å½“ï¼‰
- Advanced QAæ©Ÿèƒ½
- Data lineage tracking
- Research checklist manager

**v1.0 (2025-10-29)**  
ï¼ˆresearch-data-collection skillç›¸å½“ï¼‰
- åŸºæœ¬6ãƒ•ã‚§ãƒ¼ã‚ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- æ¨™æº–çš„ãƒ‡ãƒ¼ã‚¿åé›†æ‰‹é †

---

## Citation & License

### ã“ã®ã‚¹ã‚­ãƒ«ã‚’ä½¿ç”¨ã—ãŸç ”ç©¶ã§ã®è¬è¾ä¾‹ï¼š

```
ãƒ‡ãƒ¼ã‚¿åé›†ã¨å“è³ªä¿è¨¼ã¯ã€strategic-management-research-hub skill v3.0ã«
åŸºã¥ãä½“ç³»çš„æ‰‹é †ã«å¾“ã£ã¦å®Ÿæ–½ã•ã‚ŒãŸã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€ç ”ç©¶ã®
å†ç¾æ€§ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¿¡é ¼æ€§ãŒç¢ºä¿ã•ã‚ŒãŸã€‚
```

### License
MIT License - å­¦è¡“ãƒ»å•†ç”¨åˆ©ç”¨å¯

### è²¬ä»»å…è²¬
æœ¬ã‚¹ã‚­ãƒ«ã¯ç ”ç©¶æ”¯æ´ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚Šã€ç ”ç©¶è€…è‡ªèº«ãŒä»¥ä¸‹ã®è²¬ä»»ã‚’è² ã„ã¾ã™ï¼š
1. ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ©ç”¨è¦ç´„éµå®ˆ
2. IRBæ‰¿èªå–å¾—ï¼ˆå¿…è¦ãªå ´åˆï¼‰
3. é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿å¼•ç”¨
4. å€«ç†çš„ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
5. ãƒ‡ãƒ¼ã‚¿ç²¾åº¦æ¤œè¨¼

---

## Support & Feedback

**åˆå›ä½¿ç”¨æ™‚ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**ï¼š
```
ã€Œstrategic-management-research-hub skillã®Quick Start Guideã«å¾“ã£ã¦ã€
 æ—¥æœ¬è£½é€ æ¥­ã®R&Dæˆ¦ç•¥ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç ”ç©¶ã‚’é–‹å§‹ã—ãŸã„ã€
```

**ã‚¹ã‚­ãƒ«ã®ä½¿ã„æ–¹ãŒåˆ†ã‹ã‚‰ãªã„å ´åˆ**ï¼š
```
ã€ŒPhase 2ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠã§è¿·ã£ã¦ã„ã‚‹ã€‚
 æ—¥æœ¬ä¼æ¥­ã®çµ„ç¹”æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã¯ã©ã“ã§å…¥æ‰‹ã§ãã‚‹ã‹ï¼Ÿã€
```

**ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ**ï¼š
```
ã€ŒPhase 6ã®Benford's Law testãŒå¤±æ•—ã—ãŸã€‚
 p-value=0.03ã§è­¦å‘ŠãŒå‡ºã¦ã„ã‚‹ã€‚ã©ã†å¯¾å‡¦ã™ã¹ãã‹ï¼Ÿã€
```

---

**Ready to start your research journey?**

Simply say:
```
ã€Œstrategic-management-research-hub skillã‚’ä½¿ç”¨ã—ã¦ã€
 [ã‚ãªãŸã®ç ”ç©¶ãƒ†ãƒ¼ãƒ]ã®å®Ÿè¨¼ç ”ç©¶ã‚’é–‹å§‹ã—ãŸã„ã€
```

ä¾‹ï¼š
- ã€Œæ—¥æœ¬ä¼æ¥­ã®ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ»ã‚±ã‚¤ãƒ‘ãƒ“ãƒªãƒ†ã‚£ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚ã‚’ç ”ç©¶ã—ãŸã„ã€
- ã€Œã‚¢ã‚¸ã‚¢æ–°èˆˆå¸‚å ´ã«ãŠã‘ã‚‹åˆ¶åº¦ç’°å¢ƒã¨å‚å…¥æˆ¦ç•¥ã®ç ”ç©¶ã‚’è¡Œã„ãŸã„ã€
- ã€Œå‚ç›´çµ±åˆæˆ¦ç•¥ã¨ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ã®é–¢ä¿‚ã‚’åˆ†æã—ãŸã„ã€

---

## Appendix: Data Source URLs

### North America
- WRDS: https://wrds-www.wharton.upenn.edu/
- SEC EDGAR: https://www.sec.gov/edgar.shtml
- USPTO PatentsView: https://patentsview.org/

### Europe
- Orbis: https://www.bvdinfo.com/en-gb/our-products/data/international/orbis
- Eurostat: https://ec.europa.eu/eurostat

### Asia
- **Japan**:
  - EDINET: https://disclosure2.edinet-fsa.go.jp/
  - JPX: https://www.jpx.co.jp/markets/statistics-equities/
  - e-Stat: https://www.e-stat.go.jp/
- **South Korea**:
  - DART: https://dart.fss.or.kr/
  - KRX: http://www.krx.co.kr/
- **China**:
  - CNINFO: http://www.cninfo.com.cn/
  - Tushare: https://tushare.pro/
  - AKShare: https://github.com/akfamily/akshare

### Global
- World Bank: https://data.worldbank.org/
- IMF: https://data.imf.org/
- OECD: https://data.oecd.org/

---

**This skill represents the state-of-the-art in strategic management empirical research.**  
**Follow its guidance, and your research will meet top-tier journal standards.**  
**Good luck with your research! ğŸ“ğŸ“ŠğŸš€**

---

## APPENDIX A: Comprehensive Data Source Guide for Strategy Research

### A.1 Core Strategic Variables & Data Sources Matrix

| æˆ¦ç•¥å¤‰æ•° | ç†è«–çš„åŸºç›¤ | æ¸¬å®šæ–¹æ³• | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ | ã‚¢ã‚¯ã‚»ã‚¹ | ã‚³ã‚¹ãƒˆ |
|---------|-----------|---------|-------------|---------|--------|
| **Competitive Strategy** |
| Cost Leadership | Porter (1980) | Asset intensity, Labor productivity | Compustat, Orbis | WRDS, å¤§å­¦å¥‘ç´„ | $$$ |
| Differentiation | Porter (1980) | R&D intensity, Advertising intensity | Compustat, EDINET | WRDS, ç„¡æ–™ | $/ç„¡æ–™ |
| Product Innovation | Schumpeter (1942) | Patent count, New product launches | PatentsView, EDINET | ç„¡æ–™ | ç„¡æ–™ |
| Process Innovation | Cohen & Levinthal (1990) | Process patents, Productivity growth | PatentsView, BEA | ç„¡æ–™ | ç„¡æ–™ |
| **Dynamic Capabilities** |
| Absorptive Capacity | Cohen & Levinthal (1990) | R&D Ã— External links | Compustat + Orbis | WRDS + å¥‘ç´„ | $$$ |
| Sensing | Teece (2007) | Market research spend, Patent citations | 10-K MD&A, PatentsView | ç„¡æ–™ | ç„¡æ–™ |
| Seizing | Teece (2007) | Product launch frequency | Compustat Segments | WRDS | $$$ |
| Transforming | Teece (2007) | Organizational restructuring | SDC, Orbis M&A | WRDS, å¥‘ç´„ | $$$ |
| **Organizational Resources** |
| Human Capital | Barney (1991) | Employee skills, Training invest | Compustat, Orbis | WRDS, å¥‘ç´„ | $$$ |
| Social Capital | Nahapiet & Ghoshal (1998) | Board interlocks, Alliance networks | ISS, SDC | å¥‘ç´„ | $$$ |
| Technological Capital | Dierickx & Cool (1989) | Patent stock, R&D stock | PatentsView | ç„¡æ–™ | ç„¡æ–™ |
| Reputation | Fombrun & Shanley (1990) | Media mentions, ESG ratings | Factiva, MSCI | å¥‘ç´„ | $$$ |
| **Organizational Structure** |
| Centralization | Chandler (1962) | Span of control | Orbis ownership | å¥‘ç´„ | $$$ |
| Formalization | March & Simon (1958) | ISO certifications | ISO Survey | ç„¡æ–™ | ç„¡æ–™ |
| Specialization | Thompson (1967) | Business segments | Compustat Segments | WRDS | $$$ |
| Integration | Lawrence & Lorsch (1967) | Vertical integration ratio | Compustat | WRDS | $$$ |
| **Strategic Alliances** |
| Alliance Portfolio | Lavie (2007) | Alliance count, Diversity | SDC Alliances | WRDS | $$$ |
| Network Position | Gulati (1999) | Centrality, Betweenness | SDC (ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ) | WRDS | $$$ |
| Partner Quality | Stuart (2000) | Partner patent stock | PatentsView + ãƒãƒƒãƒãƒ³ã‚° | ç„¡æ–™ | ç„¡æ–™ |
| **Institutional Environment** |
| Regulative | Scott (1995) | Rule of law, Regulatory quality | World Bank WGI | ç„¡æ–™ | ç„¡æ–™ |
| Normative | Scott (1995) | Cultural dimensions | Hofstede, GLOBE | ä¸€éƒ¨æœ‰æ–™ | $ |
| Cognitive | Scott (1995) | Education index, R&D policy | World Bank, OECD | ç„¡æ–™ | ç„¡æ–™ |
| **M&A & Restructuring** |
| Acquisition Experience | Haleblian & Finkelstein (1999) | Prior M&A count | SDC Platinum | WRDS | $$$ |
| Cultural Fit | Chatterjee et al. (1992) | Cultural distance | Hofstede + è¨ˆç®— | ä¸€éƒ¨æœ‰æ–™ | $ |
| Integration Speed | Zollo & Singh (2004) | Time to full integration | 10-K filings | ç„¡æ–™ | ç„¡æ–™ |

### A.2 Asia-Pacific Data Sources: Complete Guide

#### **Japan ğŸ‡¯ğŸ‡µ**

**EDINETï¼ˆé‡‘èåºãƒ»æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã‚·ã‚¹ãƒ†ãƒ ï¼‰**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šä¸Šå ´ä¼æ¥­ç´„3,800ç¤¾
URL: https://disclosure2.edinet-fsa.go.jp/
API: https://disclosure2.edinet-fsa.go.jp/api/v2/documents
ã‚³ã‚¹ãƒˆï¼šå®Œå…¨ç„¡æ–™

å–å¾—å¯èƒ½ãƒ‡ãƒ¼ã‚¿ï¼š
- è²¡å‹™è«¸è¡¨ï¼ˆBS, PL, CFï¼‰
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ï¼ˆäº‹æ¥­ãƒ»åœ°åŸŸåˆ¥ï¼‰
- å½¹å“¡å ±é…¬ãƒ»æ§‹æˆ
- æ ªä¸»æ§‹æˆ
- ãƒªã‚¹ã‚¯æƒ…å ±ï¼ˆMD&Aï¼‰
- é–¢é€£å½“äº‹è€…å–å¼•

Pythonå®Ÿè£…ä¾‹ï¼š
```python
import requests
import pandas as pd

api_url = "https://disclosure2.edinet-fsa.go.jp/api/v2"
doc_list = requests.get(f"{api_url}/documents.json", 
                        params={'date': '2024-03-31', 'type': 2})
# type 2 = æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸

for doc in doc_list.json()['results']:
    doc_id = doc['docID']
    xbrl_data = requests.get(f"{api_url}/documents/{doc_id}", 
                             params={'type': 5})  # XBRL
    # Parse XBRL and extract financial data
```

æˆ¦ç•¥ç ”ç©¶ã§ã®æ´»ç”¨ï¼š
- å‚ç›´çµ±åˆåº¦ï¼šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‹ã‚‰ç®—å‡º
- å¤šè§’åŒ–æˆ¦ç•¥ï¼šäº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã€Entropy index
- å›½éš›åŒ–æˆ¦ç•¥ï¼šåœ°åŸŸåˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå£²ä¸Š
- ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆã‚¬ãƒãƒŠãƒ³ã‚¹ï¼šå½¹å“¡æ§‹æˆã€å ±é…¬ä½“ç³»
```

**JPXï¼ˆæ—¥æœ¬å–å¼•æ‰€ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰**
```
URL: https://www.jpx.co.jp/markets/statistics-equities/
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€å‡ºæ¥é«˜ã€å¸‚å ´ãƒ‡ãƒ¼ã‚¿
å½¢å¼ï¼šCSVç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
æ›´æ–°ï¼šæ—¥æ¬¡

æ´»ç”¨ï¼š
- å¸‚å ´ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆTobin's Qï¼‰
- ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ã‚¿ãƒ‡ã‚£ï¼ˆæˆ¦ç•¥ç™ºè¡¨ã®æ ªä¾¡åå¿œï¼‰
- ãƒªã‚¹ã‚¯æŒ‡æ¨™ï¼ˆBeta, Volatilityï¼‰
```

**e-Statï¼ˆæ”¿åºœçµ±è¨ˆãƒãƒ¼ã‚¿ãƒ«ï¼‰**
```
URL: https://www.e-stat.go.jp/
API: https://www.e-stat.go.jp/api/
ã‚³ã‚¹ãƒˆï¼šç„¡æ–™ï¼ˆAPI keyç™»éŒ²å¿…è¦ï¼‰

ç”£æ¥­çµ±è¨ˆï¼š
- å·¥æ¥­çµ±è¨ˆèª¿æŸ»
- çµŒæ¸ˆã‚»ãƒ³ã‚µã‚¹
- ã‚µãƒ¼ãƒ“ã‚¹ç”£æ¥­å‹•å‘èª¿æŸ»

æ´»ç”¨ï¼š
- ç”£æ¥­ãƒ¬ãƒ™ãƒ«å¤‰æ•°ï¼ˆé›†ä¸­åº¦ã€æˆé•·ç‡ï¼‰
- åœ°åŸŸçµŒæ¸ˆæŒ‡æ¨™
- ãƒã‚¯ãƒ­çµ±åˆ¶å¤‰æ•°
```

**æ—¥æœ¬ã®ç³»åˆ—ãƒ»ä¼æ¥­ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿**
```
ç³»åˆ—æƒ…å ±ï¼š
- ã€ä¼šç¤¾å››å­£å ±ã€ï¼ˆæ±æ´‹çµŒæ¸ˆï¼‰- æœ‰æ–™
- ä¼æ¥­é–‹ç¤ºè³‡æ–™ã®ã€Œä¸»è¦æ ªä¸»ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç„¡æ–™ï¼‰
- æ—¥çµŒNEEDSã®ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼ˆæœ‰æ–™ï¼‰

ç ”ç©¶ä¾‹ï¼š
- ç³»åˆ—æ‰€å±ã¨R&DæŠ•è³‡ï¼ˆrisk-sharingåŠ¹æœï¼‰
- Main banké–¢ä¿‚ã¨æŠ•è³‡è¡Œå‹•
- æ ªå¼æŒã¡åˆã„ã¨çµŒå–¶è‡ªå¾‹æ€§
```

#### **South Korea ğŸ‡°ğŸ‡·**

**DARTï¼ˆData Analysis, Retrieval and Transfer Systemï¼‰**
```
URL: https://dart.fss.or.kr/
API: https://opendart.fss.or.kr/
ã‚³ã‚¹ãƒˆï¼šå®Œå…¨ç„¡æ–™ï¼ˆAPIã‚­ãƒ¼ç™»éŒ²ã®ã¿ï¼‰

ãƒ‡ãƒ¼ã‚¿ç¯„å›²ï¼š
- è²¡å‹™è«¸è¡¨ï¼ˆ1999å¹´ã€œï¼‰
- äº‹æ¥­å ±å‘Šæ›¸
- ê°ì‚¬ë³´ê³ ì„œï¼ˆç›£æŸ»å ±å‘Šæ›¸ï¼‰
- ì§€ë¶„ê³µì‹œï¼ˆæŒåˆ†é–‹ç¤ºï¼‰

éŸ“å›½ç‰¹æœ‰ãƒ‡ãƒ¼ã‚¿ï¼š
- è²¡é–¥ï¼ˆChaebolï¼‰æ‰€å±æƒ…å ±
- æ”¿åºœé–¢ä¿‚ï¼ˆå…¬ä¼æ¥­æŒ‡å®šï¼‰
- è¼¸å‡ºå®Ÿç¸¾
- æµ·å¤–å­ä¼šç¤¾æƒ…å ±

Pythonå®Ÿè£…ï¼š
```python
import requests

api_key = "YOUR_API_KEY"
base_url = "https://opendart.fss.or.kr/api"

# ì¬ë¬´ì œí‘œ (Financial Statements)
response = requests.get(f"{base_url}/fnlttSinglAcntAll.json", 
                        params={
                            'crtfc_key': api_key,
                            'corp_code': '00126380',  # Samsung
                            'bsns_year': '2023',
                            'reprt_code': '11011'  # ì‚¬ì—…ë³´ê³ ì„œ
                        })
financial_data = response.json()
```

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- è²¡é–¥æ‰€å±åŠ¹æœï¼ˆChaebol affiliation premiumï¼‰
- æ”¿åºœã¨ã®é–¢ä¿‚ã¨å‚å…¥è¦åˆ¶
- è¼¸å‡ºå¿—å‘æˆ¦ç•¥ã¨å›½éš›åŒ–
```

**KRXï¼ˆéŸ“å›½å–å¼•æ‰€ï¼‰**
```
URL: http://www.krx.co.kr/
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€è²¡å‹™æ¯”ç‡ã€ä¼æ¥­æƒ…å ±
å½¢å¼ï¼šExcel/CSVç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

æ´»ç”¨ï¼š
- å¸‚å ´ãƒ‡ãƒ¼ã‚¿
- PER, PBRç­‰ã®æŠ•è³‡æŒ‡æ¨™
- ç”£æ¥­åˆ¥çµ±è¨ˆ
```

#### **China ğŸ‡¨ğŸ‡³**

**CNINFOï¼ˆå·¨æ½®è³‡è¨Šç½‘ / China Securities Information Networkï¼‰**
```
URL: http://www.cninfo.com.cn/
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šAæ ªãƒ»Bæ ªä¸Šå ´ä¼æ¥­
ãƒ‡ãƒ¼ã‚¿ï¼šå®šæœŸå ±å‘Šã€è²¡å‹™è«¸è¡¨
å½¢å¼ï¼šHTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¿…è¦

æ³¨æ„ç‚¹ï¼š
- ä¸­å›½èªã®ã¿
- Web scrapingå¿…è¦ï¼ˆAPIãªã—ï¼‰
- Terms of Serviceç¢ºèªå¿…é ˆ

æˆ¦ç•¥ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ï¼š
- å›½æœ‰ä¼æ¥­vs.æ°‘é–“ä¼æ¥­ï¼ˆæ‰€æœ‰å½¢æ…‹ï¼‰
- æ”¿åºœè£œåŠ©é‡‘é¡
- å…šçµ„ç¹”ã®æœ‰ç„¡
- æµ·å¤–æŠ•è³‡æƒ…å ±
```

**Tushareï¼ˆé‡‘èãƒ‡ãƒ¼ã‚¿APIï¼‰**
```
URL: https://tushare.pro/
ã‚³ã‚¹ãƒˆï¼šåŸºæœ¬ç„¡æ–™ã€ãƒ—ãƒ¬ãƒŸã‚¢ãƒ æœ‰æ–™
Python: `pip install tushare`

ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ï¼š
- æ ªä¾¡ï¼ˆæ—¥æ¬¡ãƒ»åˆ†è¶³ï¼‰
- åŸºæœ¬è²¡å‹™ãƒ‡ãƒ¼ã‚¿
- ç”£æ¥­åˆ†é¡

ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ï¼ˆæœ‰æ–™ï¼‰ï¼š
- è©³ç´°è²¡å‹™ãƒ‡ãƒ¼ã‚¿
- æ‰€æœ‰æ§‹é€ 
- ã‚¢ãƒŠãƒªã‚¹ãƒˆäºˆæƒ³

å®Ÿè£…ä¾‹ï¼š
```python
import tushare as ts

ts.set_token('YOUR_TOKEN')
pro = ts.pro_api()

# è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—
df = pro.income(ts_code='600000.SH', 
                start_date='20200101', 
                end_date='20231231')
```
```

**AKShareï¼ˆå®Œå…¨ç„¡æ–™Python APIï¼‰**
```
GitHub: https://github.com/akfamily/akshare
ã‚³ã‚¹ãƒˆï¼šå®Œå…¨ç„¡æ–™ã€ç™»éŒ²ä¸è¦
ãƒ‡ãƒ¼ã‚¿ç¯„å›²ï¼šæ ªä¾¡ã€è²¡å‹™ã€ãƒã‚¯ãƒ­

ç‰¹å¾´ï¼š
- Tushareã‚ˆã‚Šåˆ¶é™å°‘ãªã„
- ä¸­å›½ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚‚ä¸€éƒ¨ã‚«ãƒãƒ¼
- æ´»ç™ºãªé–‹ç™ºã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

å®Ÿè£…ï¼š
```python
import akshare as ak

# Aæ ªä¸Šå ´ä¼æ¥­ãƒªã‚¹ãƒˆ
stock_list = ak.stock_info_a_code_name()

# è²¡å‹™ãƒ‡ãƒ¼ã‚¿
financial_data = ak.stock_financial_abstract(symbol="600000")
```

åˆ¶åº¦ç ”ç©¶ã§ã®æ´»ç”¨ï¼š
- å›½æœ‰ä¼æ¥­æ”¹é©ã®åŠ¹æœ
- æ”¿æ²»çš„ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå…šå“¡CEOã®å½±éŸ¿ï¼‰
- åœ°åŸŸé–“åˆ¶åº¦å·®ï¼ˆæ²¿æµ·vs.å†…é™¸ï¼‰
```

#### **Taiwan ğŸ‡¹ğŸ‡¼**

**TWSEï¼ˆå°æ¹¾è¨¼åˆ¸å–å¼•æ‰€ï¼‰**
```
URL: https://www.twse.com.tw/
API: ä¸€éƒ¨ã‚ã‚Š
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€è²¡å‹™ã‚µãƒãƒªãƒ¼
å½¢å¼ï¼šCSV

æ´»ç”¨ï¼š
- å°æ¹¾åŠå°ä½“ç”£æ¥­ç ”ç©¶
- IT manufacturingæˆ¦ç•¥
```

**å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ï¼ˆMOPSï¼‰**
```
URL: https://mops.twse.com.tw/
ãƒ‡ãƒ¼ã‚¿ï¼šè²¡å‹™è«¸è¡¨ã€ä¼æ¥­é–‹ç¤º
å½¢å¼ï¼šHTMLï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¿…è¦ï¼‰

å°æ¹¾ç‰¹æœ‰ç ”ç©¶ï¼š
- ãƒ•ã‚¡ã‚¦ãƒ³ãƒ‰ãƒªãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«
- ODM/OEMæˆ¦ç•¥
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³çµ±åˆ
```

#### **ASEAN Countries ğŸŒ**

**Singapore**
```
SGXï¼ˆSingapore Exchangeï¼‰
URL: https://www.sgx.com/
ãƒ‡ãƒ¼ã‚¿ï¼šä¸Šå ´ä¼æ¥­æƒ…å ±ã€æ ªä¾¡
ã‚¢ã‚¯ã‚»ã‚¹ï¼šä¸€éƒ¨ç„¡æ–™ã€è©³ç´°ã¯å¥‘ç´„

ACRAï¼ˆAccounting and Corporate Regulatory Authorityï¼‰
URL: https://www.acra.gov.sg/
ãƒ‡ãƒ¼ã‚¿ï¼šä¼æ¥­ç™»è¨˜æƒ…å ±
ã‚³ã‚¹ãƒˆï¼šæœ‰æ–™

æˆ¦ç•¥ç ”ç©¶ï¼š
- åœ°åŸŸçµ±æ‹¬æ‹ ç‚¹æˆ¦ç•¥
- å¤šå›½ç±ä¼æ¥­ã®ã‚¢ã‚¸ã‚¢å±•é–‹
```

**Thailand**
```
SETï¼ˆStock Exchange of Thailandï¼‰
URL: https://www.set.or.th/
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€è²¡å‹™æƒ…å ±
å½¢å¼ï¼šCSV/Excel

æ´»ç”¨ï¼š
- ASEAN manufacturingæˆ¦ç•¥
- è‡ªå‹•è»Šç”£æ¥­ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
```

**Vietnam**
```
HOSEï¼ˆHo Chi Minh Stock Exchangeï¼‰
HNXï¼ˆHanoi Stock Exchangeï¼‰
ãƒ‡ãƒ¼ã‚¿ï¼šåŸºæœ¬è²¡å‹™ã€æ ªä¾¡
ã‚¢ã‚¯ã‚»ã‚¹ï¼šWebçµŒç”±ã€ä¸€éƒ¨API

æ–°èˆˆå¸‚å ´ç ”ç©¶ï¼š
- FDIæˆ¦ç•¥
- å‚å…¥ãƒ¢ãƒ¼ãƒ‰é¸æŠ
```

**Indonesia**
```
IDXï¼ˆIndonesia Stock Exchangeï¼‰
URL: https://www.idx.co.id/
ãƒ‡ãƒ¼ã‚¿ï¼šä¸Šå ´ä¼æ¥­æƒ…å ±
å½¢å¼ï¼šExcelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

å¤šå³¶å¶¼å›½å®¶ã®ç‰¹æ€§ï¼š
- åœ°ç†çš„åˆ†æ•£ã¨çµ„ç¹”æ§‹é€ 
- ã‚¤ãƒ³ãƒ•ãƒ©åˆ¶ç´„ä¸‹ã®æˆ¦ç•¥
```

**Malaysia**
```
Bursa Malaysia
URL: https://www.bursamalaysia.com/
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€ä¼æ¥­æƒ…å ±

ç ”ç©¶ãƒ†ãƒ¼ãƒï¼š
- ã‚¤ã‚¹ãƒ©ãƒ é‡‘èã¨ä¼æ¥­æˆ¦ç•¥
- å¤šæ°‘æ—ç¤¾ä¼šã®çµ„ç¹”ç®¡ç†
```

**Philippines**
```
PSEï¼ˆPhilippine Stock Exchangeï¼‰
URL: https://www.pse.com.ph/
ãƒ‡ãƒ¼ã‚¿ï¼šåŸºæœ¬ä¼æ¥­æƒ…å ±

ç ”ç©¶æ©Ÿä¼šï¼š
- BPOç”£æ¥­ã®æˆ¦ç•¥
- è²¡é–¥æ§‹é€ ï¼ˆConglomeratesï¼‰
```

### A.3 ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã§ã§ãã‚‹æˆ¦ç•¥ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾‹

#### **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ1ï¼šæ—¥æœ¬è£½é€ æ¥­ã®ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ï¼ˆÂ¥0äºˆç®—ï¼‰**

```yaml
ç ”ç©¶ãƒ†ãƒ¼ãƒï¼šR&DæŠ•è³‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼šç’°å¢ƒå‹•æ…‹æ€§ã®èª¿æ•´åŠ¹æœ

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼š
  - EDINETï¼ˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿ï¼‰: ç„¡æ–™
  - JPXï¼ˆæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ï¼‰: ç„¡æ–™
  - PatentsViewï¼ˆç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ï¼‰: ç„¡æ–™
  - e-Statï¼ˆç”£æ¥­çµ±è¨ˆï¼‰: ç„¡æ–™

å¤‰æ•°æ§‹ç¯‰ï¼š
  DV: ROA, Tobin's Q
  IV: R&D intensity, Patent stock
  Moderator: Environmental dynamismï¼ˆç”£æ¥­å£²ä¸Šå¤‰å‹•ä¿‚æ•°ï¼‰
  Controls: Firm size, Age, Leverage

ã‚µãƒ³ãƒ—ãƒ«ï¼š
  è£½é€ æ¥­ä¸Šå ´ä¼æ¥­300ç¤¾
  æœŸé–“ï¼š2010-2023å¹´
  N â‰ˆ 3,600 firm-years

æœŸé–“ï¼š8é€±é–“
æˆæœï¼šSMJæŠ•ç¨¿å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
```

#### **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2ï¼šéŸ“å›½è²¡é–¥ã®å¤šè§’åŒ–æˆ¦ç•¥ï¼ˆÂ¥0äºˆç®—ï¼‰**

```yaml
ç ”ç©¶ãƒ†ãƒ¼ãƒï¼šè²¡é–¥æ‰€å±ã¨å¤šè§’åŒ–ï¼šåˆ¶åº¦çš„è¦–ç‚¹

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼š
  - DARTï¼ˆè²¡å‹™ãƒ»æ‰€æœ‰æ§‹é€ ï¼‰: ç„¡æ–™
  - KRXï¼ˆå¸‚å ´ãƒ‡ãƒ¼ã‚¿ï¼‰: ç„¡æ–™
  - World Bankï¼ˆåˆ¶åº¦å¤‰æ•°ï¼‰: ç„¡æ–™

å¤‰æ•°ï¼š
  DV: Diversification (Entropy index)
  IV: Chaebol affiliation (dummy)
  Mediator: Internal capital market efficiency
  Controls: Size, Age, Industry

ç†è«–çš„è²¢çŒ®ï¼š
  - åˆ¶åº¦ç†è«–ã®æ–°èˆˆå¸‚å ´ã¸ã®æ‹¡å¼µ
  - è²¡é–¥ã®æˆ¦ç•¥çš„æŸ”è»Ÿæ€§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

æœŸé–“ï¼š10é€±é–“
```

#### **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ3ï¼šä¸­å›½å›½æœ‰ä¼æ¥­æ”¹é©ã®åŠ¹æœï¼ˆÂ¥0äºˆç®—ï¼‰**

```yaml
ç ”ç©¶ãƒ†ãƒ¼ãƒï¼šæ‰€æœ‰å½¢æ…‹ã¨ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼šåˆ¶åº¦å¤‰åŒ–ã®å½±éŸ¿

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼š
  - CNINFOï¼ˆè²¡å‹™ãƒ»æ‰€æœ‰ï¼‰: ç„¡æ–™ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
  - AKShareï¼ˆæ ªä¾¡ï¼‰: ç„¡æ–™
  - PatentsViewï¼ˆä¸­å›½ä¼æ¥­ç‰¹è¨±ï¼‰: ç„¡æ–™

å¤‰æ•°ï¼š
  DV: Innovation outputï¼ˆç‰¹è¨±æ•°ï¼‰
  IV: State ownership %
  Moderator: Reform intensityï¼ˆçœãƒ¬ãƒ™ãƒ«ï¼‰
  Controls: Industry, Firm characteristics

åˆ†æï¼š
  - Difference-in-Differences
  - æ”¹é©å‰å¾Œã®æ¯”è¼ƒ

ç†è«–ï¼š
  - Principal-agentç†è«–
  - Institutional change

æœŸé–“ï¼š12é€±é–“ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å«ã‚€ï¼‰
```

#### **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ4ï¼šã‚¢ã‚¸ã‚¢æ¨ªæ–­æ¯”è¼ƒï¼ˆÂ¥0äºˆç®—ï¼‰**

```yaml
ç ”ç©¶ãƒ†ãƒ¼ãƒï¼šåˆ¶åº¦çš„è·é›¢ã¨å‚å…¥ãƒ¢ãƒ¼ãƒ‰é¸æŠ

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼š
  - æ—¥æœ¬ï¼šEDINET
  - éŸ“å›½ï¼šDART
  - å°æ¹¾ï¼šMOPS
  - ä¸­å›½ï¼šCNINFO
  - åˆ¶åº¦å¤‰æ•°ï¼šWorld Bank WGI

å¤‰æ•°ï¼š
  DV: Entry modeï¼ˆJV vs. WOSï¼‰
  IV: Institutional distance
  Controls: Firm experience, Industry

ã‚µãƒ³ãƒ—ãƒ«ï¼š
  ã‚¢ã‚¸ã‚¢4ã‚«å›½ä¼æ¥­ã®æµ·å¤–å‚å…¥
  N â‰ˆ 500 entries

ç†è«–ï¼š
  - Institutional theory
  - Transaction cost economics

æœŸé–“ï¼š16é€±é–“
æˆæœï¼šAMJ/JIBSãƒ¬ãƒ™ãƒ«
```

---

## APPENDIX B: Advanced Statistical Techniques

### B.1 Endogeneity Solutions Toolkit

**Problem**: ç‹¬ç«‹å¤‰æ•°ã¨èª¤å·®é …ã®ç›¸é–¢ â†’ ãƒã‚¤ã‚¢ã‚¹æ¨å®š

**Solution 1: Instrumental Variables (IV)**

```python
from linearmodels.iv import IV2SLS

# Example: R&D endogeneity
# Instrument: Industry average R&Dï¼ˆä¼æ¥­å›ºæœ‰è¦å› ã«å½±éŸ¿ã•ã‚Œãªã„ï¼‰

# First stage: R&D ~ Industry_avg_RD + controls
# Second stage: Performance ~ R&D_hat + controls

model = IV2SLS.from_formula(
    'roa ~ [rd_intensity ~ industry_avg_rd] + size + leverage + age',
    data=df_panel
).fit(cov_type='clustered', clusters=df_panel.index.get_level_values(0))

print(model.summary)

# First-stage diagnostics
print(f"F-statistic: {model.first_stage.diagnostics['f.stat']:.2f}")
# F > 10 â†’ Strong instrument
# F < 10 â†’ Weak instrumentï¼ˆçµæœä¿¡é ¼ã§ããªã„ï¼‰
```

**Solution 2: Heckman Selection Model**

```python
from statsmodels.regression.linear_model import OLS
from statsmodels.discrete.discrete_model import Probit

# Stage 1: Probit model for sample selection
# ä¾‹ï¼šM&Aå®Ÿæ–½ vs. éå®Ÿæ–½

selection_formula = 'ma_dummy ~ firm_size + cash + debt_ratio + industry_ma_rate'
probit_model = Probit.from_formula(selection_formula, data=df).fit()

# Inverse Mills Ratio
from scipy.stats import norm
df['lambda'] = (norm.pdf(probit_model.fittedvalues) / 
                norm.cdf(probit_model.fittedvalues))

# Stage 2: OLS with IMR
outcome_formula = 'post_ma_performance ~ ma_characteristics + lambda + controls'
outcome_model = OLS.from_formula(outcome_formula, data=df[df['ma_dummy']==1]).fit()

print(outcome_model.summary())
# Î»ã®ä¿‚æ•°ãŒæœ‰æ„ â†’ Selection biasã‚ã‚Š
```

**Solution 3: Propensity Score Matching**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import NearestNeighbors

# Treatment: Strategic alliance formation

# 1. Estimate propensity scores
X_covariates = df[['firm_size', 'rd_intensity', 'prior_alliances', 'industry']]
y_treatment = df['alliance_formed']

logit = LogisticRegression().fit(X_covariates, y_treatment)
df['propensity_score'] = logit.predict_proba(X_covariates)[:, 1]

# 2. Nearest neighbor matching
treated = df[df['alliance_formed'] == 1]
control = df[df['alliance_formed'] == 0]

nn = NearestNeighbors(n_neighbors=1, metric='euclidean')
nn.fit(control[['propensity_score']])

distances, indices = nn.kneighbors(treated[['propensity_score']])

# 3. Create matched sample
matched_control = control.iloc[indices.flatten()]
matched_sample = pd.concat([treated, matched_control])

# 4. Estimate treatment effect
treatment_effect = (matched_sample[matched_sample['alliance_formed']==1]['performance'].mean() -
                    matched_sample[matched_sample['alliance_formed']==0]['performance'].mean())

print(f"Average Treatment Effect: {treatment_effect:.4f}")
```

**Solution 4: Difference-in-Differences (DiD)**

```python
# Natural experiment: Regulatory change affecting some firms

# Treatment group: Affected by regulation
# Control group: Not affected
# Pre-period: Before regulation
# Post-period: After regulation

did_formula = """
performance ~ treatment_group * post_period + 
              firm_controls + 
              C(firm_id) + C(year)
"""

did_model = PanelOLS.from_formula(did_formula, data=df_panel).fit(
    cov_type='clustered', cluster_entity=True
)

# DiD estimator = coefficient on (treatment_group Ã— post_period)
print(f"DiD Estimate: {did_model.params['treatment_group:post_period']:.4f}")
print(f"p-value: {did_model.pvalues['treatment_group:post_period']:.4f}")

# Parallel trends test (pre-treatment)
# treatmentã¨controlã®trendãŒå¹³è¡Œã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
```

### B.2 Mediation & Moderation Analysis

**Mediation (Baron & Kenny, 1986)**

```python
import statsmodels.formula.api as smf

# X â†’ M â†’ Y
# ä¾‹ï¼šR&DæŠ•è³‡ â†’ çµ„ç¹”å­¦ç¿’ â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

# Step 1: X â†’ Y (Total effect)
model_c = smf.ols('performance ~ rd_investment + controls', data=df).fit()
total_effect = model_c.params['rd_investment']

# Step 2: X â†’ M
model_a = smf.ols('organizational_learning ~ rd_investment + controls', data=df).fit()
path_a = model_a.params['rd_investment']

# Step 3: X + M â†’ Y
model_b = smf.ols('performance ~ rd_investment + organizational_learning + controls', 
                  data=df).fit()
path_b = model_b.params['organizational_learning']
direct_effect = model_b.params['rd_investment']

# Mediation effect
indirect_effect = path_a * path_b
mediation_ratio = indirect_effect / total_effect

print(f"Total effect: {total_effect:.4f}")
print(f"Direct effect: {direct_effect:.4f}")
print(f"Indirect effect: {indirect_effect:.4f}")
print(f"Mediation ratio: {mediation_ratio:.2%}")

# Sobel test for significance
from scipy.stats import norm
se_indirect = np.sqrt(path_b**2 * model_a.bse['rd_investment']**2 +
                      path_a**2 * model_b.bse['organizational_learning']**2)
z_stat = indirect_effect / se_indirect
p_value = 2 * (1 - norm.cdf(abs(z_stat)))

print(f"Sobel test: z={z_stat:.2f}, p={p_value:.4f}")
```

**Moderation (Interaction Effects)**

```python
# X Ã— Z â†’ Y
# ä¾‹ï¼šR&D Ã— ç’°å¢ƒå‹•æ…‹æ€§ â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

# Center variables (è§£é‡ˆå®¹æ˜“æ€§ã®ãŸã‚)
df['rd_centered'] = df['rd_intensity'] - df['rd_intensity'].mean()
df['dynamism_centered'] = df['env_dynamism'] - df['env_dynamism'].mean()

mod_model = smf.ols('''
performance ~ rd_centered * dynamism_centered + 
              firm_size + leverage + age
''', data=df).fit(cov_type='cluster', cov_kwds={'groups': df['firm_id']})

print(mod_model.summary())

# Simple slope analysis
low_dynamism = df['dynamism_centered'].quantile(0.25)
high_dynamism = df['dynamism_centered'].quantile(0.75)

slope_low = mod_model.params['rd_centered'] + \
            mod_model.params['rd_centered:dynamism_centered'] * low_dynamism
slope_high = mod_model.params['rd_centered'] + \
             mod_model.params['rd_centered:dynamism_centered'] * high_dynamism

print(f"\nSimple slopes:")
print(f"Low dynamism: {slope_low:.4f}")
print(f"High dynamism: {slope_high:.4f}")

# Johnson-Neyman technique (region of significance)
# ã©ã®ç¯„å›²ã®Moderatorã§åŠ¹æœãŒæœ‰æ„ã‹
```

### B.3 Multilevel Modelingï¼ˆéšå±¤ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼‰

```python
import statsmodels.formula.api as smf

# Level 1: Firm-year
# Level 2: Industry
# Level 3: Country

# Random intercept model
mlm_formula = """
performance ~ rd_intensity + firm_size + leverage + age
"""

mlm_model = smf.mixedlm(
    mlm_formula,
    data=df,
    groups=df['industry'],  # Level 2
    re_formula="1"  # Random intercept
).fit()

print(mlm_model.summary())

# Random slope modelï¼ˆå‚¾ãã‚‚å¤‰å‹•ï¼‰
mlm_random_slope = smf.mixedlm(
    mlm_formula,
    data=df,
    groups=df['industry'],
    re_formula="1 + rd_intensity"  # Random intercept & slope
).fit()

# Cross-level interaction
# Industry-level moderator Ã— Firm-level predictor
mlm_cross_level = smf.mixedlm(
    "performance ~ rd_intensity * industry_dynamism + firm_controls",
    data=df,
    groups=df['industry'],
    re_formula="1"
).fit()
```

### B.4 Survival Analysis (Cox Hazard Model)

```python
from lifelines import CoxPHFitter

# ä¾‹ï¼šä¼æ¥­ã®å¸‚å ´é€€å‡ºï¼ˆexit, failure, delistingï¼‰

# Event: 1 = exited, 0 = censored
# Duration: Years until exit (or end of study)

df_survival = df[['firm_id', 'duration', 'exited', 
                  'firm_size', 'leverage', 'roa', 'rd_intensity']]

cph = CoxPHFitter()
cph.fit(df_survival, duration_col='duration', event_col='exited')

print(cph.summary)

# Hazard ratio interpretation
# HR > 1 â†’ Increased hazard (faster exit)
# HR < 1 â†’ Decreased hazard (slower exit)

# Survival curves by group
from lifelines import KaplanMeierFitter

kmf = KaplanMeierFitter()

# High R&D vs. Low R&D
high_rd = df_survival[df_survival['rd_intensity'] > df_survival['rd_intensity'].median()]
low_rd = df_survival[df_survival['rd_intensity'] <= df_survival['rd_intensity'].median()]

kmf.fit(high_rd['duration'], high_rd['exited'], label='High R&D')
ax = kmf.plot_survival_function()

kmf.fit(low_rd['duration'], low_rd['exited'], label='Low R&D')
kmf.plot_survival_function(ax=ax)

plt.title('Survival Curves by R&D Intensity')
plt.xlabel('Years')
plt.ylabel('Survival Probability')
plt.show()
```

---

## APPENDIX C: Publication Checklist for Top Journals

### C.1 Strategic Management Journal (SMJ) Requirements

**Data & Methods**:
- [ ] Sample selection clearly justified theoretically
- [ ] Survivor bias addressed (delisting firms included)
- [ ] Statistical power analysis reported
- [ ] Endogeneity concerns addressed (IV, FE, or discussion)
- [ ] Cluster-robust standard errors used
- [ ] At least 5 robustness checks
- [ ] Interaction effects plotted
- [ ] Alternative specifications tested

**Theory & Contribution**:
- [ ] Clear theoretical positioning (RBV, TCE, Institutional, etc.)
- [ ] Theoretical contribution explicitly stated (challenge/extend/integrate)
- [ ] Hypotheses derived from theory (not post-hoc)
- [ ] Boundary conditions discussed
- [ ] Managerial implications provided

**Transparency & Reproducibility**:
- [ ] Data sources fully documented
- [ ] Variable construction explained
- [ ] Replication materials available (or promised upon acceptance)
- [ ] Limitations honestly discussed

### C.2 Academy of Management Journal (AMJ) Requirements

**Additional to SMJ**:
- [ ] Qualitative insights (interviews, case examples) encouraged
- [ ] Multiple methods triangulation valued
- [ ] Organizational-level phenomena (not just firm-level)
- [ ] Attention to micro-foundations
- [ ] Process mechanisms explained
- [ ] Generalizability discussed

### C.3 Organization Science (OS) Requirements

**Additional**:
- [ ] Formal modeling or simulation (if applicable)
- [ ] Longitudinal data preferred
- [ ] Attention to organizational learning, routines
- [ ] Computational methods welcomed
- [ ] Strong theory development

### C.4 Administrative Science Quarterly (ASQ) Requirements

**Highest Standards**:
- [ ] Novel theoretical contribution (major)
- [ ] Rich contextual understanding
- [ ] Qualitative evidence often required
- [ ] Historical or processual analysis valued
- [ ] Inductive theory building from data
- [ ] Exceptional writing quality

---

## APPENDIX D: Error Messages & Solutions

### D.1 Common Data Collection Errors

**Error**: `ConnectionError: Max retries exceeded`
```python
# Solution: Implement exponential backoff
import time
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

session = requests.Session()
retry = Retry(
    total=5,
    backoff_factor=1,  # Wait 1, 2, 4, 8, 16 seconds
    status_forcelist=[429, 500, 502, 503, 504]
)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
session.mount('https://', adapter)

response = session.get(url)
```

**Error**: `KeyError: 'gvkey' not found after merge`
```python
# Solution: Check merge key existence before merge
print(f"Compustat unique GVKEYs: {df_compustat['gvkey'].nunique()}")
print(f"Patents unique GVKEYs: {df_patents['gvkey'].nunique()}")

# Use indicator to track merge success
df_merged = pd.merge(df_compustat, df_patents, 
                     on='gvkey', how='left', indicator=True)
print(df_merged['_merge'].value_counts())
```

**Error**: `MemoryError: Unable to allocate array`
```python
# Solution: Process in chunks
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
    # Or save to database incrementally
```

### D.2 Statistical Analysis Errors

**Error**: `LinAlgError: Singular matrix`
```python
# Cause: Perfect multicollinearity
# Solution: Check VIF and correlation matrix
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()
vif["Variable"] = X.columns
vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif[vif["VIF"] > 10])  # Problem variables

# Remove highly correlated variables
```

**Error**: `ValueError: array must not contain infs or NaNs`
```python
# Solution: Comprehensive data cleaning
df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=regression_variables)

# Or impute
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
```

**Error**: Regression coefficients unreasonably large
```python
# Cause: Scale mismatch
# Solution: Standardize variables
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['rd_intensity', 'firm_size']] = scaler.fit_transform(
    df[['rd_intensity', 'firm_size']]
)
```

---

## APPENDIX E: Sample Research Timeline

### Typical 12-Week Timelineï¼ˆãƒ•ãƒ«ã‚¿ã‚¤ãƒ ç ”ç©¶ï¼‰

**Weeks 1-2: Phase 1 (Research Design)**
- Literature review
- Theory selection
- RQ formulation
- Variable conceptualization

**Weeks 3-4: Phase 2-3 (Data Discovery & Sample Design)**
- Data source evaluation
- Sample selection criteria
- Power analysis
- Data access setup

**Weeks 5-7: Phase 4-5 (Collection & Integration)**
- Raw data download
- Data cleaning
- Variable construction
- Multi-source merging

**Week 8: Phase 6 (Quality Assurance)**
- Outlier detection
- Benford's Law test
- Structural breaks
- Balance analysis

**Weeks 9-10: Phase 7 (Analysis)**
- Descriptive statistics
- Main regression
- Robustness checks
- Theory testing

**Weeks 11-12: Phase 8 (Documentation)**
- Replication package
- Data dictionary
- Code comments
- Test suite

**Total: 12 weeks for experienced researchers**  
**First-time: 16-20 weeks recommended**

---

## æœ€çµ‚ç¢ºèªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### ãƒ‡ãƒ¼ã‚¿å“è³ª
- [ ] ã™ã¹ã¦ã®å¤‰æ•°ã«å‡ºå…¸ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹
- [ ] ã‚µãƒã‚¤ãƒãƒ«ãƒã‚¤ã‚¢ã‚¹å¯¾ç­–æ¸ˆã¿
- [ ] ä¼šè¨ˆæ’ç­‰å¼ãŒæˆç«‹ï¼ˆèª¤å·®<2%ï¼‰
- [ ] Benford's Law teståˆæ ¼
- [ ] ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ã‚¢å‡¦ç†æ¸ˆã¿ï¼ˆ1%/99% winsorizeï¼‰
- [ ] æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ–‡æ›¸åŒ–

### åˆ†æå“è³ª
- [ ] çµ±è¨ˆçš„æ¤œå‡ºåŠ› â‰¥ 80%
- [ ] ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–æ¨™æº–èª¤å·®ä½¿ç”¨
- [ ] VIF < 10ï¼ˆå¤šé‡å…±ç·šæ€§ãªã—ï¼‰
- [ ] 5ç¨®é¡ä»¥ä¸Šã®robustness checks
- [ ] å†…ç”Ÿæ€§ã¸ã®å¯¾å‡¦ï¼ˆIV, FE, ã¾ãŸã¯è­°è«–ï¼‰
- [ ] äº¤äº’ä½œç”¨åŠ¹æœã‚’å›³ç¤º

### å†ç¾æ€§
- [ ] å®Œå…¨ãªreplication packageã‚ã‚Š
- [ ] ãƒ‡ãƒ¼ã‚¿è¾æ›¸å®Œå‚™
- [ ] ã™ã¹ã¦ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå‹•ä½œç¢ºèªæ¸ˆã¿
- [ ] Pytest test suiteåˆæ ¼
- [ ] Dockerç’°å¢ƒæ§‹ç¯‰æ¸ˆã¿
- [ ] README.mdè©³ç´°

### ç†è«–çš„è²¢çŒ®
- [ ] æ—¢å­˜ç†è«–ã¨ã®é–¢ä¿‚æ˜ç¢º
- [ ] æ–°è¦æ€§ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹
- [ ] å¢ƒç•Œæ¡ä»¶ãŒè­°è«–ã•ã‚Œã¦ã„ã‚‹
- [ ] å®Ÿå‹™çš„ç¤ºå”†ã‚ã‚Š
- [ ] å°†æ¥ç ”ç©¶ã®æ–¹å‘æ€§æç¤º

---

**ğŸ“ ã“ã‚Œã§ strategic-management-research-hub v3.0 ã®å®Œå…¨ç‰ˆãŒå®Œæˆã—ã¾ã—ãŸï¼**

**æœ¬ã‚¹ã‚­ãƒ«ã®ç‰¹å¾´**ï¼š
- âœ… 8ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
- âœ… ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã§ä¸–ç•Œãƒ¬ãƒ™ãƒ«ã®ç ”ç©¶ãŒå¯èƒ½
- âœ… ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åŸºæº–å®Œå…¨å¯¾å¿œ
- âœ… å®Œå…¨å†ç¾å¯èƒ½æ€§
- âœ… åˆå¿ƒè€…ã‹ã‚‰ä¸Šç´šè€…ã¾ã§å¯¾å¿œ

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**ï¼š
```
ã€Œstrategic-management-research-hub skillã‚’ä½¿ç”¨ã—ã¦ã€
 [ã‚ãªãŸã®ç ”ç©¶ãƒ†ãƒ¼ãƒ]ã®å®Ÿè¨¼ç ”ç©¶ã‚’é–‹å§‹ã—ãŸã„ã€
```

**Good luck with your research! ğŸ“ŠğŸš€ğŸ“**

#æˆ¦ç•¥çµŒå–¶ç ”ç©¶ #å®Ÿè¨¼ç ”ç©¶ #ãƒ‡ãƒ¼ã‚¿åé›† #å“è³ªä¿è¨¼ #ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«

---

## APPENDIX F: Advanced Text Analysis for Strategic Research

### F.1 10-K MD&A Analysis (Management Discussion & Analysis)

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: SEC EDGARï¼ˆç„¡æ–™ï¼‰

**Why Important for Strategy Research**:
- çµŒå–¶è€…ã®æˆ¦ç•¥çš„æ„å›³ã®æŠŠæ¡
- å°†æ¥ã®æˆ¦ç•¥è»¢æ›ã®äºˆæ¸¬
- ãƒªã‚¹ã‚¯èªè­˜ã®åˆ†æ
- Forward-looking statementsã®å®šé‡åŒ–

#### Text Collection from SEC EDGAR

```python
import requests
from bs4 import BeautifulSoup
import re

class SECTextCollector:
    def __init__(self):
        self.base_url = "https://www.sec.gov/cgi-bin/browse-edgar"
        self.headers = {
            'User-Agent': 'YourUniversity research@email.edu'  # å¿…é ˆ
        }
    
    def get_10k_urls(self, cik, start_year, end_year):
        """ä¼æ¥­ã®10-K URLãƒªã‚¹ãƒˆã‚’å–å¾—"""
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',
            'dateb': f'{end_year}1231',
            'owner': 'exclude',
            'count': 100
        }
        
        response = requests.get(self.base_url, params=params, headers=self.headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        urls = []
        for row in soup.find_all('tr'):
            filing_date = row.find('td', text=re.compile(r'\d{4}-\d{2}-\d{2}'))
            if filing_date:
                year = int(filing_date.text[:4])
                if start_year <= year <= end_year:
                    doc_link = row.find('a', {'id': 'documentsbutton'})
                    if doc_link:
                        urls.append({
                            'year': year,
                            'url': f"https://www.sec.gov{doc_link['href']}"
                        })
        
        return urls
    
    def extract_mda_section(self, filing_url):
        """10-Kã‹ã‚‰MD&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        response = requests.get(filing_url, headers=self.headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Item 7ã‚’æ¤œç´¢ï¼ˆMD&Aï¼‰
        text = soup.get_text()
        
        # Item 7ã¨Item 7A/Item 8ã®é–“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        item7_pattern = r'ITEM\s+7\.?\s+MANAGEMENT[\s\S]*?(?=ITEM\s+7A|ITEM\s+8)'
        match = re.search(item7_pattern, text, re.IGNORECASE)
        
        if match:
            mda_text = match.group(0)
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            mda_text = re.sub(r'\s+', ' ', mda_text)  # ç©ºç™½æ­£è¦åŒ–
            mda_text = re.sub(r'_+', '', mda_text)   # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢å‰Šé™¤
            return mda_text
        
        return None

# ä½¿ç”¨ä¾‹
collector = SECTextCollector()

# Appleã®CIK: 0000320193
urls = collector.get_10k_urls('0000320193', 2015, 2023)

mda_texts = {}
for filing in urls:
    mda = collector.extract_mda_section(filing['url'])
    if mda:
        mda_texts[filing['year']] = mda
        print(f"Extracted MD&A for {filing['year']}: {len(mda)} characters")
```

#### Sentiment Analysis

```python
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd

# LM Financial Dictionaryï¼ˆé‡‘èç‰¹åŒ–ï¼‰
from pysentiment2 import LM

def analyze_mda_sentiment(mda_text):
    """MD&Aã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
    
    # 1. VADERï¼ˆæ±ç”¨ï¼‰
    sia = SentimentIntensityAnalyzer()
    vader_scores = sia.polarity_scores(mda_text)
    
    # 2. Loughran-McDonaldè¾æ›¸ï¼ˆé‡‘èç‰¹åŒ–ï¼‰
    lm = LM()
    lm_tokens = lm.tokenize(mda_text)
    lm_scores = lm.get_score(lm_tokens)
    
    return {
        'vader_positive': vader_scores['pos'],
        'vader_negative': vader_scores['neg'],
        'vader_neutral': vader_scores['neu'],
        'vader_compound': vader_scores['compound'],
        'lm_positive': lm_scores['Positive'],
        'lm_negative': lm_scores['Negative'],
        'lm_polarity': lm_scores['Polarity'],
        'lm_subjectivity': lm_scores['Subjectivity']
    }

# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨
sentiment_df = pd.DataFrame([
    {
        'year': year,
        **analyze_mda_sentiment(text)
    }
    for year, text in mda_texts.items()
])

print(sentiment_df)

# æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨
# DV: å°†æ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
# IV: MD&Aã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼ˆçµŒå–¶è€…ã®æ¥½è¦³åº¦ï¼‰
# ä»®èª¬ï¼šæ¥½è¦³çš„MD&A â†’ å°†æ¥ã®æŠ•è³‡å¢—åŠ  â†’ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¤‰åŒ–
```

#### Strategic Theme Extraction (Topic Modeling)

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

def extract_strategic_topics(mda_texts, n_topics=5):
    """LDAã§MD&Aã‹ã‚‰æˆ¦ç•¥ãƒ†ãƒ¼ãƒã‚’æŠ½å‡º"""
    
    # å‰å‡¦ç†
    from nltk.corpus import stopwords
    stop_words = set(stopwords.words('english'))
    stop_words.update(['company', 'business', 'year', 'fiscal'])
    
    # TF-IDF
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=list(stop_words),
        ngram_range=(1, 2)  # bigramå«ã‚€
    )
    
    tfidf = vectorizer.fit_transform(list(mda_texts.values()))
    
    # LDA
    lda = LatentDirichletAllocation(
        n_components=n_topics,
        random_state=42
    )
    
    lda_topics = lda.fit_transform(tfidf)
    
    # ãƒˆãƒ”ãƒƒã‚¯è§£é‡ˆ
    feature_names = vectorizer.get_feature_names_out()
    
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[-10:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        print(f"\nTopic {topic_idx+1}: {', '.join(top_words)}")
    
    # å„å¹´ã®ãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ
    topic_df = pd.DataFrame(
        lda_topics,
        columns=[f'topic_{i+1}' for i in range(n_topics)],
        index=list(mda_texts.keys())
    )
    
    return topic_df

topic_distribution = extract_strategic_topics(mda_texts, n_topics=5)

# æˆ¦ç•¥ç ”ç©¶ã§ã®ä½¿ç”¨
# ä¾‹ï¼šTopic 1ï¼ˆã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£èªï¼‰ã®å¢—åŠ  â†’ R&DæŠ•è³‡å¢—
# ä¾‹ï¼šTopic 2ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›èªï¼‰ã®å¢—åŠ  â†’ ãƒªã‚¹ãƒˆãƒ©äºˆæ¸¬
```

#### Forward-Looking Statements Measurement

```python
import re

def measure_forward_looking(mda_text):
    """Forward-looking statementsã®å®šé‡åŒ–"""
    
    # Forward-looking keywords
    forward_keywords = [
        'will', 'expect', 'anticipate', 'believe', 'plan', 
        'project', 'estimate', 'forecast', 'predict', 
        'intend', 'target', 'goal', 'outlook', 'guidance'
    ]
    
    # Uncertainty keywords
    uncertainty_keywords = [
        'risk', 'uncertain', 'may', 'could', 'might', 
        'possible', 'potential', 'depends', 'subject to'
    ]
    
    # æ–‡ã«åˆ†å‰²
    sentences = re.split(r'[.!?]+', mda_text)
    
    forward_count = 0
    uncertain_count = 0
    
    for sentence in sentences:
        sentence_lower = sentence.lower()
        
        # Forward-lookingæ–‡ã‹ãƒã‚§ãƒƒã‚¯
        if any(kw in sentence_lower for kw in forward_keywords):
            forward_count += 1
        
        # ä¸ç¢ºå®Ÿæ€§èªã‚’å«ã‚€ã‹ãƒã‚§ãƒƒã‚¯
        if any(kw in sentence_lower for kw in uncertainty_keywords):
            uncertain_count += 1
    
    total_sentences = len([s for s in sentences if len(s.strip()) > 20])
    
    return {
        'forward_looking_ratio': forward_count / total_sentences,
        'uncertainty_ratio': uncertain_count / total_sentences,
        'forward_looking_count': forward_count,
        'uncertainty_count': uncertain_count
    }

# é©ç”¨
forward_df = pd.DataFrame([
    {
        'year': year,
        **measure_forward_looking(text)
    }
    for year, text in mda_texts.items()
])

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H: Forward-lookingæ¯”ç‡ãŒé«˜ã„ä¼æ¥­ â†’ æˆ¦ç•¥çš„æŠ•è³‡å¢— â†’ å°†æ¥æˆé•·ç‡é«˜
# H: ä¸ç¢ºå®Ÿæ€§èªãŒå¤šã„ä¼æ¥­ â†’ ãƒªã‚¹ã‚¯èªè­˜é«˜ â†’ ä¿å®ˆçš„æˆ¦ç•¥
```

### F.2 Earnings Call Transcript Analysis

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**:
- Motley Fool Transcriptsï¼ˆä¸€éƒ¨ç„¡æ–™ï¼‰
- Seeking Alphaï¼ˆä¸€éƒ¨ç„¡æ–™ï¼‰
- S&P Capital IQï¼ˆæœ‰æ–™ã€WRDSã‹ã‚‰ï¼‰

#### Strategic Content Analysis

```python
def analyze_strategy_discussion(transcript):
    """æ±ºç®—èª¬æ˜ä¼šã§ã®æˆ¦ç•¥è­°è«–ã®å®šé‡åŒ–"""
    
    # æˆ¦ç•¥é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    strategy_keywords = {
        'innovation': ['innovation', 'r&d', 'research', 'development', 'patent'],
        'expansion': ['expansion', 'growth', 'new market', 'international'],
        'efficiency': ['efficiency', 'cost', 'optimization', 'streamline'],
        'acquisition': ['acquisition', 'merger', 'buy', 'acquire'],
        'digital': ['digital', 'technology', 'automation', 'ai', 'machine learning']
    }
    
    results = {}
    for theme, keywords in strategy_keywords.items():
        count = sum(transcript.lower().count(kw) for kw in keywords)
        results[f'{theme}_mentions'] = count
    
    # ç›¸å¯¾çš„é‡è¦–åº¦
    total_strategic = sum(results.values())
    for theme in strategy_keywords.keys():
        results[f'{theme}_emphasis'] = (
            results[f'{theme}_mentions'] / total_strategic 
            if total_strategic > 0 else 0
        )
    
    return results

# Q&A sectionã®åˆ†é›¢åˆ†æ
def analyze_qa_tone(transcript):
    """Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒˆãƒ¼ãƒ³åˆ†æ"""
    
    # Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
    qa_pattern = r'QUESTION.*?(?=QUESTION|$)'
    qa_sections = re.findall(qa_pattern, transcript, re.DOTALL | re.IGNORECASE)
    
    # è³ªå•ã®ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡
    challenging_keywords = ['concern', 'worry', 'decline', 'risk', 'challenge']
    positive_keywords = ['opportunity', 'growth', 'strength', 'success']
    
    challenging_questions = sum(
        1 for qa in qa_sections 
        if any(kw in qa.lower() for kw in challenging_keywords)
    )
    
    positive_questions = sum(
        1 for qa in qa_sections 
        if any(kw in qa.lower() for kw in positive_keywords)
    )
    
    return {
        'total_questions': len(qa_sections),
        'challenging_ratio': challenging_questions / len(qa_sections) if qa_sections else 0,
        'positive_ratio': positive_questions / len(qa_sections) if qa_sections else 0
    }

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³è¨€åŠå¢— â†’ å®Ÿéš›ã®R&DæŠ•è³‡å¢—ï¼ˆ6-12ãƒ¶æœˆå¾Œï¼‰
# H: Q&Aã§æŒ‘æˆ¦çš„è³ªå•å¤š â†’ çµŒå–¶ä¸é€æ˜æ€§ â†’ æ ªä¾¡volatilityé«˜
```

---

## APPENDIX G: Network Analysis for Strategic Research

### G.1 Board Interlock Networks

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: 
- ISS Directorsï¼ˆæœ‰æ–™ï¼‰
- BoardExï¼ˆæœ‰æ–™ï¼‰
- å…¬é–‹æƒ…å ±ï¼ˆProxy statements, DEF 14Aï¼‰

#### Network Construction

```python
import networkx as nx
import pandas as pd

def build_board_network(director_data):
    """å–ç· å½¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰"""
    
    # director_data columns: director_id, director_name, firm_id, firm_name, year
    
    # Bipartite graph: directors <-> firms
    B = nx.Graph()
    
    # ãƒãƒ¼ãƒ‰è¿½åŠ 
    directors = director_data['director_id'].unique()
    firms = director_data['firm_id'].unique()
    
    B.add_nodes_from(directors, bipartite=0)  # Directors
    B.add_nodes_from(firms, bipartite=1)      # Firms
    
    # ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆdirector-firm affiliationsï¼‰
    for _, row in director_data.iterrows():
        B.add_edge(row['director_id'], row['firm_id'], year=row['year'])
    
    # Firm-to-firm projection (interlock network)
    firms = {n for n, d in B.nodes(data=True) if d['bipartite'] == 1}
    G_firms = nx.bipartite.projected_graph(B, firms)
    
    return G_firms

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŒ‡æ¨™è¨ˆç®—
def calculate_network_metrics(firm_id, G):
    """ä¼æ¥­ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŒ‡æ¨™"""
    
    metrics = {}
    
    # Degree centralityï¼ˆç›´æ¥çš„interlockæ•°ï¼‰
    metrics['degree_centrality'] = nx.degree_centrality(G)[firm_id]
    
    # Betweenness centralityï¼ˆãƒ–ãƒªãƒƒã‚¸å½¹å‰²ï¼‰
    metrics['betweenness_centrality'] = nx.betweenness_centrality(G)[firm_id]
    
    # Eigenvector centralityï¼ˆinfluential firmsã¨ã®æ¥ç¶šï¼‰
    metrics['eigenvector_centrality'] = nx.eigenvector_centrality(G, max_iter=1000)[firm_id]
    
    # Clustering coefficientï¼ˆtriadic closureï¼‰
    metrics['clustering'] = nx.clustering(G)[firm_id]
    
    # Number of interlocks
    metrics['num_interlocks'] = G.degree(firm_id)
    
    return metrics

# å…¨ä¼æ¥­ã«é©ç”¨
G_board = build_board_network(director_df)

network_metrics = pd.DataFrame([
    {
        'firm_id': firm,
        'year': year,
        **calculate_network_metrics(firm, G_board)
    }
    for firm in G_board.nodes()
])

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H1: é«˜centralityä¼æ¥­ â†’ æƒ…å ±å„ªä½ â†’ æ—©æœŸå¸‚å ´å‚å…¥
# H2: Board interlock â†’ æˆ¦ç•¥æ¨¡å€£ï¼ˆisomorphismï¼‰
# H3: Betweennessé«˜ â†’ ãƒ–ãƒªãƒƒã‚¸å½¹ â†’ æ–°è¦ææºæ©Ÿä¼šå¤š
```

#### Network Visualization

```python
import matplotlib.pyplot as plt

def visualize_board_network(G, focal_firms=None, output_file='board_network.png'):
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–"""
    
    # Layout
    pos = nx.spring_layout(G, k=0.5, iterations=50)
    
    plt.figure(figsize=(15, 15))
    
    # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºï¼šDegree centralityæ¯”ä¾‹
    node_sizes = [G.degree(node) * 100 for node in G.nodes()]
    
    # ãƒãƒ¼ãƒ‰ã‚«ãƒ©ãƒ¼ï¼šFocal firmså¼·èª¿
    node_colors = []
    for node in G.nodes():
        if focal_firms and node in focal_firms:
            node_colors.append('red')
        else:
            node_colors.append('lightblue')
    
    # æç”»
    nx.draw_networkx_nodes(G, pos, 
                           node_size=node_sizes,
                           node_color=node_colors,
                           alpha=0.6)
    
    nx.draw_networkx_edges(G, pos, alpha=0.2)
    
    # ãƒ©ãƒ™ãƒ«ï¼ˆé«˜centralityä¼æ¥­ã®ã¿ï¼‰
    degree_cent = nx.degree_centrality(G)
    top_firms = sorted(degree_cent, key=degree_cent.get, reverse=True)[:20]
    labels = {node: node for node in top_firms}
    nx.draw_networkx_labels(G, pos, labels, font_size=8)
    
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Network visualization saved to {output_file}")

# å®Ÿè¡Œ
visualize_board_network(G_board, focal_firms=['AAPL', 'MSFT', 'GOOG'])
```

### G.2 Strategic Alliance Networks

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**:
- SDC Joint Ventures & Alliancesï¼ˆWRDSï¼‰
- Thomson Reuters Securities Data
- ä¼æ¥­é–‹ç¤ºæƒ…å ±

#### Alliance Network Construction

```python
def build_alliance_network(alliance_data):
    """æˆ¦ç•¥çš„ææºãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
    
    # alliance_data columns: alliance_id, firm1_id, firm2_id, 
    #                        alliance_type, date, industry
    
    G = nx.Graph()
    
    # ã‚¨ãƒƒã‚¸è¿½åŠ ï¼ˆææºé–¢ä¿‚ï¼‰
    for _, row in alliance_data.iterrows():
        G.add_edge(
            row['firm1_id'],
            row['firm2_id'],
            alliance_id=row['alliance_id'],
            type=row['alliance_type'],
            date=row['date']
        )
    
    return G

def calculate_alliance_portfolio_metrics(firm_id, G, firm_data):
    """ä¼æ¥­ã®ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæŒ‡æ¨™"""
    
    if firm_id not in G:
        return {
            'num_alliances': 0,
            'partner_diversity': 0,
            'avg_partner_size': 0
        }
    
    # ææºæ•°
    partners = list(G.neighbors(firm_id))
    num_alliances = len(partners)
    
    # Partner diversityï¼ˆç”£æ¥­å¤šæ§˜æ€§ï¼‰
    partner_industries = firm_data.loc[
        firm_data['firm_id'].isin(partners), 'industry'
    ]
    partner_diversity = partner_industries.nunique() / len(partner_industries) if partners else 0
    
    # å¹³å‡partnerè¦æ¨¡
    partner_sizes = firm_data.loc[
        firm_data['firm_id'].isin(partners), 'total_assets'
    ]
    avg_partner_size = partner_sizes.mean() if not partner_sizes.empty else 0
    
    # Network position
    degree_cent = nx.degree_centrality(G).get(firm_id, 0)
    betweenness = nx.betweenness_centrality(G).get(firm_id, 0)
    
    return {
        'num_alliances': num_alliances,
        'partner_diversity': partner_diversity,
        'avg_partner_size': avg_partner_size,
        'alliance_degree_centrality': degree_cent,
        'alliance_betweenness': betweenness
    }

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H1: ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå¤šæ§˜æ€§ â†’ ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆæœâ†‘
# H2: Alliance network centrality â†’ å¸‚å ´æƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹ â†’ first-mover advantage
# H3: Partner size heterogeneity â†’ complementary resources â†’ ææºæˆåŠŸç‡â†‘
```

### G.3 Patent Citation Networks

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USPTO PatentsViewï¼ˆç„¡æ–™ï¼‰

#### Patent Network Analysis

```python
def build_patent_citation_network(patent_data):
    """ç‰¹è¨±å¼•ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
    
    # patent_data columns: patent_id, assignee_id, cited_patent_id
    
    # Directed graphï¼ˆå¼•ç”¨æ–¹å‘ï¼‰
    G = nx.DiGraph()
    
    for _, row in patent_data.iterrows():
        if pd.notna(row['cited_patent_id']):
            G.add_edge(
                row['patent_id'],
                row['cited_patent_id'],
                assignee=row['assignee_id']
            )
    
    return G

def calculate_knowledge_flow_metrics(firm_id, patent_network, patent_assignee_map):
    """ä¼æ¥­ã®çŸ¥è­˜ãƒ•ãƒ­ãƒ¼æŒ‡æ¨™"""
    
    firm_patents = [p for p, a in patent_assignee_map.items() if a == firm_id]
    
    # Knowledge inflowï¼ˆè¢«å¼•ç”¨ï¼‰
    inflow_citations = sum(
        patent_network.in_degree(p) for p in firm_patents if p in patent_network
    )
    
    # Knowledge outflowï¼ˆå¼•ç”¨ï¼‰
    outflow_citations = sum(
        patent_network.out_degree(p) for p in firm_patents if p in patent_network
    )
    
    # Self-citationsç‡
    total_citations_made = outflow_citations
    self_citations = 0
    
    for patent in firm_patents:
        if patent in patent_network:
            cited_patents = patent_network.successors(patent)
            self_citations += sum(
                1 for cp in cited_patents 
                if patent_assignee_map.get(cp) == firm_id
            )
    
    self_citation_rate = self_citations / total_citations_made if total_citations_made > 0 else 0
    
    # Knowledge diversityï¼ˆå¼•ç”¨å…ˆã®æŠ€è¡“ã‚¯ãƒ©ã‚¹å¤šæ§˜æ€§ï¼‰
    # ï¼ˆå®Ÿè£…ã«ã¯ç‰¹è¨±ã®IPCã‚¯ãƒ©ã‚¹æƒ…å ±ãŒå¿…è¦ï¼‰
    
    return {
        'knowledge_inflow': inflow_citations,
        'knowledge_outflow': outflow_citations,
        'self_citation_rate': self_citation_rate,
        'net_knowledge_flow': inflow_citations - outflow_citations
    }

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H1: é«˜knowledge inflow â†’ absorptive capacityâ†‘ â†’ ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æˆæœâ†‘
# H2: ä½self-citationç‡ â†’ external knowledge exploration â†’ radical innovation
# H3: Knowledge network centrality â†’ æŠ€è¡“çš„å½±éŸ¿åŠ› â†’ æ¨™æº–åŒ–ä¸»å°
```

---

## APPENDIX H: Machine Learning & Causal Inference Integration

### H.1 Causal Forest (Heterogeneous Treatment Effects)

**ç›®çš„**: å‡¦ç½®åŠ¹æœã®ä¼æ¥­å±æ€§ã«ã‚ˆã‚‹ç•°è³ªæ€§ã‚’æ¨å®š

```python
from econml.dml import CausalForestDML
from sklearn.ensemble import RandomForestRegressor
import numpy as np

def estimate_heterogeneous_effects(df, treatment, outcome, controls, heterogeneity_vars):
    """
    å‡¦ç½®åŠ¹æœã®ç•°è³ªæ€§æ¨å®š
    
    ä¾‹ï¼šM&AãŒæ¥­ç¸¾ã«ä¸ãˆã‚‹åŠ¹æœãŒã€ä¼æ¥­è¦æ¨¡ãƒ»ç”£æ¥­ãƒ»çµŒå–¶è€…ç‰¹æ€§ã§ã©ã†ç•°ãªã‚‹ã‹
    
    Parameters:
    - treatment: å‡¦ç½®å¤‰æ•°ï¼ˆä¾‹: M&Aå®Ÿæ–½ãƒ€ãƒŸãƒ¼ï¼‰
    - outcome: çµæœå¤‰æ•°ï¼ˆä¾‹: ROAï¼‰
    - controls: çµ±åˆ¶å¤‰æ•°ãƒªã‚¹ãƒˆ
    - heterogeneity_vars: ç•°è³ªæ€§ã‚’èª¿ã¹ã‚‹å¤‰æ•°ãƒªã‚¹ãƒˆ
    """
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    T = df[treatment].values
    Y = df[outcome].values
    X = df[heterogeneity_vars].values
    W = df[controls].values if controls else None
    
    # Causal Forestæ¨å®š
    est = CausalForestDML(
        model_y=RandomForestRegressor(n_estimators=100, random_state=42),
        model_t=RandomForestRegressor(n_estimators=100, random_state=42),
        discrete_treatment=True,
        n_estimators=100,
        random_state=42
    )
    
    est.fit(Y, T, X=X, W=W)
    
    # å€‹åˆ¥å‡¦ç½®åŠ¹æœï¼ˆCATE: Conditional Average Treatment Effectï¼‰
    cate = est.effect(X)
    
    # ä¿¡é ¼åŒºé–“
    cate_lower, cate_upper = est.effect_interval(X, alpha=0.05)
    
    # çµæœã‚’DataFrameã«
    results = df.copy()
    results['cate'] = cate
    results['cate_lower'] = cate_lower
    results['cate_upper'] = cate_upper
    
    # Feature importanceï¼ˆã©ã®å¤‰æ•°ãŒç•°è³ªæ€§ã‚’ç”Ÿã‚€ã‹ï¼‰
    feature_importances = est.feature_importances_
    importance_df = pd.DataFrame({
        'feature': heterogeneity_vars,
        'importance': feature_importances
    }).sort_values('importance', ascending=False)
    
    print("Feature Importances for Treatment Effect Heterogeneity:")
    print(importance_df)
    
    return results, importance_df

# ä½¿ç”¨ä¾‹ï¼šM&Aã®ç•°è³ªçš„åŠ¹æœ
df_with_cate, importance = estimate_heterogeneous_effects(
    df=panel_df,
    treatment='ma_dummy',
    outcome='roa_change',
    controls=['firm_size', 'leverage', 'firm_age'],
    heterogeneity_vars=['firm_size', 'rd_intensity', 'prior_ma_experience', 'industry_dynamism']
)

# å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.scatter(df_with_cate['firm_size'], df_with_cate['cate'], alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Firm Size')
plt.ylabel('M&A Treatment Effect on ROA')
plt.title('Heterogeneous M&A Effects by Firm Size')
plt.savefig('./figures/heterogeneous_ma_effects.png', dpi=300)

# æˆ¦ç•¥çš„ç¤ºå”†
# CATEãŒæ­£â†’M&AãŒæœ‰åŠ¹ãªä¼æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç‰¹å®š
# CATEãŒè² â†’M&AãŒæœ‰å®³ãªä¼æ¥­ç‰¹æ€§ã‚’ç‰¹å®š
# Feature importanceâ†’ã©ã®ä¼æ¥­ç‰¹æ€§ãŒåŠ¹æœã‚’å·¦å³ã™ã‚‹ã‹
```

### H.2 Double Machine Learning (DML)

**ç›®çš„**: å†…ç”Ÿæ€§ã«é ‘å¥ãªæ¨å®šï¼ˆé«˜æ¬¡å…ƒçµ±åˆ¶å¤‰æ•°ä¸‹ï¼‰

```python
from econml.dml import LinearDML
from sklearn.ensemble import GradientBoostingRegressor

def dml_estimation(df, treatment, outcome, controls, instruments=None):
    """
    Double Machine Learningã§å‡¦ç½®åŠ¹æœæ¨å®š
    
    åˆ©ç‚¹ï¼š
    - é«˜æ¬¡å…ƒçµ±åˆ¶å¤‰æ•°ã§ã‚‚ consistent
    - éç·šå½¢confoundingã«é ‘å¥
    - Selection on observablesä¸‹ã§ unbiased
    """
    
    Y = df[outcome].values
    T = df[treatment].values
    X = df[controls].values
    
    # DMLæ¨å®š
    est = LinearDML(
        model_y=GradientBoostingRegressor(n_estimators=100),
        model_t=GradientBoostingRegressor(n_estimators=100),
        discrete_treatment=False,
        linear_first_stages=False
    )
    
    est.fit(Y, T, X=X, W=None)
    
    # å‡¦ç½®åŠ¹æœæ¨å®šå€¤
    ate = est.effect(X).mean()
    ate_se = est.effect_stderr(X).mean()
    
    print(f"\nDouble Machine Learning Results:")
    print(f"Average Treatment Effect: {ate:.4f}")
    print(f"Standard Error: {ate_se:.4f}")
    print(f"95% CI: [{ate - 1.96*ate_se:.4f}, {ate + 1.96*ate_se:.4f}]")
    
    # å€‹åˆ¥åŠ¹æœæ¨å®š
    individual_effects = est.effect(X)
    
    return {
        'ate': ate,
        'ate_se': ate_se,
        'individual_effects': individual_effects
    }

# ä½¿ç”¨ä¾‹ï¼šR&DæŠ•è³‡ã®åŠ¹æœï¼ˆé«˜æ¬¡å…ƒçµ±åˆ¶ä¸‹ï¼‰
controls = ['firm_size', 'firm_age', 'leverage', 'cash_holdings', 
            'tangibility', 'market_to_book', 'sales_growth',
            'industry_concentration', 'gdp_growth', 'interest_rate']

dml_results = dml_estimation(
    df=panel_df,
    treatment='rd_intensity',
    outcome='roa_lead2',  # 2å¹´å¾ŒROA
    controls=controls
)

# å¾“æ¥ã®OLSã¨æ¯”è¼ƒ
import statsmodels.formula.api as smf

ols_formula = 'roa_lead2 ~ rd_intensity + ' + ' + '.join(controls)
ols_model = smf.ols(ols_formula, data=panel_df).fit(cov_type='cluster', 
                                                      cov_kwds={'groups': panel_df['firm_id']})

print(f"\nOLS coefficient: {ols_model.params['rd_intensity']:.4f}")
print(f"DML coefficient: {dml_results['ate']:.4f}")
print("\nDML is more robust to confounding bias")
```

### H.3 Synthetic Control Method

**ç›®çš„**: ã‚¤ãƒ™ãƒ³ãƒˆç ”ç©¶ï¼ˆå°‘æ•°å‡¦ç½®ãƒ¦ãƒ‹ãƒƒãƒˆï¼‰

```python
from CausalPy import pymc_experiments
import numpy as np

def synthetic_control_analysis(df, treated_firm, treatment_date, outcome_var, donor_pool):
    """
    Synthetic Controlæ¨å®š
    
    ä¾‹ï¼šç‰¹å®šä¼æ¥­ã®M&AåŠ¹æœã‚’ã€é¡ä¼¼ä¼æ¥­ã®åŠ é‡å¹³å‡ï¼ˆsynthetic controlï¼‰ã¨æ¯”è¼ƒ
    
    Parameters:
    - treated_firm: å‡¦ç½®ã‚’å—ã‘ãŸä¼æ¥­ID
    - treatment_date: å‡¦ç½®æ™‚ç‚¹
    - outcome_var: çµæœå¤‰æ•°
    - donor_pool: Controlå€™è£œä¼æ¥­ã®ãƒªã‚¹ãƒˆ
    """
    
    # Pre-treatmentæœŸé–“
    pre_treatment = df[df['date'] < treatment_date]
    post_treatment = df[df['date'] >= treatment_date]
    
    # Treated firmã®ãƒ‡ãƒ¼ã‚¿
    treated_pre = pre_treatment[pre_treatment['firm_id'] == treated_firm][outcome_var].values
    treated_post = post_treatment[post_treatment['firm_id'] == treated_firm][outcome_var].values
    
    # Donor poolã®ãƒ‡ãƒ¼ã‚¿ï¼ˆè¡Œåˆ—å½¢å¼ï¼‰
    donor_pre = pre_treatment[pre_treatment['firm_id'].isin(donor_pool)].pivot(
        index='date', columns='firm_id', values=outcome_var
    ).values
    
    donor_post = post_treatment[post_treatment['firm_id'].isin(donor_pool)].pivot(
        index='date', columns='firm_id', values=outcome_var
    ).values
    
    # Synthetic controlã®é‡ã¿æ¨å®šï¼ˆpre-treatmenté©åˆï¼‰
    from scipy.optimize import minimize
    
    def objective(weights):
        synthetic = donor_pre @ weights
        return np.sum((treated_pre - synthetic) ** 2)
    
    # åˆ¶ç´„ï¼šé‡ã¿ã®åˆè¨ˆ=1ã€éè² 
    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    bounds = [(0, 1) for _ in range(len(donor_pool))]
    
    result = minimize(
        objective,
        x0=np.ones(len(donor_pool)) / len(donor_pool),
        bounds=bounds,
        constraints=constraints
    )
    
    optimal_weights = result.x
    
    # Synthetic controlã®æ§‹ç¯‰
    synthetic_pre = donor_pre @ optimal_weights
    synthetic_post = donor_post @ optimal_weights
    
    # å‡¦ç½®åŠ¹æœï¼ˆpost-treatmentå·®ï¼‰
    treatment_effect = treated_post - synthetic_post
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(12, 6))
    
    time_axis = np.arange(len(treated_pre) + len(treated_post))
    plt.plot(time_axis[:len(treated_pre)], treated_pre, 'b-', label='Treated Firm', linewidth=2)
    plt.plot(time_axis[len(treated_pre):], treated_post, 'b-', linewidth=2)
    
    plt.plot(time_axis[:len(synthetic_pre)], synthetic_pre, 'r--', label='Synthetic Control', linewidth=2)
    plt.plot(time_axis[len(synthetic_pre):], synthetic_post, 'r--', linewidth=2)
    
    plt.axvline(x=len(treated_pre), color='gray', linestyle=':', label='Treatment')
    plt.xlabel('Time')
    plt.ylabel(outcome_var)
    plt.legend()
    plt.title('Synthetic Control Analysis')
    plt.savefig('./figures/synthetic_control.png', dpi=300)
    
    print(f"\nSynthetic Control Weights:")
    weight_df = pd.DataFrame({
        'firm_id': donor_pool,
        'weight': optimal_weights
    }).sort_values('weight', ascending=False)
    print(weight_df)
    
    print(f"\nAverage Treatment Effect (post-period): {treatment_effect.mean():.4f}")
    
    return {
        'weights': optimal_weights,
        'treatment_effect': treatment_effect,
        'synthetic_control': np.concatenate([synthetic_pre, synthetic_post])
    }

# ä½¿ç”¨ä¾‹ï¼šAppleã®ç‰¹å®šM&AåŠ¹æœ
sc_results = synthetic_control_analysis(
    df=panel_df,
    treated_firm='AAPL',
    treatment_date='2014-05-01',  # Beatsè²·å
    outcome_var='innovation_output',
    donor_pool=['MSFT', 'GOOG', 'AMZN', 'FB', 'NFLX']
)
```

---

## APPENDIX I: Extended ESG & Sustainability Data Sources

### I.1 Comprehensive ESG Database Catalog

#### Tier 1: Premium ESG Data (æœ‰æ–™ã€é«˜å“è³ª)

**1. MSCI ESG Research**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š14,000+ ä¼æ¥­
æŒ‡æ¨™æ•°ï¼š1,000+ ESG metrics
å¼·ã¿ï¼š
- ç”£æ¥­åˆ¥ãƒãƒ†ãƒªã‚¢ãƒªãƒ†ã‚£ãƒãƒƒãƒ—
- ESG controversies tracking
- Climate risk scores

Costï¼š$50,000-$200,000/year
æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- ESG rating â†’ firm value (Tobin's Q)
- ESG controversies â†’ reputation loss
- Climate risk exposure â†’ è³‡æœ¬ã‚³ã‚¹ãƒˆ
```

**2. Refinitiv (Thomson Reuters) ESG**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š11,000+ ä¼æ¥­
æŒ‡æ¨™æ•°ï¼š630+ ESG metrics (10ã‚«ãƒ†ã‚´ãƒªã€186ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª)
å¼·ã¿ï¼š
- é•·æœŸæ™‚ç³»åˆ—ï¼ˆ2002ã€œï¼‰
- Datastreamçµ±åˆ
- Carbon emissionsè©³ç´°

Costï¼šDatastreamå¥‘ç´„ã«å«ã¾ã‚Œã‚‹ï¼ˆå¤§å­¦å¥‘ç´„ï¼‰
APIï¼šåˆ©ç”¨å¯èƒ½
```

**3. Sustainalytics ESG Risk Ratings**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š20,000+ ä¼æ¥­
æŒ‡æ¨™ï¼šESG risk scores (0-100)
å¼·ã¿ï¼š
- Unmanaged risk focus
- Material ESG issuesé‡è¦–
- Morningstarçµ±åˆ

Costï¼š$10,000-$100,000/year
```

**4. Bloomberg ESG Data**
```
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š14,000+ ä¼æ¥­
å¼·ã¿ï¼š
- Bloomberg terminalçµ±åˆ
- Real-time news + ESG
- ç‹¬è‡ªESG disclosure score

Costï¼šBloomberg terminalå¥‘ç´„å¿…è¦ï¼ˆ$24,000/yearï¼‰
```

#### Tier 2: Free & Low-Cost ESG Sources

**1. CDP (Carbon Disclosure Project)** ğŸŒŸ **æ¨å¥¨ãƒ»ç„¡æ–™**
```
URLï¼šhttps://www.cdp.net/en/data
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š13,000+ ä¼æ¥­ï¼ˆvoluntary disclosureï¼‰
ãƒ‡ãƒ¼ã‚¿ï¼š
- Climate Change: Scope 1, 2, 3æ’å‡ºé‡
- Water Security: æ°´ä½¿ç”¨é‡ã€ãƒªã‚¹ã‚¯
- Forests: Deforestation risk

ã‚¢ã‚¯ã‚»ã‚¹ï¼š
- ç ”ç©¶è€…ç„¡æ–™ï¼ˆapplicationå¿…è¦ï¼‰
- ä¼æ¥­åˆ¥detailed questionnaire responses
- APIï¼šã‚ã‚Šï¼ˆåˆ¶é™ä»˜ãï¼‰

Pythonå®Ÿè£…ï¼š
```python
import requests

# CDP APIï¼ˆè¦ç™»éŒ²ï¼‰
api_key = "YOUR_CDP_API_KEY"
headers = {"Authorization": f"Bearer {api_key}"}

# ä¼æ¥­ã®æ°—å€™å¤‰å‹•ãƒ‡ãƒ¼ã‚¿å–å¾—
company_id = "CDP001234"
response = requests.get(
    f"https://api.cdp.net/2024/companies/{company_id}/climate",
    headers=headers
)

climate_data = response.json()
print(f"Scope 1 emissions: {climate_data['scope1_emissions']} tCO2e")
```

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- Carbon emissions â†’ Operating costs
- Climate risk disclosure â†’ Investor attention
- Water risk â†’ Supply chain resilience
```

**2. GRI (Global Reporting Initiative) Database** ğŸŒŸ **æ¨å¥¨ãƒ»ç„¡æ–™**
```
URLï¼šhttps://database.globalreporting.org/
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š60,000+ sustainability reports
ãƒ‡ãƒ¼ã‚¿å½¢å¼ï¼šPDF reportsï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†æå¿…è¦ï¼‰

æ´»ç”¨ï¼š
- Sustainability disclosure qualityæ¸¬å®š
- GRIåŸºæº–æº–æ‹ åº¦
- Materiality assessmentåˆ†æ

Pythonå®Ÿè£…ï¼š
```python
import pdfplumber
import re

def extract_gri_disclosures(pdf_path):
    """GRIãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰é–‹ç¤ºé …ç›®æŠ½å‡º"""
    
    with pdfplumber.open(pdf_path) as pdf:
        full_text = ''
        for page in pdf.pages:
            full_text += page.extract_text()
    
    # GRIæŒ‡æ¨™æ¤œç´¢
    gri_pattern = r'GRI\s+(\d+-\d+)'
    gri_disclosures = re.findall(gri_pattern, full_text)
    
    # é–‹ç¤ºã‚¹ã‚³ã‚¢ç®—å‡º
    total_gri_indicators = 91  # GRI Standards
    disclosure_rate = len(set(gri_disclosures)) / total_gri_indicators
    
    return {
        'disclosed_indicators': len(set(gri_disclosures)),
        'disclosure_rate': disclosure_rate
    }
```

æˆ¦ç•¥ç ”ç©¶ä»®èª¬ï¼š
- GRI disclosure quality â†’ ESG performance
- Materiality focus â†’ Stakeholder alignment
```

**3. Arabesque S-Ray** ï¼ˆé™å®šç„¡æ–™ï¼‰
```
URLï¼šhttps://www.arabesque.com/s-ray/
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š10,000+ ä¼æ¥­
ãƒ‡ãƒ¼ã‚¿ï¼š
- ESG scores (100ç‚¹æº€ç‚¹)
- Temperature alignment (Paris Agreement)
- SDG alignment scores

ã‚¢ã‚¯ã‚»ã‚¹ï¼šResearch trials available
```

**4. Corporate Human Rights Benchmark (CHRB)** ğŸ†“
```
URLï¼šhttps://www.worldbenchmarkingalliance.org/corporate-human-rights-benchmark/
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š230 largest companies (apparel, agri, extractives)
ãƒ‡ãƒ¼ã‚¿ï¼šHuman rights performance indicators

ç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼šExcel
æˆ¦ç•¥ç ”ç©¶ï¼šSupply chain management Ã— Human rights
```

**5. Free the Truth (FtT)** ğŸ†“
```
URLï¼šhttps://freethetruth.io/
ãƒ‡ãƒ¼ã‚¿ï¼šCorporate lobbying + climate positions
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šMajor corporations

æ´»ç”¨ï¼šCorporate political activityç ”ç©¶
```

#### Tier 3: Government & Regulatory ESG Data

**1. U.S. EPA (Environmental Protection Agency)** ğŸŒŸ **æ¨å¥¨ãƒ»ç„¡æ–™**
```
**Toxic Release Inventory (TRI)**
URLï¼šhttps://www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šU.S. facilities
ãƒ‡ãƒ¼ã‚¿ï¼šæœ‰å®³ç‰©è³ªæ’å‡ºé‡ï¼ˆ1988ã€œç¾åœ¨ï¼‰

**Greenhouse Gas Reporting Program (GHGRP)**
URLï¼šhttps://www.epa.gov/ghgreporting
ãƒ‡ãƒ¼ã‚¿ï¼šFacility-level GHG emissions

Python APIï¼š
```python
import pandas as pd

# TRI data download
tri_url = "https://enviro.epa.gov/enviro/efservice/tri_facility/state_abbr/CA/rows/0:1000/CSV"
tri_data = pd.read_csv(tri_url)

# Facility to company matchingï¼ˆname fuzzy matchingï¼‰
from fuzzywuzzy import fuzz
# ... matching logic
```

æˆ¦ç•¥ç ”ç©¶ä¾‹ï¼š
- Toxic emissions â†’ Local community relations
- Facility-level emissions â†’ Corporate carbon strategy
```

**2. European Union ETS (Emissions Trading System)** ğŸŒŸ **ç„¡æ–™**
```
URLï¼šhttps://ec.europa.eu/clima/ets/
ãƒ‡ãƒ¼ã‚¿ï¼šEUä¼æ¥­ã®carbon emissionsï¼ˆverifiedï¼‰
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š10,000+ installations in 31 countries

å¼·ã¿ï¼š
- Verified emissionsï¼ˆé«˜ä¿¡é ¼æ€§ï¼‰
- 2005ã€œç¾åœ¨ã®é•·æœŸãƒ‡ãƒ¼ã‚¿
- Free allowance allocationæƒ…å ±

æˆ¦ç•¥ç ”ç©¶ï¼š
- EU ETS participation â†’ Carbon efficiency
- Free allocation â†’ Windfall profits
- Carbon price exposure â†’ Investment decisions
```

**3. UK Modern Slavery Registry** ğŸ†“
```
URLï¼šhttps://www.modernslaveryregistry.org/
ãƒ‡ãƒ¼ã‚¿ï¼šUKä¼æ¥­ã®Modern Slavery statements
ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š20,000+ statements

ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼š
- Supply chain due diligence quality
- Risk assessment comprehensiveness
```

### I.2 ESG Variable Construction Examples

#### Carbon Intensity

```python
def calculate_carbon_metrics(df):
    """ä¼æ¥­ã®ã‚«ãƒ¼ãƒœãƒ³æŒ‡æ¨™è¨ˆç®—"""
    
    # Carbon intensityï¼ˆå£²ä¸Šã‚ãŸã‚Šï¼‰
    df['carbon_intensity_revenue'] = df['total_emissions_tco2'] / df['revenue_millions']
    
    # Carbon intensityï¼ˆè³‡ç”£ã‚ãŸã‚Šï¼‰
    df['carbon_intensity_assets'] = df['total_emissions_tco2'] / df['total_assets']
    
    # Scope 3 ratioï¼ˆã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ï¼‰
    df['scope3_ratio'] = df['scope3_emissions'] / df['total_emissions']
    
    # Carbon efficiency trendï¼ˆYoYæ”¹å–„ï¼‰
    df['carbon_efficiency_change'] = df.groupby('firm_id')['carbon_intensity_revenue'].pct_change()
    
    return df

# ç”£æ¥­èª¿æ•´æ¸ˆã¿carbon performance
industry_median = df.groupby('industry')['carbon_intensity_revenue'].transform('median')
df['carbon_performance_vs_industry'] = df['carbon_intensity_revenue'] / industry_median

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H1: Carbon efficiency improvement â†’ Operating margin improvement
# H2: High Scope 3 ratio â†’ Supply chain disruption risk
# H3: Carbon performance vs. industry â†’ Green premium in stock returns
```

#### ESG Controversy Score

```python
def construct_esg_controversy_score(news_df):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ESG controversyã‚¹ã‚³ã‚¢æ§‹ç¯‰"""
    
    # Controversy keywords
    controversy_keywords = {
        'environmental': ['pollution', 'spill', 'contamination', 'toxic', 'emissions violation'],
        'social': ['discrimination', 'harassment', 'labor violation', 'child labor', 'strike'],
        'governance': ['fraud', 'bribery', 'corruption', 'insider trading', 'accounting scandal']
    }
    
    # Each firm-year-month
    results = []
    
    for (firm, year, month), group in news_df.groupby(['firm_id', 'year', 'month']):
        
        scores = {}
        for category, keywords in controversy_keywords.items():
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹æœ¬æ–‡ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
            controversy_count = sum(
                any(kw in article.lower() for kw in keywords)
                for article in group['article_text']
            )
            scores[f'{category}_controversy_count'] = controversy_count
        
        scores['total_controversy'] = sum(scores.values())
        scores['firm_id'] = firm
        scores['year'] = year
        scores['month'] = month
        
        results.append(scores)
    
    controversy_df = pd.DataFrame(results)
    
    # Aggregate to annual
    annual_controversy = controversy_df.groupby(['firm_id', 'year']).sum().reset_index()
    
    return annual_controversy

# æˆ¦ç•¥ç ”ç©¶ä»®èª¬
# H: ESG controversy â†’ Stock return volatilityâ†‘
# H: ESG controversy â†’ CEO turnover probabilityâ†‘
# H: Past controversy â†’ Future ESG investmentâ†‘ï¼ˆlearningï¼‰
```

---

## APPENDIX J: Additional Asian Data Sources & Strategies

### J.1 ASEAN Deep Dive

#### **Singapore ğŸ‡¸ğŸ‡¬**

**SGX (Singapore Exchange)**
```
URLï¼šhttps://www.sgx.com/
ãƒ‡ãƒ¼ã‚¿ï¼šä¸Šå ´ä¼æ¥­æƒ…å ±ã€æ ªä¾¡ã€è²¡å‹™ã‚µãƒãƒªãƒ¼
ã‚¢ã‚¯ã‚»ã‚¹ï¼šä¸€éƒ¨ç„¡æ–™ã€è©³ç´°ãƒ‡ãƒ¼ã‚¿ã¯å¥‘ç´„

**Unique Data**ï¼š
- REITsï¼ˆReal Estate Investment Trustsï¼‰æœ€å¤§å¸‚å ´
- Infrastructure companiesï¼ˆAsian focusï¼‰
```

**ACRA (Accounting and Corporate Regulatory Authority)**
```
URLï¼šhttps://www.acra.gov.sg/
ãƒ‡ãƒ¼ã‚¿ï¼šä¼æ¥­ç™»è¨˜æƒ…å ±ã€è²¡å‹™è«¸è¡¨
Costï¼šæœ‰æ–™ï¼ˆper company basisï¼‰

æˆ¦ç•¥ç ”ç©¶ï¼š
- Regional HQ strategy
- Asian operational baseåŠ¹æœ
```

#### **Malaysia ğŸ‡²ğŸ‡¾**

**Bursa Malaysia**
```
URLï¼šhttps://www.bursamalaysia.com/
ãƒ‡ãƒ¼ã‚¿ï¼šä¸Šå ´ä¼æ¥­æƒ…å ±ã€è²¡å‹™
ã‚¢ã‚¯ã‚»ã‚¹ï¼šç„¡æ–™ï¼ˆExcel downloadï¼‰

**Malaysia-Specific Research**ï¼š
- Bumiputera policyåŠ¹æœ
- Government-Linked Companies (GLCs)
- Shariah-compliant firmsï¼ˆIslamic financeï¼‰

Pythonå®Ÿè£…ï¼š
```python
import requests
from bs4 import BeautifulSoup

def scrape_bursa_data(stock_code):
    """Bursa Malaysiaã‹ã‚‰ä¼æ¥­ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    
    url = f"https://www.bursamalaysia.com/market_information/equities_prices?stock_code={stock_code}"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Parse financial data
    # ...å®Ÿè£…
    
    return financial_data
```

ç ”ç©¶ä¾‹ï¼š
- GLCs governance â†’ Performance
- Shariah compliance â†’ Risk-return profile
```

#### **Thailand ğŸ‡¹ğŸ‡­**

**SET (Stock Exchange of Thailand)**
```
URLï¼šhttps://www.set.or.th/
ãƒ‡ãƒ¼ã‚¿ï¼šæ ªä¾¡ã€è²¡å‹™æƒ…å ±
ã‚¢ã‚¯ã‚»ã‚¹ï¼šCSV/Excelç„¡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

**Thailand-Specific**ï¼š
- Automotive clusterç ”ç©¶ï¼ˆToyota, Hondaé›†ç©ï¼‰
- ASEAN supply chain hub
- Royal family affiliated firms

æˆ¦ç•¥ç ”ç©¶ï¼š
- Industrial clusteråŠ¹æœ
- ASEAN regional integration
- Political connections & performance
```

#### **Vietnam ğŸ‡»ğŸ‡³**

**HOSE & HNX**
```
HOSE (Ho Chi Minh)ï¼šhttps://www.hsx.vn/
HNX (Hanoi)ï¼šhttps://www.hnx.vn/
ãƒ‡ãƒ¼ã‚¿ï¼šåŸºæœ¬è²¡å‹™ã€æ ªä¾¡

**Emerging Market Research Opportunities**ï¼š
- SOE reformåŠ¹æœ
- FDI entry mode selection
- Institutional voidså¯¾å¿œ

Web scrapingä¾‹ï¼š
```python
def scrape_vietnam_stocks():
    """ãƒ™ãƒˆãƒŠãƒ æ ªå¼ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    
    hose_url = "https://www.hsx.vn/Modules/Listed/Web/StockList"
    
    # æ³¨æ„ï¼šrobots.txtç¢ºèªã€Terms of Serviceéµå®ˆ
    response = requests.get(hose_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Parse stock list
    # ...
    
    return stock_data
```

ç ”ç©¶ãƒ†ãƒ¼ãƒï¼š
- Transition economy strategies
- Liability of foreignness in Vietnam
```

### J.2 Middle East & Africaï¼ˆæ‹¡å¼µï¼‰

#### **UAE ğŸ‡¦ğŸ‡ª**

**DFM & ADX**
```
Dubai Financial Marketï¼šhttps://www.dfm.ae/
Abu Dhabi Securities Exchangeï¼šhttps://www.adx.ae/
ãƒ‡ãƒ¼ã‚¿ï¼šGCCä¼æ¥­æƒ…å ±

æˆ¦ç•¥ç ”ç©¶ï¼š
- Family business in GCC
- Sovereign wealth fund investments
- Oil price dependence & diversification
```

#### **South Africa ğŸ‡¿ğŸ‡¦**

**JSE (Johannesburg Stock Exchange)**
```
URLï¼šhttps://www.jse.co.za/
ãƒ‡ãƒ¼ã‚¿ï¼šã‚¢ãƒ•ãƒªã‚«æœ€å¤§å¸‚å ´
ã‚¢ã‚¯ã‚»ã‚¹ï¼šä¸€éƒ¨ç„¡æ–™

Africa researchï¼š
- Mining companies strategy
- BEE (Black Economic Empowerment) impact
- Emerging market MNCs
```

---

## APPENDIX K: Complete Workflow Automation Script

### K.1 End-to-End Research Pipeline

```python
"""
complete_strategic_research_pipeline.py

å®Œå…¨è‡ªå‹•åŒ–ã•ã‚ŒãŸæˆ¦ç•¥ç ”ç©¶ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
Phase 1 â†’ Phase 8ã‚’ä¸€æ‹¬å®Ÿè¡Œ
"""

import pandas as pd
import numpy as np
from datetime import datetime
import logging

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆã“ã®ã‚¹ã‚­ãƒ«ã§æä¾›ï¼‰
from data_collectors import (
    CompustatCollector,
    PatentsViewCollector,
    EDINETCollector,
    SECTextCollector
)

from data_quality_checker import (
    AdvancedQualityAssurance,
    SampleSizeCalculator
)

from network_analyzer import (
    BoardNetworkAnalyzer,
    AllianceNetworkAnalyzer,
    PatentCitationNetworkAnalyzer
)

from text_analyzer import (
    MDAAnalyzer,
    EarningsCallAnalyzer
)

from causal_ml import (
    CausalForestEstimator,
    DMLEstimator,
    SyntheticControlAnalyzer
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    filename='research_pipeline.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class StrategicResearchPipeline:
    """
    çµ±åˆç ”ç©¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ä½¿ç”¨ä¾‹ï¼š
    ```python
    pipeline = StrategicResearchPipeline(
        research_question="R&D intensity â†’ firm performance",
        sample_criteria={'industry': 'manufacturing', 'years': (2010, 2023)},
        output_dir='./output/'
    )
    
    pipeline.run_full_pipeline()
    ```
    """
    
    def __init__(self, research_question, sample_criteria, output_dir):
        self.research_question = research_question
        self.sample_criteria = sample_criteria
        self.output_dir = output_dir
        self.data = {}
        
        logging.info(f"Pipeline initialized: {research_question}")
    
    def phase1_research_design(self):
        """Phase 1: ç ”ç©¶è¨­è¨ˆ"""
        logging.info("Phase 1: Research Design")
        
        # Variable identificationï¼ˆè‡ªå‹• or æ‰‹å‹•æŒ‡å®šï¼‰
        self.variables = {
            'dv': 'roa',
            'iv': 'rd_intensity',
            'moderator': 'env_dynamism',
            'controls': ['firm_size', 'leverage', 'firm_age']
        }
        
        # Power analysis
        calc = SampleSizeCalculator()
        power_result = calc.regression_sample_size(
            num_predictors=len(self.variables['controls']) + 2,
            expected_r2=0.15,
            power=0.80
        )
        
        logging.info(f"Required sample size: {power_result['recommended_n']}")
        
        return power_result
    
    def phase2_data_collection(self):
        """Phase 2-3: ãƒ‡ãƒ¼ã‚¿åé›†"""
        logging.info("Phase 2-3: Data Collection")
        
        # Compustat financial data
        compustat = CompustatCollector()
        self.data['financials'] = compustat.collect_sample(
            start_year=self.sample_criteria['years'][0],
            end_year=self.sample_criteria['years'][1],
            industry=self.sample_criteria.get('industry')
        )
        logging.info(f"Compustat: {len(self.data['financials'])} observations")
        
        # Patent data
        patents = PatentsViewCollector()
        self.data['patents'] = patents.collect_firm_patents(
            firms=self.data['financials']['gvkey'].unique(),
            start_year=self.sample_criteria['years'][0],
            end_year=self.sample_criteria['years'][1]
        )
        logging.info(f"Patents: {self.data['patents']['gvkey'].nunique()} firms")
        
        # Text dataï¼ˆoptionalï¼‰
        if self.sample_criteria.get('include_text'):
            sec_text = SECTextCollector()
            self.data['mda_text'] = sec_text.collect_mda_texts(
                ciks=self.data['financials']['cik'].unique()
            )
            logging.info(f"MD&A texts: {len(self.data['mda_text'])} firm-years")
        
        return self.data
    
    def phase4_data_integration(self):
        """Phase 4-5: ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»å¤‰æ•°æ§‹ç¯‰"""
        logging.info("Phase 4-5: Data Integration & Variable Construction")
        
        # Merge datasets
        df_panel = pd.merge(
            self.data['financials'],
            self.data['patents'],
            on=['gvkey', 'year'],
            how='left'
        )
        
        # Variable construction
        df_panel['roa'] = df_panel['ni'] / df_panel['at']
        df_panel['rd_intensity'] = df_panel['xrd'] / df_panel['sale']
        df_panel['firm_size'] = np.log(df_panel['at'])
        df_panel['leverage'] = df_panel['dltt'] / df_panel['at']
        
        # Environment dynamismï¼ˆç”£æ¥­ãƒ¬ãƒ™ãƒ«ï¼‰
        df_panel['env_dynamism'] = df_panel.groupby(['sich', 'year'])['sale'].transform(
            lambda x: x.std() / x.mean()
        )
        
        # Lagged variables
        for var in ['rd_intensity', 'firm_size', 'leverage']:
            df_panel[f'{var}_lag1'] = df_panel.groupby('gvkey')[var].shift(1)
        
        self.df_panel = df_panel
        logging.info(f"Panel dataset: {len(df_panel)} observations, {df_panel['gvkey'].nunique()} firms")
        
        return df_panel
    
    def phase6_quality_assurance(self):
        """Phase 6: å“è³ªä¿è¨¼"""
        logging.info("Phase 6: Quality Assurance")
        
        qa = AdvancedQualityAssurance(
            self.df_panel,
            firm_id='gvkey',
            time_var='year'
        )
        
        qa_report = qa.run_comprehensive_qa()
        
        # Save QA report
        qa.generate_report(
            output_formats=['html', 'json'],
            output_dir=f'{self.output_dir}/qa_reports/'
        )
        
        logging.info("Quality assurance complete")
        logging.info(f"Outliers detected: {qa_report['outliers']['total_outliers']}")
        logging.info(f"Benford's Law: {'Pass' if qa_report['benfords_law']['conforms_to_benford'] else 'Fail'}")
        
        return qa_report
    
    def phase7_analysis(self):
        """Phase 7: çµ±è¨ˆåˆ†æ"""
        logging.info("Phase 7: Statistical Analysis")
        
        from linearmodels.panel import PanelOLS
        
        # Set panel index
        df_analysis = self.df_panel.set_index(['gvkey', 'year'])
        
        # Main regression
        formula = '''
        roa ~ rd_intensity_lag1 * env_dynamism + 
              firm_size_lag1 + leverage_lag1 + firm_age + 
              EntityEffects + TimeEffects
        '''
        
        model = PanelOLS.from_formula(formula, data=df_analysis).fit(
            cov_type='clustered',
            cluster_entity=True
        )
        
        # Save results
        with open(f'{self.output_dir}/main_results.txt', 'w') as f:
            f.write(str(model.summary))
        
        logging.info("Main analysis complete")
        logging.info(f"R-squared: {model.rsquared:.4f}")
        
        # Robustness checks
        robustness_results = self._run_robustness_checks(df_analysis)
        
        return {
            'main_model': model,
            'robustness': robustness_results
        }
    
    def _run_robustness_checks(self, df):
        """Robustnessãƒã‚§ãƒƒã‚¯"""
        logging.info("Running robustness checks...")
        
        checks = {}
        
        # 1. Alternative DV
        for dv in ['roe', 'tobins_q']:
            formula = f'''
            {dv} ~ rd_intensity_lag1 + firm_size_lag1 + leverage_lag1 + 
                   EntityEffects + TimeEffects
            '''
            model = PanelOLS.from_formula(formula, data=df).fit(
                cov_type='clustered', cluster_entity=True
            )
            checks[f'dv_{dv}'] = model
        
        # 2. Exclude outliers
        df_no_outliers = df[df['outlier_flag'] == 0]
        formula_base = '''
        roa ~ rd_intensity_lag1 + firm_size_lag1 + leverage_lag1 + 
              EntityEffects + TimeEffects
        '''
        checks['no_outliers'] = PanelOLS.from_formula(
            formula_base, data=df_no_outliers
        ).fit(cov_type='clustered', cluster_entity=True)
        
        # 3. Balanced panel only
        df_balanced = df.groupby(level=0).filter(
            lambda x: len(x) == df.index.get_level_values(1).nunique()
        )
        checks['balanced'] = PanelOLS.from_formula(
            formula_base, data=df_balanced
        ).fit(cov_type='clustered', cluster_entity=True)
        
        logging.info(f"Completed {len(checks)} robustness checks")
        
        return checks
    
    def phase8_documentation(self):
        """Phase 8: æ–‡æ›¸åŒ–ãƒ»å†ç¾ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸"""
        logging.info("Phase 8: Documentation")
        
        # Data dictionary
        self._create_data_dictionary()
        
        # Replication scripts
        self._create_replication_scripts()
        
        # README
        self._create_readme()
        
        logging.info("Documentation complete")
    
    def _create_data_dictionary(self):
        """ãƒ‡ãƒ¼ã‚¿è¾æ›¸ä½œæˆ"""
        data_dict = []
        
        for var in self.df_panel.columns:
            data_dict.append({
                'Variable': var,
                'N': self.df_panel[var].count(),
                'Mean': self.df_panel[var].mean() if pd.api.types.is_numeric_dtype(self.df_panel[var]) else None,
                'SD': self.df_panel[var].std() if pd.api.types.is_numeric_dtype(self.df_panel[var]) else None
            })
        
        dd_df = pd.DataFrame(data_dict)
        dd_df.to_excel(f'{self.output_dir}/data_dictionary.xlsx', index=False)
    
    def _create_replication_scripts(self):
        """å†ç¾ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ"""
        # Placeholder - å®Ÿéš›ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        pass
    
    def _create_readme(self):
        """READMEä½œæˆ"""
        readme_content = f"""
# Replication Package

## Research Question
{self.research_question}

## Sample
- Period: {self.sample_criteria['years'][0]}-{self.sample_criteria['years'][1]}
- Industry: {self.sample_criteria.get('industry', 'All')}
- Final N: {len(self.df_panel)} firm-years

## Data Sources
- Compustat (via WRDS)
- USPTO PatentsView
- [Additional sources]

## Replication Instructions
1. Run `01_download_data.py`
2. Run `02_process_data.py`
3. Run `03_main_analysis.py`

## Requirements
- Python 3.9+
- See `requirements.txt`

## Contact
[Your Name]
[Email]

Generated: {datetime.now().strftime('%Y-%m-%d')}
"""
        
        with open(f'{self.output_dir}/README.md', 'w') as f:
            f.write(readme_content)
    
    def run_full_pipeline(self):
        """å…¨ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ"""
        logging.info("=" * 50)
        logging.info("STARTING FULL RESEARCH PIPELINE")
        logging.info("=" * 50)
        
        try:
            # Phase 1
            power_result = self.phase1_research_design()
            
            # Phase 2-3
            self.phase2_data_collection()
            
            # Phase 4-5
            self.phase4_data_integration()
            
            # Phase 6
            qa_report = self.phase6_quality_assurance()
            
            # Phase 7
            analysis_results = self.phase7_analysis()
            
            # Phase 8
            self.phase8_documentation()
            
            logging.info("=" * 50)
            logging.info("PIPELINE COMPLETED SUCCESSFULLY")
            logging.info("=" * 50)
            
            return {
                'power_analysis': power_result,
                'qa_report': qa_report,
                'analysis_results': analysis_results
            }
            
        except Exception as e:
            logging.error(f"Pipeline failed: {str(e)}")
            raise


# ========== USAGE EXAMPLE ==========

if __name__ == "__main__":
    
    # ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
    pipeline = StrategicResearchPipeline(
        research_question="Does R&D intensity improve firm performance, and is this effect moderated by environmental dynamism?",
        sample_criteria={
            'industry': 'manufacturing',  # SIC 2000-3999
            'years': (2010, 2023),
            'min_observations': 5,
            'include_text': False
        },
        output_dir='./strategic_research_output/'
    )
    
    # å…¨ãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ
    results = pipeline.run_full_pipeline()
    
    print("\n" + "="*60)
    print("RESEARCH PIPELINE COMPLETE")
    print("="*60)
    print(f"\nOutput directory: {pipeline.output_dir}")
    print(f"Final dataset: {len(pipeline.df_panel)} observations")
    print(f"Main R-squared: {results['analysis_results']['main_model'].rsquared:.4f}")
    print("\nCheck pipeline.log for detailed execution log")
```

### K.2 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```bash
strategic-research-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ compustat/
â”‚   â”‚   â”œâ”€â”€ patents/
â”‚   â”‚   â”œâ”€â”€ sec_texts/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ financial_cleaned.parquet
â”‚   â”‚   â”œâ”€â”€ patent_metrics.parquet
â”‚   â”‚   â””â”€â”€ variable_constructions.parquet
â”‚   â””â”€â”€ final/
â”‚       â”œâ”€â”€ analysis_panel.dta
â”‚       â”œâ”€â”€ analysis_panel.csv
â”‚       â””â”€â”€ analysis_panel.parquet
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collectors/
â”‚   â”‚   â”œâ”€â”€ compustat_collector.py
â”‚   â”‚   â”œâ”€â”€ patents_collector.py
â”‚   â”‚   â”œâ”€â”€ edinet_collector.py
â”‚   â”‚   â””â”€â”€ sec_text_collector.py
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”‚   â”œâ”€â”€ variable_construction.py
â”‚   â”‚   â””â”€â”€ panel_builder.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ descriptive_stats.py
â”‚   â”‚   â”œâ”€â”€ main_regression.py
â”‚   â”‚   â”œâ”€â”€ robustness_checks.py
â”‚   â”‚   â””â”€â”€ causal_ml_analysis.py
â”‚   â”œâ”€â”€ network/
â”‚   â”‚   â”œâ”€â”€ board_network.py
â”‚   â”‚   â”œâ”€â”€ alliance_network.py
â”‚   â”‚   â””â”€â”€ patent_citation_network.py
â”‚   â”œâ”€â”€ text_analysis/
â”‚   â”‚   â”œâ”€â”€ mda_sentiment.py
â”‚   â”‚   â”œâ”€â”€ topic_modeling.py
â”‚   â”‚   â””â”€â”€ earnings_call_analysis.py
â”‚   â””â”€â”€ complete_pipeline.py  # ä¸Šè¨˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_integrity.py
â”‚   â”œâ”€â”€ test_variable_construction.py
â”‚   â””â”€â”€ test_merge_logic.py
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ qa_reports/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ documentation/
â”‚   â”œâ”€â”€ data_dictionary.xlsx
â”‚   â”œâ”€â”€ variable_definitions.md
â”‚   â”œâ”€â”€ qa_report.html
â”‚   â””â”€â”€ sample_construction.md
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ REPLICATION.md
â””â”€â”€ LICENSE
```

---

## æœ€çµ‚ç¢ºèªï¼šæ‹¡å¼µç‰ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### è¿½åŠ æ©Ÿèƒ½ç¢ºèª
- [x] ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆ10-K MD&Aã€æ±ºç®—èª¬æ˜ä¼šï¼‰å®Œå‚™
- [x] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼ˆå–ç· å½¹ãƒ»ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒ»ç‰¹è¨±å¼•ç”¨ï¼‰å®Œå‚™
- [x] æ©Ÿæ¢°å­¦ç¿’Ã—å› æœæ¨è«–çµ±åˆï¼ˆCausal Forest, DML, Synthetic Controlï¼‰
- [x] ESG/ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å¤§å¹…æ‹¡å……
- [x] ã‚¢ã‚¸ã‚¢è«¸å›½ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¿½åŠ ï¼ˆASEANã€ä¸­æ±ã€ã‚¢ãƒ•ãƒªã‚«ï¼‰
- [x] å®Œå…¨è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- [x] ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚«ãƒãƒ¬ãƒƒã‚¸
- [x] åŒ—ç±³ï¼ˆç±³å›½ãƒ»ã‚«ãƒŠãƒ€ï¼‰ï¼šCompustat, CRSP, PatentsView, SEC EDGAR
- [x] æ¬§å·ï¼šOrbis, Worldscope, PATSTAT
- [x] ã‚¢ã‚¸ã‚¢11ã‚«å›½+ï¼šæ—¥æœ¬ã€éŸ“å›½ã€ä¸­å›½ã€å°æ¹¾ã€ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã€ãƒãƒ¬ãƒ¼ã‚·ã‚¢ã€ã‚¿ã‚¤ã€ãƒ™ãƒˆãƒŠãƒ ã€ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢ã€ãƒ•ã‚£ãƒªãƒ”ãƒ³ã€ã‚¤ãƒ³ãƒ‰
- [x] ã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡æ–™ã‚½ãƒ¼ã‚¹ï¼šWorld Bank, IMF, OECD, CDP, GRI
- [x] ESGå°‚é–€ï¼šMSCI, Refinitiv, Sustainalytics, CDP, EPA, EU ETS

### åˆ†ææ‰‹æ³•ã‚«ãƒãƒ¬ãƒƒã‚¸
- [x] åŸºæœ¬ãƒ‘ãƒãƒ«åˆ†æï¼ˆFE, RE, Pooled OLSï¼‰
- [x] å†…ç”Ÿæ€§å¯¾ç­–ï¼ˆIV, Heckman, PSM, DiDï¼‰
- [x] èª¿æ•´åŠ¹æœãƒ»åª’ä»‹åŠ¹æœåˆ†æ
- [x] å¤šéšå±¤ãƒ¢ãƒ‡ãƒ«ï¼ˆMLMï¼‰
- [x] ç”Ÿå­˜åˆ†æï¼ˆCox Hazardï¼‰
- [x] ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã€ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼‰
- [x] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æï¼ˆCentrality, Clusteringï¼‰
- [x] æ©Ÿæ¢°å­¦ç¿’ï¼ˆCausal Forest, DMLï¼‰
- [x] åˆæˆçµ±åˆ¶æ³•ï¼ˆSynthetic Controlï¼‰

### å†ç¾æ€§ä¿è¨¼
- [x] å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ç³»è­œè¿½è·¡
- [x] AEAæº–æ‹ ã®æ–‡æ›¸åŒ–
- [x] Pytest test suite
- [x] Dockerç’°å¢ƒ
- [x] REPLICATIONã‚¬ã‚¤ãƒ‰
- [x] è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

---

**ğŸ“âœ¨ strategic-management-research-hub v3.1 å®Œæˆï¼**

**ä¸»è¦æ‹¡å¼µç‚¹**ï¼š
1. **ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†æ**: SEC MD&Aã€æ±ºç®—èª¬æ˜ä¼štranscriptã®å®Œå…¨åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
2. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ**: 3ç¨®é¡ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå–ç· å½¹ãƒ»ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒ»ç‰¹è¨±å¼•ç”¨ï¼‰
3. **MLÃ—å› æœæ¨è«–**: Causal Forest, DML, Synthetic Controlã®å®Ÿè£…
4. **ESGæ‹¡å……**: 20+ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã€ç„¡æ–™ãƒ»æœ‰æ–™ä¸¡æ–¹ã‚«ãƒãƒ¼
5. **ã‚¢ã‚¸ã‚¢æ‹¡å¼µ**: ASEANå…¨åŸŸ+ä¸­æ±ãƒ»ã‚¢ãƒ•ãƒªã‚«
6. **å®Œå…¨è‡ªå‹•åŒ–**: Phase 1-8ã‚’ä¸€æ‹¬å®Ÿè¡Œã™ã‚‹çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**åˆè¨ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ•°**: 70+ï¼ˆç„¡æ–™ãƒ»æœ‰æ–™å«ã‚€ï¼‰
**åˆè¨ˆåˆ†ææ‰‹æ³•**: 25+ï¼ˆåŸºæœ¬çµ±è¨ˆã€œæœ€å…ˆç«¯MLï¼‰
**ã‚«ãƒãƒ¼åœ°åŸŸ**: å…¨ä¸–ç•Œï¼ˆåŒ—ç±³ãƒ»æ¬§å·ãƒ»ã‚¢ã‚¸ã‚¢ãƒ»ä¸­æ±ãƒ»ã‚¢ãƒ•ãƒªã‚«ï¼‰

ã“ã‚Œã§ã€**ã‚¼ãƒ­äºˆç®—ã‹ã‚‰ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æ²è¼‰ã¾ã§ã€å®Œå…¨å¯¾å¿œå¯èƒ½**ãªçµ±åˆã‚¹ã‚­ãƒ«ãŒå®Œæˆã—ã¾ã—ãŸã€‚

#æˆ¦ç•¥çµŒå–¶ç ”ç©¶ #å®Ÿè¨¼ç ”ç©¶ #ãƒ‡ãƒ¼ã‚¿åé›† #æ©Ÿæ¢°å­¦ç¿’ #å› æœæ¨è«– #ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ #ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ #ESGç ”ç©¶ #ã‚¢ã‚¸ã‚¢ç ”ç©¶ #ãƒˆãƒƒãƒ—ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«