# Strategic Management Research Hub - FAQ

**ã‚ˆãã‚ã‚‹è³ªå•ã¨å³åº§è§£æ±ºã‚¬ã‚¤ãƒ‰**

æœ€çµ‚æ›´æ–°ï¼š2025-11-01

---

## ğŸ“‹ ç›®æ¬¡

1. [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ç’°å¢ƒè¨­å®š](#1-ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç’°å¢ƒè¨­å®š)
2. [ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼](#2-ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼)
3. [ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ](#3-ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ)
4. [åˆ†æã‚¨ãƒ©ãƒ¼](#4-åˆ†æã‚¨ãƒ©ãƒ¼)
5. [çµæœã®è§£é‡ˆ](#5-çµæœã®è§£é‡ˆ)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#6-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)

---

## 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»ç’°å¢ƒè¨­å®š

### Q1.1: `ModuleNotFoundError: No module named 'XXX'`

**ç—‡çŠ¶ï¼š** Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„

**åŸå› ï¼š** ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**è§£æ±ºç­–ï¼š**

```bash
# å…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd /Users/changu/Desktop/ç ”ç©¶/skills/user/strategic-management-research-hub
pip install -r requirements.txt

# ç‰¹å®šã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿
pip install pandas numpy scipy statsmodels
```

**ã‚ˆãã‚ã‚‹ã‚±ãƒ¼ã‚¹ï¼š**
- `wrds`ï¼šWRDSå¥‘ç´„è€…ã®ã¿å¿…è¦ â†’ `pip install wrds`
- `econml`ï¼šå› æœæ¨è«–ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰â†’ `pip install econml`
- `transformers`ï¼šFinBERTç”¨ï¼ˆå¤§å®¹é‡ï¼‰â†’ `pip install transformers torch`

---

### Q1.2: pip install ãŒé…ã„ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹

**åŸå› ï¼š** ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€Ÿåº¦ã€ã¾ãŸã¯PyPIã‚µãƒ¼ãƒãƒ¼æ··é›‘

**è§£æ±ºç­–ï¼š**

```bash
# ãƒŸãƒ©ãƒ¼ã‚µã‚¤ãƒˆä½¿ç”¨ï¼ˆä¸­å›½ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ï¼‰
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“å»¶é•·
pip install --timeout 300 -r requirements.txt

# æ—¢å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
pip install -r requirements.txt --upgrade --ignore-installed
```

---

### Q1.3: Pythonä»®æƒ³ç’°å¢ƒã®ä½œæˆæ–¹æ³•

**æ¨å¥¨ç†ç”±ï¼š** ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ç‹¬ç«‹ã—ãŸç’°å¢ƒã€ä¾å­˜é–¢ä¿‚ã®è¡çªå›é¿

**æ‰‹é †ï¼š**

```bash
# venvä½¿ç”¨ï¼ˆPythonæ¨™æº–ï¼‰
cd /Users/changu/Desktop/ç ”ç©¶/skills/user/strategic-management-research-hub
python -m venv venv_strategy

# ä»®æƒ³ç’°å¢ƒæœ‰åŠ¹åŒ–
# macOS/Linux:
source venv_strategy/bin/activate
# Windows:
venv_strategy\Scripts\activate

# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# ç„¡åŠ¹åŒ–
deactivate
```

**condaä½¿ç”¨ã®å ´åˆï¼š**

```bash
conda create -n strategy_research python=3.10
conda activate strategy_research
pip install -r requirements.txt
```

---

## 2. ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼

### Q2.1: WRDSæ¥ç¶šã‚¨ãƒ©ãƒ¼ã€ŒAuthentication failedã€

**ç—‡çŠ¶ï¼š**
```python
wrds.Connection(wrds_username='your_username')
# Error: Authentication failed
```

**åŸå› ï¼š**
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ããªã„
2. WRDSã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒæœŸé™åˆ‡ã‚Œ
3. åˆå›æ¥ç¶šæ™‚ã®èªè¨¼æœªå®Œäº†

**è§£æ±ºç­–ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šèªè¨¼æƒ…å ±ã®ç¢ºèª
# https://wrds-www.wharton.upenn.edu/ ã§ãƒ­ã‚°ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šåˆå›è¨­å®šï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä¿å­˜ï¼‰
import wrds
conn = wrds.Connection(wrds_username='actual_username')
# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã‚’æ±‚ã‚ã‚‰ã‚Œã‚‹ â†’ å…¥åŠ›

# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šæ¥ç¶šãƒ†ã‚¹ãƒˆ
test = conn.raw_sql("SELECT * FROM comp.funda LIMIT 5")
print(test)

conn.close()
```

**ãã‚Œã§ã‚‚å¤±æ•—ã™ã‚‹å ´åˆï¼š**
- WRDS Help Deské€£çµ¡ï¼šwrds@wharton.upenn.edu
- å¤§å­¦ã®WRDSç®¡ç†è€…ã«ç¢ºèª

---

### Q2.2: PatentsView API ã‚¨ãƒ©ãƒ¼ã€Œ429 Too Many Requestsã€

**ç—‡çŠ¶ï¼š**
```
requests.exceptions.HTTPError: 429 Client Error: Too Many Requests
```

**åŸå› ï¼š** API Rate Limitè¶…éï¼ˆ45 requests/minuteï¼‰

**è§£æ±ºç­–ï¼š**

```python
# æ–¹æ³•1ï¼šå¾…æ©Ÿæ™‚é–“ã‚’è¿½åŠ 
import time

for firm in firm_list:
    patents = collector.collect_firm_patents(firm, 2020, 2023)
    time.sleep(2)  # 2ç§’å¾…æ©Ÿ

# æ–¹æ³•2ï¼šãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
# ä¸€åº¦ã«å–å¾—ã™ã‚‹ä¼æ¥­æ•°ã‚’æ¸›ã‚‰ã™
firm_list_batch1 = firm_list[:50]  # æœ€åˆã®50ç¤¾
# å‡¦ç†å¾Œã€æ¬¡ã®ãƒãƒƒãƒ

# æ–¹æ³•3ï¼šrate_limitãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’èª¿æ•´
# data_collectors.pyã®@limitsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´
# @limits(calls=30, period=60)  # ã‚ˆã‚Šä¿å®ˆçš„ã«
```

---

### Q2.3: EDINET API ã‹ã‚‰ç©ºã®ãƒ‡ãƒ¼ã‚¿ãŒè¿”ã•ã‚Œã‚‹

**ç—‡çŠ¶ï¼š**
```python
df = edinet.collect_sample('2023-01-01', '2023-12-31', ['010'])
# çµæœ: 0è¡Œã®DataFrame
```

**åŸå› ï¼š**
1. æŒ‡å®šæ—¥ã«å ±å‘Šæ›¸æå‡ºãªã—
2. ç”£æ¥­ã‚³ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã‚‹
3. APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¤ã‚Š

**è§£æ±ºç­–ï¼š**

```python
# ãƒ‡ãƒãƒƒã‚°ï¼šç‰¹å®šæ—¥ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆç¢ºèª
docs = edinet.get_document_list('2023-06-30')
print(f"åˆ©ç”¨å¯èƒ½ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(docs.get('results', []))}")
print(docs['results'][:3])  # æœ€åˆã®3ä»¶è¡¨ç¤º

# ç”£æ¥­ã‚³ãƒ¼ãƒ‰ç¢ºèª
# '010': è£½é€ æ¥­ï¼ˆæ­£ï¼‰
# '001': é‡‘èæ¥­
# '004': é‹è¼¸æ¥­

# æœŸé–“ã‚’æ‹¡å¤§ã—ã¦ãƒ†ã‚¹ãƒˆ
df = edinet.collect_sample('2023-06-01', '2023-08-31', None)  # å…¨ç”£æ¥­
print(f"å–å¾—: {len(df)}ä»¶")
```

---

### Q2.4: ä¼æ¥­åãƒãƒƒãƒãƒ³ã‚°ã§ä¸€è‡´ã—ãªã„ï¼ˆç‰¹è¨±ãƒ‡ãƒ¼ã‚¿ï¼‰

**ç—‡çŠ¶ï¼š**
```python
patents = collector.collect_firm_patents('Sony', 2020, 2023)
# çµæœ: 0ä»¶
```

**åŸå› ï¼š** ä¼æ¥­åè¡¨è¨˜ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³

**è§£æ±ºç­–ï¼š**

```python
# æ­£å¼åç§°ã‚’ä½¿ç”¨
patents = collector.collect_firm_patents('Sony Corporation', 2020, 2023)

# ã¾ãŸã¯è¤‡æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã§è©¦è¡Œ
name_variants = [
    'Sony Corporation',
    'Sony Corp',
    'Sony Group Corporation',
    'ã‚½ãƒ‹ãƒ¼æ ªå¼ä¼šç¤¾'
]

for name in name_variants:
    patents = collector.collect_firm_patents(name, 2020, 2023)
    if not patents.empty:
        print(f"æˆåŠŸ: {name}")
        break

# Fuzzy matchingã‚’ä½¿ç”¨
from fuzzywuzzy import process

# Compustatã¨Patentsã®ä¼æ¥­åãƒãƒƒãƒãƒ³ã‚°
matched = patents_collector.match_companies_to_compustat(
    patents_df,
    compustat_df,
    threshold=85  # é¡ä¼¼åº¦85%ä»¥ä¸Š
)
```

---

## 3. ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ

### Q3.1: å¤–ã‚Œå€¤ãŒå¤šã™ãã‚‹ï¼ˆ>10%ï¼‰

**ç—‡çŠ¶ï¼š**
```
QA Report: 1,523 outliers detected (15.2%)
```

**åŸå› ï¼š**
1. æ¥µç«¯ãªå€¤ãŒå®Ÿéš›ã«å­˜åœ¨ï¼ˆæ–°èˆˆä¼æ¥­ã€ç‰¹æ®Šã‚¤ãƒ™ãƒ³ãƒˆï¼‰
2. ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ï¼ˆå…¥åŠ›ãƒŸã‚¹ã€å˜ä½é–“é•ã„ï¼‰
3. Outlieræ¤œå‡ºã®thresholdãŒå³ã—ã™ãã‚‹

**è§£æ±ºç­–ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šOutlierã®è©³ç´°ç¢ºèª
high_conf_outliers = df[df['outlier_confidence'] >= 0.67]
print(high_conf_outliers[['firm_name', 'year', 'roa', 'total_assets']].head(20))

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šWinsorizationï¼ˆå¤–ã‚Œå€¤ã‚’èª¿æ•´ï¼‰
from scipy.stats.mstats import winsorize

df['roa_winsorized'] = winsorize(df['roa'], limits=[0.01, 0.01])
# ä¸Šä¸‹1%ã‚’èª¿æ•´

# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šæ¥­ç•Œåˆ¥ã«æ¨™æº–åŒ–
df['roa_industry_adj'] = df.groupby('industry')['roa'].transform(
    lambda x: (x - x.mean()) / x.std()
)

# ã‚¹ãƒ†ãƒƒãƒ—4ï¼šOutlierãƒ•ãƒ©ã‚°ã‚’ä½¿ã£ãŸåˆ†æ
# ãƒ¡ã‚¤ãƒ³åˆ†æ: Outlieré™¤å¤–
df_main = df[df['outlier_flag'] == 0]

# ãƒ­ãƒã‚¹ãƒˆãƒã‚¹ãƒã‚§ãƒƒã‚¯: Outlierå«ã‚€
df_robust = df  # å…¨ã‚µãƒ³ãƒ—ãƒ«
```

---

### Q3.2: Benford's Lawé•åãŒæ¤œå‡ºã•ã‚ŒãŸ

**ç—‡çŠ¶ï¼š**
```
âš ï¸  Benford's Law violations detected: ['total_assets', 'sales']
```

**æ„å‘³ï¼š** ãƒ‡ãƒ¼ã‚¿ã«äººç‚ºçš„æ“ä½œã®å¯èƒ½æ€§

**å¯¾å‡¦æ³•ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šé•åã®æ·±åˆ»åº¦ç¢ºèª
benford_result = qa_report['benfords_law']
for var, test in benford_result['variable_tests'].items():
    if test['p_value'] < 0.05:
        print(f"{var}: Ï‡Â² = {test['chi2_statistic']:.2f}, p = {test['p_value']:.4f}")

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å†ç¢ºèª
# - å…ƒãƒ‡ãƒ¼ã‚¿ã«æˆ»ã£ã¦æ¤œè¨¼
# - ä»–ã®å¤‰æ•°ï¼ˆnet_income, cashï¼‰ã‚‚åŒæ§˜ã‹ç¢ºèª

# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šæ—¢çŸ¥ã®ä¾‹å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª
# Benfordé•åã®æ­£å½“ãªç†ç”±ï¼š
# - æœ€ä½è³‡æœ¬é‡‘è¦åˆ¶ï¼ˆä¾‹ï¼š$10Mä»¥ä¸Šã®ä¼æ¥­ã®ã¿ï¼‰
# - ç”£æ¥­ç‰¹æœ‰ã®ä¾¡æ ¼å¸¯ï¼ˆä¾‹ï¼šèˆªç©ºæ©Ÿã¯$100Må˜ä½ï¼‰
# - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒã‚¤ã‚¢ã‚¹

# ã‚¹ãƒ†ãƒƒãƒ—4ï¼šè«–æ–‡ã§ã®å ±å‘Š
# Limitationsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ˜è¨˜ï¼š
# "Benford's Law test indicated potential data quality issues 
#  in [variables], possibly due to [regulatory thresholds/
#  industry characteristics]. We conducted robustness checks 
#  excluding these variables, and results remained consistent."
```

---

### Q3.3: ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®é«˜ã„Attrition Rateï¼ˆ>30%ï¼‰

**ç—‡çŠ¶ï¼š**
```
âš ï¸  High attrition rate: 35.2%
```

**å•é¡Œï¼š** Survival biasãŒåˆ†æçµæœã‚’æ­ªã‚ã‚‹å¯èƒ½æ€§

**è§£æ±ºç­–ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šAttritionåŸå› ã®ç‰¹å®š
attrite_firms = df[df['attrite'] == 1]['firm_id'].unique()
survive_firms = df[df['attrite'] == 0]['firm_id'].unique()

# ç‰¹æ€§æ¯”è¼ƒ
from scipy.stats import ttest_ind

for var in ['roa', 'firm_size', 'leverage']:
    t, p = ttest_ind(
        df[df['attrite'] == 1][var].dropna(),
        df[df['attrite'] == 0][var].dropna()
    )
    print(f"{var}: t={t:.2f}, p={p:.4f}")

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šå¯¾å‡¦æ³•ã®é¸æŠ

# æ–¹æ³•Aï¼šHeckman Selection Model
from statsmodels.regression.linear_model import Heckman

# ç¬¬1æ®µéšï¼šAttritionäºˆæ¸¬
# ç¬¬2æ®µéšï¼šSelectionè£œæ­£å¾Œã®æœ¬åˆ†æ

# æ–¹æ³•Bï¼šInverse Probability Weighting (IPW)
from sklearn.linear_model import LogisticRegression

# Attritionç¢ºç‡æ¨å®š
lr = LogisticRegression()
lr.fit(df[['firm_size', 'roa', 'leverage']], df['attrite'])
df['prob_attrite'] = lr.predict_proba(df[['firm_size', 'roa', 'leverage']])[:, 1]

# IPW: Attritionã—ã«ãã„ä¼æ¥­ã«é«˜ã„ã‚¦ã‚§ã‚¤ãƒˆã‚’ä»˜ä¸
df['ipw'] = 1 / (1 - df['prob_attrite'])

# åŠ é‡å›å¸°
# model = PanelOLS(...).fit(weights=df['ipw'])

# æ–¹æ³•Cï¼šBalanced panel onlyã§Robustness check
df_balanced = df.groupby('firm_id').filter(
    lambda x: len(x) == df['year'].nunique()
)
```

---

## 4. åˆ†æã‚¨ãƒ©ãƒ¼

### Q4.1: VIFï¼ˆåˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ¬ä¿‚æ•°ï¼‰ãŒ10ã‚’è¶…ãˆã‚‹

**ç—‡çŠ¶ï¼š**
```
VIF Results:
  firm_size: 12.34
  rd_intensity: 15.67
```

**å•é¡Œï¼š** å¤šé‡å…±ç·šæ€§ã«ã‚ˆã‚Šä¿‚æ•°æ¨å®šãŒä¸å®‰å®š

**è§£æ±ºç­–ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šç›¸é–¢è¡Œåˆ—ã§åŸå› ç‰¹å®š
corr_matrix = df[['firm_size', 'rd_intensity', 'leverage', 'firm_age']].corr()
print(corr_matrix)

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šå¯¾å‡¦æ³•

# æ–¹æ³•Aï¼šé«˜ç›¸é–¢å¤‰æ•°ã®ä¸€æ–¹ã‚’é™¤å¤–
# firm_sizeã¨rd_intensityãŒé«˜ç›¸é–¢(r>0.7)ãªã‚‰ã€ä¸€æ–¹ã‚’å‰Šé™¤

# æ–¹æ³•Bï¼šç›´äº¤åŒ–ï¼ˆResidualizingï¼‰
# rd_intensity ã‚’ firm_size ã§å›å¸°ã—ã€æ®‹å·®ã‚’ä½¿ç”¨
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(df[['firm_size']], df['rd_intensity'])
df['rd_intensity_resid'] = df['rd_intensity'] - lr.predict(df[['firm_size']])

# Model: roa ~ rd_intensity_resid + firm_size + ...

# æ–¹æ³•Cï¼šä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
from sklearn.decomposition import PCA

pca = PCA(n_components=3)
X_pca = pca.fit_transform(df[['firm_size', 'rd_intensity', 'leverage']])
df['PC1'] = X_pca[:, 0]
df['PC2'] = X_pca[:, 1]

# Model: roa ~ PC1 + PC2 + ...
```

---

### Q4.2: ãƒ‘ãƒãƒ«å›å¸°ã§ã€ŒSingular matrixã€ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶ï¼š**
```python
model = PanelOLS.from_formula('roa ~ rd + size + EntityEffects', data=df_panel).fit()
# LinAlgError: Singular matrix
```

**åŸå› ï¼š**
1. å®Œå…¨å…±ç·šæ€§ï¼ˆãƒ€ãƒŸãƒ¼å¤‰æ•°ãƒˆãƒ©ãƒƒãƒ—ï¼‰
2. æ¬ æå€¤ã«ã‚ˆã‚‹è‡ªç”±åº¦ä¸è¶³
3. Fixed effectsã¨æ™‚é–“ä¸å¤‰å¤‰æ•°ã®æ··åœ¨

**è§£æ±ºç­–ï¼š**

```python
# åŸå› 1å¯¾å‡¦ï¼šãƒ€ãƒŸãƒ¼å¤‰æ•°ã®ç¢ºèª
# Ã— é–“é•ã„: year_2020 + year_2021 + year_2022 + TimeEffects
# â—‹ æ­£ã—ã„: TimeEffects ã®ã¿ï¼ˆè‡ªå‹•ã§ãƒ€ãƒŸãƒ¼ç”Ÿæˆï¼‰

# åŸå› 2å¯¾å‡¦ï¼šæ¬ æå€¤é™¤å»
df_panel_clean = df_panel.dropna(subset=['roa', 'rd', 'size'])

# åŸå› 3å¯¾å‡¦ï¼šæ™‚é–“ä¸å¤‰å¤‰æ•°ã‚’Fixed effectsã‹ã‚‰é™¤å¤–
# Ã— é–“é•ã„: industry_dummy + EntityEffectsï¼ˆindustryä¸å¤‰ãªã‚‰ï¼‰
# â—‹ æ­£ã—ã„: industry_dummyã‚’é™¤å¤–ã€EntityEffectsã®ã¿

# ãƒ‡ãƒãƒƒã‚°ï¼šå¤‰æ•°ã®åˆ†æ•£ã‚’ç¢ºèª
for var in ['rd', 'size', 'leverage']:
    within_var = df_panel.groupby('firm_id')[var].transform(lambda x: x - x.mean()).var()
    print(f"{var} within-variance: {within_var:.4f}")
    # within-variance â‰ˆ 0 ãªã‚‰ã€Fixed effectsã§æ¶ˆå¤±
```

---

### Q4.3: äº¤äº’ä½œç”¨é …ã®ä¿‚æ•°ãŒéæœ‰æ„

**ç—‡çŠ¶ï¼š**
```
Model: roa ~ rd * env_dynamism + controls
rd:env_dynamism coefficient: 0.023 (p=0.234)  # éæœ‰æ„
```

**åŸå› ï¼š**
1. æœ¬å½“ã«äº¤äº’ä½œç”¨ãŒãªã„ï¼ˆç†è«–çš„äºˆæ¸¬ãŒèª¤ã‚Šï¼‰
2. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºä¸è¶³ï¼ˆæ¤œå‡ºåŠ›ä¸è¶³ï¼‰
3. æ¸¬å®šèª¤å·®ã«ã‚ˆã‚ŠåŠ¹æœãŒå¸Œé‡ˆ

**è§£æ±ºç­–ï¼š**

```python
# ã‚¹ãƒ†ãƒƒãƒ—1ï¼šSimple slopeåˆ†æã§è©³ç´°ç¢ºèª
# env_dynamisãŒä½ãƒ»ä¸­ãƒ»é«˜ã®ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«rdåŠ¹æœã‚’æ¨å®š

low_dyn = df[df['env_dynamism'] < df['env_dynamism'].quantile(0.33)]
high_dyn = df[df['env_dynamism'] > df['env_dynamism'].quantile(0.67)]

# ä½å‹•æ…‹æ€§ã‚°ãƒ«ãƒ¼ãƒ—
model_low = PanelOLS.from_formula('roa ~ rd + controls', data=low_dyn).fit()
print(f"Low dynamism: rd coef = {model_low.params['rd']:.4f}")

# é«˜å‹•æ…‹æ€§ã‚°ãƒ«ãƒ¼ãƒ—
model_high = PanelOLS.from_formula('roa ~ rd + controls', data=high_dyn).fit()
print(f"High dynamism: rd coef = {model_high.params['rd']:.4f}")

# å·®ãŒå¤§ãã‘ã‚Œã°ã€äº¤äº’ä½œç”¨ã¯å®Ÿè³ªçš„ã«å­˜åœ¨

# ã‚¹ãƒ†ãƒƒãƒ—2ï¼šModeratorå¤‰æ•°ã®æ¸¬å®šæ”¹å–„
# env_dynamismã‚’è¤‡æ•°æŒ‡æ¨™ã§æ¸¬å®š
df['env_dynamism_composite'] = (
    df['sales_volatility'] +
    df['tech_change_rate'] +
    df['competitor_turnover']
) / 3

# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šæ¤œå‡ºåŠ›åˆ†æ
from statsmodels.stats.power import TTestIndPower

# æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœé‡ã§ã®æ¤œå‡ºåŠ›ç¢ºèª
power_analysis = TTestIndPower()
required_n = power_analysis.solve_power(
    effect_size=0.3,  # æœŸå¾…ã•ã‚Œã‚‹äº¤äº’ä½œç”¨ã®åŠ¹æœé‡
    alpha=0.05,
    power=0.80
)
print(f"Required N: {required_n:.0f}, Current N: {len(df)}")
```

---

## 5. çµæœã®è§£é‡ˆ

### Q5.1: ä¿‚æ•°ã®çµŒæ¸ˆçš„æœ‰æ„æ€§vs.çµ±è¨ˆçš„æœ‰æ„æ€§

**ç—‡çŠ¶ï¼š**
```
rd_intensity coefficient: 0.0023 (p=0.001)
```

**è³ªå•ï¼š** p<0.001ã§çµ±è¨ˆçš„ã«æœ‰æ„ã ãŒã€ä¿‚æ•°0.0023ã¯å®Ÿå‹™çš„ã«æ„å‘³ãŒã‚ã‚‹ã‹ï¼Ÿ

**è§£ç­”ï¼š**

```python
# çµŒæ¸ˆçš„æœ‰æ„æ€§ã®è©•ä¾¡

# æ–¹æ³•1ï¼šæ¨™æº–åŒ–ä¿‚æ•°ï¼ˆBetaï¼‰
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['roa_std', 'rd_std']] = scaler.fit_transform(df[['roa', 'rd_intensity']])

model_std = PanelOLS.from_formula('roa_std ~ rd_std + controls', data=df_panel).fit()
print(f"Standardized coef: {model_std.params['rd_std']:.3f}")
# è§£é‡ˆï¼šrd_intensityãŒ1SDå¢—åŠ  â†’ ROAãŒ{coef}SDå¤‰åŒ–

# æ–¹æ³•2ï¼šMarginal effectï¼ˆé™ç•ŒåŠ¹æœï¼‰
# rd_intensityã‚’1%ãƒã‚¤ãƒ³ãƒˆå¢—ã‚„ã™ã¨ROAã¸ã®å½±éŸ¿
coef = 0.0023
rd_increase = 0.01  # 1%ãƒã‚¤ãƒ³ãƒˆ
roa_change = coef * rd_increase
print(f"RD 1ppå¢—åŠ  â†’ ROA {roa_change*100:.2f}ppå¤‰åŒ–")

# ç”£æ¥­å¹³å‡ROA=5%ãªã‚‰ã€0.0023*0.01 = 0.000023 (0.0023pp) ã®å¤‰åŒ–
# â†’ ç›¸å¯¾çš„ã«å°ã•ã„

# æ–¹æ³•3ï¼šå®Ÿä¾‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
mean_rd = df['rd_intensity'].mean()  # ä¾‹ï¼š0.03 (3%)
new_rd = mean_rd * 1.10  # 10%å¢—åŠ 
predicted_roa_change = coef * (new_rd - mean_rd)
print(f"RD 10%å¢—åŠ  â†’ ROA {predicted_roa_change*100:.3f}ppå¤‰åŒ–")

# çµè«–ï¼šçµ±è¨ˆçš„æœ‰æ„ã§ã‚‚ã€çµŒæ¸ˆçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãŒå°ã•ã„å ´åˆã‚ã‚Š
# â†’ Limitationsã§è­°è«–ã€ã¾ãŸã¯æ¸¬å®šç²¾åº¦ã®æ”¹å–„ã‚’æ¤œè¨
```

---

### Q5.2: Fixed effects vs. Random effects é¸æŠ

**è³ªå•ï¼š** ã©ã¡ã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ

**æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ï¼š**

```python
from linearmodels.panel import PanelOLS, RandomEffects

# Model 1: Fixed Effects
model_fe = PanelOLS.from_formula(
    'roa ~ rd + controls + EntityEffects + TimeEffects',
    data=df_panel
).fit(cov_type='clustered', cluster_entity=True)

# Model 2: Random Effects
model_re = RandomEffects.from_formula(
    'roa ~ rd + controls',
    data=df_panel
).fit()

# Hausman Testï¼ˆFE vs. REï¼‰
from scipy.stats import chi2

# ç°¡æ˜“Hausman test
fe_coef = model_fe.params['rd']
re_coef = model_re.params['rd']
fe_se = model_fe.std_errors['rd']
re_se = model_re.std_errors['rd']

hausman_stat = ((fe_coef - re_coef) ** 2) / (fe_se**2 - re_se**2)
p_value = 1 - chi2.cdf(hausman_stat, df=1)

print(f"Hausman test: Ï‡Â² = {hausman_stat:.2f}, p = {p_value:.4f}")

# è§£é‡ˆï¼š
# p < 0.05 â†’ FEæ¨å¥¨ï¼ˆunobserved heterogeneityãŒç›¸é–¢ï¼‰
# p > 0.05 â†’ REå¯ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰

# æˆ¦ç•¥ç ”ç©¶ã®å®Ÿå‹™çš„ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼š
# - ä¼æ¥­å›ºæœ‰åŠ¹æœãŒé‡è¦ãªå ´åˆï¼ˆé€šå¸¸ï¼‰ â†’ Fixed Effects
# - ã‚¯ãƒ­ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¤‰æ•°ï¼ˆæ™‚é–“ä¸å¤‰ï¼‰ãŒé‡è¦ â†’ Random Effects
# - ä¸ç¢ºã‹ãªå ´åˆ â†’ ä¸¡æ–¹å ±å‘Šã—ã€Hausman testçµæœã‚’è¨˜è¼‰
```

---

## 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### Q6.1: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ãŒé…ã„

**ç—‡çŠ¶ï¼š**
```python
df = pd.read_csv('large_dataset.csv')  # 10 GB, 100ä¸‡è¡Œ
# â†’ ãƒ¡ãƒ¢ãƒªä¸è¶³ã€å‡¦ç†ã«1æ™‚é–“ä»¥ä¸Š
```

**è§£æ±ºç­–ï¼š**

```python
# æ–¹æ³•1ï¼šChunkèª­ã¿è¾¼ã¿
chunks = []
for chunk in pd.read_csv('large_dataset.csv', chunksize=10000):
    # å¿…è¦ãªåˆ—ã®ã¿é¸æŠ
    chunk = chunk[['gvkey', 'year', 'roa', 'rd']]
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    chunk = chunk[chunk['year'] >= 2010]
    chunks.append(chunk)

df = pd.concat(chunks, ignore_index=True)

# æ–¹æ³•2ï¼šDaskä½¿ç”¨ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
import dask.dataframe as dd

ddf = dd.read_csv('large_dataset.csv')
ddf_filtered = ddf[ddf['year'] >= 2010]
df = ddf_filtered.compute()  # Pandas DataFrameã«å¤‰æ›

# æ–¹æ³•3ï¼šParquetå½¢å¼ã§ä¿å­˜ï¼ˆåœ§ç¸®+é«˜é€Ÿï¼‰
# åˆå›
df.to_parquet('dataset.parquet', compression='snappy')

# ä»¥é™
df = pd.read_parquet('dataset.parquet')  # CSVã‚ˆã‚Š10-100å€é«˜é€Ÿ

# æ–¹æ³•4ï¼šãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
df['year'] = df['year'].astype('int16')  # int64 â†’ int16
df['gvkey'] = df['gvkey'].astype('category')  # objectã‚ˆã‚Šé«˜é€Ÿ
```

---

### Q6.2: ãƒ‘ãƒãƒ«å›å¸°ãŒé…ã™ãã‚‹ï¼ˆ>10åˆ†ï¼‰

**åŸå› ï¼š** å¤§è¦æ¨¡ãƒ‘ãƒãƒ«ï¼ˆfirms Ã— years ãŒå¤§ãã„ï¼‰

**è§£æ±ºç­–ï¼š**

```python
# æ–¹æ³•1ï¼šClustered SEã®è¨ˆç®—ã‚’æœ€å°é™ã«
model = PanelOLS(...).fit(cov_type='unadjusted')  # æœ€é€Ÿ
# Robustness checkã§ã®ã¿ clustered SEä½¿ç”¨

# æ–¹æ³•2ï¼šã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›ï¼ˆPilot studyï¼‰
df_sample = df.groupby('industry').sample(frac=0.1, random_state=42)
# 10%ã‚µãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿãƒ†ã‚¹ãƒˆ â†’ æœ¬ç•ªã¯å…¨ãƒ‡ãƒ¼ã‚¿

# æ–¹æ³•3ï¼šJulia + FixedEffectModels.jlï¼ˆæœ€é«˜é€Ÿï¼‰
# Pythonã‚ˆã‚Š10-100å€é«˜é€Ÿ
# è©³ç´°ï¼šhttps://github.com/FixedEffects/FixedEffectModels.jl

# æ–¹æ³•4ï¼šParallel processing
from joblib import Parallel, delayed

def run_regression(data_chunk):
    model = PanelOLS(..., data=data_chunk).fit()
    return model.params

results = Parallel(n_jobs=4)(
    delayed(run_regression)(chunk) 
    for chunk in data_chunks
)
```

---

## è¿½åŠ ã‚µãƒãƒ¼ãƒˆ

### ã¾ã è§£æ±ºã—ãªã„å ´åˆ

1. **ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼š** `./logs/pipeline.log`
2. **SKILL.mdå‚ç…§ï¼š** è©³ç´°ãªæŠ€è¡“æ–‡æ›¸
3. **GitHub Issuesï¼š** ï¼ˆå…¬é–‹ãƒªãƒã‚¸ãƒˆãƒªã®å ´åˆï¼‰
4. **ãƒ¡ãƒ¼ãƒ«ç›¸è«‡ï¼š** research@strategic-hub.eduï¼ˆæ¶ç©ºï¼‰

---

## ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

ã“ã®FAQã«è¿½åŠ ã—ã¦ã»ã—ã„è³ªå•ãŒã‚ã‚Œã°ã€ãœã²ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚

**æœ€çµ‚æ›´æ–°ï¼š** 2025-11-01  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š** 3.0
